# ç¬¬ 9 ç« 
> ðŸ—ï¸ **ç³»çµ±æž¶æ§‹**ï¼šæœ¬ç« æ•´åˆ **Chapters 1-8** çš„æ‰€æœ‰å…§å®¹ï¼Œæ§‹å»ºå®Œæ•´çš„ WebGuard ç³»çµ±ã€‚å»ºè­°æŒ‰é †åºå­¸ç¿’å‰é¢ç« ç¯€ã€‚
ï¼šå®Œæ•´æ¸¬è©¦ç³»çµ±æž¶æ§‹ï¼ˆWebGuardï¼‰

å‰å…«ç« æˆ‘å€‘åˆ†åˆ¥æŽ¢è¨Žäº† Skills é–‹ç™¼ï¼ˆChapter 3ï¼‰ã€ç€è¦½å™¨è‡ªå‹•åŒ–ï¼ˆChapter 4ï¼‰ã€æ•¸æ“šè™•ç†ï¼ˆChapter 5ï¼‰ã€API æ¸¬è©¦ï¼ˆChapter 6ï¼‰ã€Skills ç·¨æŽ’ï¼ˆChapter 7ï¼‰ã€CI/CD æ•´åˆï¼ˆChapter 8ï¼‰ã€‚æœ¬ç« å°‡æ•´åˆæ‰€æœ‰æŠ€è¡“ï¼Œæ§‹å»ºä¸€å€‹ç”Ÿç”¢ç´šçš„**ä¼æ¥­æ¸¬è©¦å¹³å° WebGuard**ï¼šå››å±¤æž¶æ§‹è¨­è¨ˆã€å¾®æœå‹™åŒ–éƒ¨ç½²ã€é«˜å¯ç”¨æ€§ã€ç›£æŽ§å‘Šè­¦ã€å¯æ“´å±•æ€§ã€‚é€™æ˜¯ä¸€å€‹çœŸå¯¦å¯éƒ¨ç½²çš„ç³»çµ±ï¼Œä¸åªæ˜¯ç¤ºç¯„ä»£ç¢¼ã€‚

## 9.1 WebGuard ç³»çµ±æ¦‚è¿°

### 9.1.1 è¨­è¨ˆç›®æ¨™

**WebGuard** æ˜¯ä¸€å€‹åŸºæ–¼ Claude Code Skills çš„åˆ†å¸ƒå¼æ¸¬è©¦å¹³å°ï¼Œè¨­è¨ˆç›®æ¨™ï¼š

- **å¤šé¡žåž‹æ¸¬è©¦**ï¼šæ”¯æŒç€è¦½å™¨ E2Eã€APIã€æ€§èƒ½ã€å®‰å…¨æ¸¬è©¦
- **é«˜ä¸¦ç™¼**ï¼šå–®ç¯€é»žæ”¯æŒ 50+ ä¸¦è¡Œæ¸¬è©¦ï¼Œé›†ç¾¤å¯æ©«å‘æ“´å±•è‡³æ•¸ç™¾ä¸¦ç™¼
- **å¯é æ€§**ï¼š99.9% å¯ç”¨æ€§ï¼Œè‡ªå‹•æ•…éšœæ¢å¾©
- **å¯è§€æ¸¬æ€§**ï¼šå®Œæ•´çš„æŒ‡æ¨™ã€æ—¥èªŒã€éˆè·¯è¿½è¹¤
- **æ˜“ç”¨æ€§**ï¼šWeb UI + CLI + APIï¼Œé©åˆé–‹ç™¼ã€æ¸¬è©¦ã€é‹ç¶­åœ˜éšŠ

### 9.1.2 æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½æ¨¡å¡Š | èªªæ˜Ž | æŠ€è¡“æ£§ |
|----------|------|--------|
| æ¸¬è©¦ç·¨æŽ’ | å®šæ™‚ä»»å‹™ã€å·¥ä½œæµç·¨æŽ’ã€ä¾è³´ç®¡ç† | Celery, APScheduler |
| æ¸¬è©¦åŸ·è¡Œ | ç€è¦½å™¨/API/æ•¸æ“š Skills åŸ·è¡Œå¼•æ“Ž | Python, Node.js, Stagehand |
| æ•¸æ“šå­˜å„² | æ¸¬è©¦çµæžœã€é…ç½®ã€æ­·å²æ•¸æ“š | PostgreSQL, Redis, MinIO |
| å ±å‘Šç”Ÿæˆ | Allure å ±å‘Šã€è¶¨å‹¢åˆ†æžã€éƒµä»¶é€šçŸ¥ | Allure, Grafana, SMTP |
| ç›£æŽ§å‘Šè­¦ | ç³»çµ±æŒ‡æ¨™ã€æ¸¬è©¦å¥åº·åº¦ã€ç•°å¸¸å‘Šè­¦ | Prometheus, AlertManager |
| API ç¶²é—œ | RESTful APIã€èªè­‰æŽˆæ¬Š | FastAPI, JWT |
| Web ç•Œé¢ | æ¸¬è©¦ç®¡ç†ã€çµæžœæŸ¥çœ‹ã€é…ç½®ä¸­å¿ƒ | React, Next.js |

### 9.1.3 å››å±¤æž¶æ§‹è¨­è¨ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ç·¨æŽ’å±¤ (Orchestration Layer)                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ APScheduler â”‚ â”‚   Celery    â”‚ â”‚  SkillOrchestrator     â”‚ â”‚
â”‚ â”‚ (å®šæ™‚ä»»å‹™)  â”‚ â”‚ (åˆ†å¸ƒå¼éšŠåˆ—)â”‚ â”‚  (å·¥ä½œæµç·¨æŽ’)           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   åŸ·è¡Œå±¤ (Execution Layer)                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Browser      â”‚ â”‚ API          â”‚ â”‚ Data                â”‚  â”‚
â”‚ â”‚ Test Engine  â”‚ â”‚ Test Engine  â”‚ â”‚ Processing Engine   â”‚  â”‚
â”‚ â”‚ (Stagehand)  â”‚ â”‚ (httpx)      â”‚ â”‚ (Pandas)            â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å­˜å„²å±¤ (Storage Layer)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ PostgreSQL   â”‚ â”‚ Redis        â”‚ â”‚ MinIO / S3          â”‚  â”‚
â”‚ â”‚ (çµæ§‹åŒ–æ•¸æ“š) â”‚ â”‚ (å¿«å–/ä½‡åˆ—)  â”‚ â”‚ (æ–‡ä»¶å­˜å„²)          â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å ±å‘Šå±¤ (Reporting Layer)                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Allure       â”‚ â”‚ Grafana      â”‚ â”‚ Notification        â”‚  â”‚
â”‚ â”‚ (æ¸¬è©¦å ±å‘Š)   â”‚ â”‚ (å„€è¡¨æ¿)     â”‚ â”‚ (Slack/Email)       â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚ Prometheus   â”‚ â”‚  (æŒ‡æ¨™æ”¶é›†èˆ‡æŸ¥è©¢)
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## 9.2 æ•¸æ“šåº«è¨­è¨ˆï¼ˆPostgreSQLï¼‰

### 9.2.1 æ ¸å¿ƒè¡¨çµæ§‹

```sql
-- æ¸¬è©¦å¥—ä»¶è¡¨
CREATE TABLE test_suites (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL UNIQUE,
    description TEXT,
    enabled BOOLEAN DEFAULT TRUE,
    schedule_cron VARCHAR(100),  -- Cron è¡¨é”å¼ï¼Œå¦‚ '0 2 * * *'
    max_concurrent_tests INTEGER DEFAULT 5,
    timeout_minutes INTEGER DEFAULT 60,
    retry_on_failure BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- æ¸¬è©¦æ¡ˆä¾‹è¡¨
CREATE TABLE test_cases (
    id SERIAL PRIMARY KEY,
    suite_id INTEGER NOT NULL,
    name VARCHAR(255) NOT NULL,
    skill_name VARCHAR(100) NOT NULL,
    params JSONB,  -- Skill åƒæ•¸ï¼ŒJSON æ ¼å¼
    priority INTEGER DEFAULT 0,  -- å„ªå…ˆç´šï¼Œæ•¸å­—è¶Šå¤§è¶Šé«˜
    timeout_seconds INTEGER DEFAULT 300,
    enabled BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW(),
    CONSTRAINT fk_suite
        FOREIGN KEY (suite_id)
        REFERENCES test_suites(id)
        ON DELETE CASCADE
);

-- æ¸¬è©¦åŸ·è¡Œè¨˜éŒ„è¡¨
CREATE TABLE test_executions (
    id SERIAL PRIMARY KEY,
    suite_id INTEGER NOT NULL,
    started_at TIMESTAMP NOT NULL DEFAULT NOW(),
    completed_at TIMESTAMP,
    status VARCHAR(20) NOT NULL,  -- running, passed, failed, timeout
    total_tests INTEGER,
    passed_tests INTEGER,
    failed_tests INTEGER,
    skipped_tests INTEGER,
    execution_time_ms INTEGER,
    triggered_by VARCHAR(100),  -- è§¸ç™¼æ–¹å¼ï¼šschedule, manual, ci
    ci_commit_sha VARCHAR(64),  -- CI æäº¤ hash
    ci_branch VARCHAR(100),
    error_message TEXT,
    CONSTRAINT fk_suite_exec
        FOREIGN KEY (suite_id)
        REFERENCES test_suites(id)
        ON DELETE CASCADE
);

-- æ¸¬è©¦çµæžœè©³æƒ…è¡¨
CREATE TABLE test_results (
    id SERIAL PRIMARY KEY,
    execution_id INTEGER NOT NULL,
    test_case_id INTEGER NOT NULL,
    test_name VARCHAR(255) NOT NULL,
    skill_name VARCHAR(100),
    status VARCHAR(20) NOT NULL,  -- passed, failed, skipped, timeout, error
    error_message TEXT,
    error_type VARCHAR(100),  -- assertion, timeout, network, system
    stack_trace TEXT,
    execution_time_ms INTEGER,
    screenshot_url VARCHAR(500),
    video_url VARCHAR(500),
    log_url VARCHAR(500),
    retry_count INTEGER DEFAULT 0,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    CONSTRAINT fk_execution_result
        FOREIGN KEY (execution_id)
        REFERENCES test_executions(id)
        ON DELETE CASCADE,
    CONSTRAINT fk_test_case
        FOREIGN KEY (test_case_id)
        REFERENCES test_cases(id)
);

-- æ¸¬è©¦æŒ‡æ¨™è¡¨ï¼ˆæ€§èƒ½æ¸¬è©¦ï¼‰
CREATE TABLE test_metrics (
    id SERIAL PRIMARY KEY,
    test_result_id INTEGER NOT NULL,
    metric_name VARCHAR(100) NOT NULL,  -- response_time, memory_usage, cpu_usage
    metric_value NUMERIC,
    metric_unit VARCHAR(20),  -- ms, MB, %
    created_at TIMESTAMP DEFAULT NOW(),
    CONSTRAINT fk_test_result_metric
        FOREIGN KEY (test_result_id)
        REFERENCES test_results(id)
        ON DELETE CASCADE
);

-- Skill è¨»å†Šè¡¨
CREATE TABLE skills_registry (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL UNIQUE,
    category VARCHAR(50),  -- browser, api, data
    description TEXT,
    version VARCHAR(20),
    enabled BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- å‰µå»ºç´¢å¼•ï¼ˆæ€§èƒ½å„ªåŒ–ï¼‰
CREATE INDEX idx_test_executions_started ON test_executions(started_at DESC);
CREATE INDEX idx_test_executions_status ON test_executions(status);
CREATE INDEX idx_test_executions_suite ON test_executions(suite_id, started_at DESC);
CREATE INDEX idx_test_results_execution ON test_results(execution_id);
CREATE INDEX idx_test_results_status ON test_results(status);
CREATE INDEX idx_test_results_created ON test_results(created_at DESC);
CREATE INDEX idx_test_cases_suite ON test_cases(suite_id);

-- å‰µå»ºè¦–åœ–ï¼ˆçµ±è¨ˆæŸ¥è©¢ï¼‰
CREATE VIEW v_test_suite_statistics AS
SELECT
    ts.id AS suite_id,
    ts.name AS suite_name,
    COUNT(DISTINCT te.id) AS total_executions,
    COUNT(DISTINCT CASE WHEN te.status = 'passed' THEN te.id END) AS passed_executions,
    COUNT(DISTINCT CASE WHEN te.status = 'failed' THEN te.id END) AS failed_executions,
    ROUND(
        CAST(COUNT(DISTINCT CASE WHEN te.status = 'passed' THEN te.id END) AS NUMERIC) /
        NULLIF(COUNT(DISTINCT te.id), 0) * 100,
        2
    ) AS pass_rate,
    AVG(te.execution_time_ms) AS avg_execution_time_ms,
    MAX(te.completed_at) AS last_execution_at
FROM test_suites ts
LEFT JOIN test_executions te ON ts.id = te.suite_id
WHERE te.completed_at >= NOW() - INTERVAL '30 days'  -- æœ€è¿‘ 30 å¤©
GROUP BY ts.id, ts.name;
```

### 9.2.2 æ•¸æ“šé·ç§»ï¼ˆAlembicï¼‰

ä½¿ç”¨ Alembic ç®¡ç†æ•¸æ“šåº« Schema ç‰ˆæœ¬ï¼š

```python
# alembic/versions/001_initial_schema.py
from alembic import op
import sqlalchemy as sa


def upgrade():
    """å‡ç´šæ•¸æ“šåº« Schema"""
    # å‰µå»º test_suites è¡¨
    op.create_table(
        'test_suites',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('name', sa.String(255), nullable=False, unique=True),
        sa.Column('description', sa.Text()),
        sa.Column('enabled', sa.Boolean(), default=True),
        # ... å…¶ä»–æ¬„ä½
    )

    # å‰µå»º test_executions è¡¨
    op.create_table('test_executions', ...)

    # å‰µå»ºç´¢å¼•
    op.create_index('idx_test_executions_started', 'test_executions', ['started_at'])


def downgrade():
    """å›žæ»¾ Schema è®Šæ›´"""
    op.drop_index('idx_test_executions_started')
    op.drop_table('test_executions')
    op.drop_table('test_suites')
```


## 9.3 ç·¨æŽ’å±¤å¯¦ç¾

### 9.3.1 Celery åˆ†å¸ƒå¼ä»»å‹™éšŠåˆ—

**å®‰è£èˆ‡é…ç½®ï¼š**

```bash
pip install celery redis
```

**celery_config.pyï¼š**

```python
from celery import Celery
from kombu import Exchange, Queue

# å‰µå»º Celery æ‡‰ç”¨
celery_app = Celery(
    'webguard',
    broker='redis://localhost:6379/0',  # ä½¿ç”¨ Redis ä½œç‚ºæ¶ˆæ¯ä»£ç†
    backend='redis://localhost:6379/1'  # ä½¿ç”¨ Redis å­˜å„²ä»»å‹™çµæžœ
)

# é…ç½®
celery_app.conf.update(
    # ä»»å‹™åºåˆ—åŒ–
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Taipei',
    enable_utc=True,

    # ä»»å‹™è·¯ç”±ï¼ˆä¸åŒé¡žåž‹ä»»å‹™åˆ†é…åˆ°ä¸åŒéšŠåˆ—ï¼‰
    task_routes={
        'webguard.tasks.browser.*': {'queue': 'browser'},
        'webguard.tasks.api.*': {'queue': 'api'},
        'webguard.tasks.data.*': {'queue': 'data'},
    },

    # éšŠåˆ—å®šç¾©
    task_queues=(
        Queue('default', Exchange('default'), routing_key='default'),
        Queue('browser', Exchange('browser'), routing_key='browser'),
        Queue('api', Exchange('api'), routing_key='api'),
        Queue('data', Exchange('data'), routing_key='data'),
    ),

    # Worker é…ç½®
    worker_prefetch_multiplier=1,  # ä¸€æ¬¡åªå–ä¸€å€‹ä»»å‹™ï¼ˆé¿å…é˜»å¡žï¼‰
    worker_max_tasks_per_child=100,  # 100 å€‹ä»»å‹™å¾Œé‡å•Ÿ workerï¼ˆé˜²æ­¢å…§å­˜æ´©æ¼ï¼‰

    # ä»»å‹™è¶…æ™‚
    task_soft_time_limit=1800,  # 30 åˆ†é˜è»Ÿè¶…æ™‚ï¼ˆç™¼é€ SoftTimeLimitExceeded ç•°å¸¸ï¼‰
    task_time_limit=2100,  # 35 åˆ†é˜ç¡¬è¶…æ™‚ï¼ˆå¼·åˆ¶çµ‚æ­¢ï¼‰

    # çµæžœéŽæœŸæ™‚é–“
    result_expires=3600  # 1 å°æ™‚å¾Œåˆªé™¤çµæžœ
)
```

**å®šç¾©ä»»å‹™ï¼ˆtasks.pyï¼‰ï¼š**

```python
from celery_config import celery_app
from webguard.orchestrator import SkillOrchestrator
from webguard.models import TestExecution, TestResult
import logging

logger = logging.getLogger(__name__)


@celery_app.task(bind=True, name='webguard.tasks.execute_test_suite')
def execute_test_suite(self, suite_id: int, triggered_by: str = 'manual'):
    """
    åŸ·è¡Œæ¸¬è©¦å¥—ä»¶

    Args:
        suite_id: æ¸¬è©¦å¥—ä»¶ ID
        triggered_by: è§¸ç™¼æ–¹å¼ï¼ˆschedule, manual, ciï¼‰
    """
    from webguard.database import Session
    from webguard.models import TestSuite

    session = Session()

    try:
        # 1. ç²å–æ¸¬è©¦å¥—ä»¶
        suite = session.query(TestSuite).filter_by(id=suite_id).first()
        if not suite:
            raise ValueError(f"æ¸¬è©¦å¥—ä»¶ä¸å­˜åœ¨: {suite_id}")

        # 2. å‰µå»ºåŸ·è¡Œè¨˜éŒ„
        execution = TestExecution(
            suite_id=suite_id,
            status='running',
            triggered_by=triggered_by
        )
        session.add(execution)
        session.commit()

        # 3. ç²å–æ¸¬è©¦æ¡ˆä¾‹
        test_cases = suite.test_cases
        if not test_cases:
            execution.status = 'skipped'
            execution.error_message = 'ç„¡æ¸¬è©¦æ¡ˆä¾‹'
            session.commit()
            return

        # 4. åˆå§‹åŒ–ç·¨æŽ’å™¨
        orchestrator = SkillOrchestrator()

        # 5. è¨»å†Š Skillsï¼ˆå‹•æ…‹åŠ è¼‰ï¼‰
        from webguard.skill_loader import load_all_skills
        load_all_skills(orchestrator)

        # 6. æ§‹å»ºå·¥ä½œæµ
        workflow = []
        for test_case in test_cases:
            if test_case.enabled:
                workflow.append({
                    "skill": test_case.skill_name,
                    "params": test_case.params or {},
                    "metadata": {
                        "test_case_id": test_case.id,
                        "test_name": test_case.name
                    }
                })

        # 7. åŸ·è¡Œå·¥ä½œæµ
        import asyncio
        result = asyncio.run(
            orchestrator.execute_sequence_with_flow(
                workflow,
                stop_on_error=False  # ä¸å› å–®å€‹å¤±æ•—åœæ­¢
            )
        )

        # 8. ä¿å­˜çµæžœ
        execution.total_tests = result['total_steps']
        execution.passed_tests = result['passed']
        execution.failed_tests = result['failed']
        execution.status = 'passed' if result['success'] else 'failed'
        execution.completed_at = datetime.now()

        # ä¿å­˜è©³ç´°çµæžœ
        for skill_result in result['results']:
            test_result = TestResult(
                execution_id=execution.id,
                test_case_id=skill_result['metadata']['test_case_id'],
                test_name=skill_result['metadata']['test_name'],
                skill_name=skill_result['skill_name'],
                status='passed' if skill_result['success'] else 'failed',
                error_message=skill_result.get('error'),
                execution_time_ms=skill_result.get('duration_ms')
            )
            session.add(test_result)

        session.commit()

        logger.info(
            f"æ¸¬è©¦å¥—ä»¶ '{suite.name}' åŸ·è¡Œå®Œæˆ: "
            f"{execution.passed_tests}/{execution.total_tests} é€šéŽ"
        )

    except Exception as e:
        logger.error(f"åŸ·è¡Œæ¸¬è©¦å¥—ä»¶å¤±æ•—: {e}", exc_info=True)

        if execution:
            execution.status = 'error'
            execution.error_message = str(e)
            execution.completed_at = datetime.now()
            session.commit()

        raise

    finally:
        session.close()


@celery_app.task(name='webguard.tasks.browser.run_e2e_test')
def run_e2e_test(test_config: dict):
    """é‹è¡Œç€è¦½å™¨ E2E æ¸¬è©¦"""
    # å¯¦ä½œ...
    pass


@celery_app.task(name='webguard.tasks.api.run_api_test')
def run_api_test(endpoint_config: dict):
    """é‹è¡Œ API æ¸¬è©¦"""
    # å¯¦ä½œ...
    pass
```

**å•Ÿå‹• Workerï¼š**

```bash
# å•Ÿå‹•è™•ç†ç€è¦½å™¨æ¸¬è©¦çš„ workerï¼ˆä¸¦ç™¼ 2ï¼‰
celery -A celery_config worker -Q browser -c 2 -n browser_worker@%h

# å•Ÿå‹•è™•ç† API æ¸¬è©¦çš„ workerï¼ˆä¸¦ç™¼ 10ï¼‰
celery -A celery_config worker -Q api -c 10 -n api_worker@%h

# å•Ÿå‹•è™•ç†æ•¸æ“šä»»å‹™çš„ workerï¼ˆä¸¦ç™¼ 5ï¼‰
celery -A celery_config worker -Q data -c 5 -n data_worker@%h
```

### 9.3.2 APScheduler å®šæ™‚ä»»å‹™

```python
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from webguard.database import Session
from webguard.models import TestSuite
from webguard.tasks import execute_test_suite
import logging

logger = logging.getLogger(__name__)

scheduler = BackgroundScheduler()


def load_scheduled_suites():
    """å¾žæ•¸æ“šåº«è¼‰å…¥å®šæ™‚æ¸¬è©¦å¥—ä»¶"""
    session = Session()

    try:
        suites = session.query(TestSuite).filter(
            TestSuite.enabled == True,
            TestSuite.schedule_cron.isnot(None)
        ).all()

        for suite in suites:
            # ç§»é™¤èˆŠä»»å‹™ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            job_id = f'suite_{suite.id}'
            if scheduler.get_job(job_id):
                scheduler.remove_job(job_id)

            # æ·»åŠ æ–°ä»»å‹™
            scheduler.add_job(
                execute_test_suite.delay,  # Celery ç•°æ­¥ä»»å‹™
                trigger=CronTrigger.from_crontab(suite.schedule_cron),
                args=[suite.id],
                kwargs={'triggered_by': 'schedule'},
                id=job_id,
                name=f'Test Suite: {suite.name}',
                replace_existing=True
            )

            logger.info(
                f"å·²è¨»å†Šå®šæ™‚ä»»å‹™: {suite.name} ({suite.schedule_cron})"
            )

    finally:
        session.close()


# æ‡‰ç”¨å•Ÿå‹•æ™‚è¼‰å…¥å®šæ™‚ä»»å‹™
def start_scheduler():
    """å•Ÿå‹•èª¿åº¦å™¨"""
    load_scheduled_suites()
    scheduler.start()
    logger.info("å®šæ™‚ä»»å‹™èª¿åº¦å™¨å·²å•Ÿå‹•")


# å‹•æ…‹æ›´æ–°ï¼ˆAPI èª¿ç”¨ï¼‰
def refresh_scheduled_suites():
    """é‡æ–°è¼‰å…¥å®šæ™‚ä»»å‹™ï¼ˆAPI æ›´æ–°å¾Œèª¿ç”¨ï¼‰"""
    load_scheduled_suites()
    logger.info("å®šæ™‚ä»»å‹™å·²é‡æ–°è¼‰å…¥")
```


## 9.4 åŸ·è¡Œå±¤å¯¦ç¾

### 9.4.1 Skill åŠ è¼‰å™¨

å‹•æ…‹åŠ è¼‰æ‰€æœ‰ Skillsï¼š

```python
# webguard/skill_loader.py
import importlib
import inspect
from pathlib import Path
from webguard.orchestrator import SkillOrchestrator
import logging

logger = logging.getLogger(__name__)


def load_all_skills(orchestrator: SkillOrchestrator):
    """
    è‡ªå‹•ç™¼ç¾ä¸¦åŠ è¼‰æ‰€æœ‰ Skills

    æŽƒæ skills/ ç›®éŒ„ä¸‹çš„æ‰€æœ‰ skill.py æ–‡ä»¶
    """
    skills_dir = Path(__file__).parent / 'skills'

    if not skills_dir.exists():
        logger.warning(f"Skills ç›®éŒ„ä¸å­˜åœ¨: {skills_dir}")
        return

    for skill_dir in skills_dir.iterdir():
        if not skill_dir.is_dir():
            continue

        skill_file = skill_dir / 'skill.py'
        if not skill_file.exists():
            continue

        # å‹•æ…‹å°Žå…¥æ¨¡çµ„
        try:
            module_name = f'webguard.skills.{skill_dir.name}.skill'
            module = importlib.import_module(module_name)

            # å°‹æ‰¾ä¸»å‡½æ•¸ï¼ˆé€šå¸¸èˆ‡ç›®éŒ„åç›¸åŒï¼‰
            skill_func = None
            for name, obj in inspect.getmembers(module):
                if inspect.isfunction(obj) and not name.startswith('_'):
                    skill_func = obj
                    break

            if skill_func:
                skill_name = skill_dir.name
                orchestrator.register(skill_name, skill_func)
                logger.info(f"å·²è¼‰å…¥ Skill: {skill_name}")

        except Exception as e:
            logger.error(f"è¼‰å…¥ Skill å¤±æ•— ({skill_dir.name}): {e}")
```

### 9.4.2 æ¸¬è©¦åŸ·è¡Œå¼•æ“Ž

```python
# webguard/execution_engine.py
from typing import Dict, Any, List
from webguard.orchestrator import ResilientOrchestrator
from webguard.models import TestResult
import asyncio
import logging

logger = logging.getLogger(__name__)


class TestExecutionEngine:
    """
    æ¸¬è©¦åŸ·è¡Œå¼•æ“Ž

    ç®¡ç†æ¸¬è©¦åŸ·è¡Œçš„ç”Ÿå‘½é€±æœŸ
    """

    def __init__(self):
        self.orchestrator = ResilientOrchestrator()

    async def execute_test_workflow(
        self,
        workflow: List[Dict[str, Any]],
        execution_id: int
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œæ¸¬è©¦å·¥ä½œæµ

        Args:
            workflow: æ¸¬è©¦å·¥ä½œæµå®šç¾©
            execution_id: åŸ·è¡Œ ID

        Returns:
            åŸ·è¡Œçµæžœæ‘˜è¦
        """
        # åŸ·è¡Œå·¥ä½œæµï¼ˆæ”¯æŒé‡è©¦ã€è£œå„Ÿï¼‰
        result = await self.orchestrator.execute_with_compensation(workflow)

        # æ”¶é›†æŒ‡æ¨™
        metrics = self.orchestrator.get_execution_summary()

        return {
            **result,
            "metrics": metrics,
            "execution_id": execution_id
        }

    async def execute_parallel_tests(
        self,
        test_configs: List[Dict[str, Any]],
        max_concurrency: int = 10
    ) -> Dict[str, Any]:
        """
        ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹æ¸¬è©¦

        Args:
            test_configs: æ¸¬è©¦é…ç½®åˆ—è¡¨
            max_concurrency: æœ€å¤§ä¸¦ç™¼æ•¸

        Returns:
            åŸ·è¡Œçµæžœ
        """
        results = await self.orchestrator.execute_parallel(
            test_configs,
            max_concurrency=max_concurrency
        )

        return results
```


## 9.5 å­˜å„²å±¤å¯¦ç¾

### 9.5.1 Redis æ•¸æ“šçµæ§‹è¨­è¨ˆ

```python
# Redis ä½¿ç”¨å ´æ™¯ï¼š
# 1. Celery æ¶ˆæ¯éšŠåˆ—
# 2. æ¸¬è©¦çµæžœå¿«å–
# 3. åˆ†å¸ƒå¼éŽ–
# 4. å¯¦æ™‚çµ±è¨ˆ

import redis
from typing import Optional, Dict, Any
import json

redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=2,  # å°ˆç”¨æ–¼ WebGuard æ•¸æ“š
    decode_responses=True
)


class RedisCache:
    """Redis å¿«å–ç®¡ç†"""

    @staticmethod
    def cache_test_result(execution_id: int, result: Dict[str, Any], ttl: int = 3600):
        """å¿«å–æ¸¬è©¦çµæžœï¼ˆ1 å°æ™‚éŽæœŸï¼‰"""
        key = f'test_result:{execution_id}'
        redis_client.setex(
            key,
            ttl,
            json.dumps(result)
        )

    @staticmethod
    def get_cached_result(execution_id: int) -> Optional[Dict[str, Any]]:
        """ç²å–å¿«å–çš„æ¸¬è©¦çµæžœ"""
        key = f'test_result:{execution_id}'
        data = redis_client.get(key)

        if data:
            return json.loads(data)
        return None

    @staticmethod
    def increment_test_counter(suite_id: int, status: str):
        """å¢žé‡çµ±è¨ˆï¼ˆå¯¦æ™‚ï¼‰"""
        key = f'stats:suite:{suite_id}:{status}'
        redis_client.incr(key)

    @staticmethod
    def get_realtime_stats(suite_id: int) -> Dict[str, int]:
        """ç²å–å¯¦æ™‚çµ±è¨ˆ"""
        keys = [
            f'stats:suite:{suite_id}:passed',
            f'stats:suite:{suite_id}:failed',
            f'stats:suite:{suite_id}:skipped'
        ]

        values = redis_client.mget(keys)

        return {
            'passed': int(values[0] or 0),
            'failed': int(values[1] or 0),
            'skipped': int(values[2] or 0)
        }
```

### 9.5.2 MinIO æ–‡ä»¶å­˜å„²

```python
# ä½¿ç”¨ MinIOï¼ˆS3 å…¼å®¹ï¼‰å­˜å„²æˆªåœ–ã€è¦–é »ã€æ—¥èªŒ
from minio import Minio
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

minio_client = Minio(
    'localhost:9000',
    access_key='minioadmin',
    secret_key='minioadmin',
    secure=False
)

# å‰µå»º bucket
bucket_name = 'webguard'
if not minio_client.bucket_exists(bucket_name):
    minio_client.make_bucket(bucket_name)


def upload_screenshot(file_path: str, test_result_id: int) -> str:
    """
    ä¸Šå‚³æˆªåœ–åˆ° MinIO

    Returns:
        å¯è¨ªå•çš„ URL
    """
    object_name = f'screenshots/{test_result_id}/{Path(file_path).name}'

    try:
        minio_client.fput_object(
            bucket_name,
            object_name,
            file_path,
            content_type='image/png'
        )

        # ç”Ÿæˆå¯è¨ªå•çš„ URLï¼ˆ7 å¤©æœ‰æ•ˆï¼‰
        url = minio_client.presigned_get_object(
            bucket_name,
            object_name,
            expires=timedelta(days=7)
        )

        logger.info(f"æˆªåœ–å·²ä¸Šå‚³: {object_name}")
        return url

    except Exception as e:
        logger.error(f"ä¸Šå‚³æˆªåœ–å¤±æ•—: {e}")
        raise
```


## 9.6 å ±å‘Šå±¤å¯¦ç¾

### 9.6.1 Prometheus æŒ‡æ¨™æ”¶é›†

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_client import start_http_server

# å®šç¾©æŒ‡æ¨™
test_executions_total = Counter(
    'webguard_test_executions_total',
    'Total number of test executions',
    ['suite_name', 'status']
)

test_duration_seconds = Histogram(
    'webguard_test_duration_seconds',
    'Test execution duration in seconds',
    ['suite_name'],
    buckets=(1, 5, 10, 30, 60, 120, 300, 600)
)

active_tests = Gauge(
    'webguard_active_tests',
    'Number of currently running tests'
)


# åœ¨æ¸¬è©¦åŸ·è¡Œæ™‚æ›´æ–°æŒ‡æ¨™
def record_test_execution(suite_name: str, status: str, duration_seconds: float):
    """è¨˜éŒ„æ¸¬è©¦åŸ·è¡ŒæŒ‡æ¨™"""
    test_executions_total.labels(suite_name=suite_name, status=status).inc()
    test_duration_seconds.labels(suite_name=suite_name).observe(duration_seconds)


# å•Ÿå‹• Prometheus metrics ä¼ºæœå™¨
start_http_server(8000)  # http://localhost:8000/metrics
```

**Prometheus é…ç½®ï¼ˆprometheus.ymlï¼‰ï¼š**

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'webguard'
    static_configs:
      - targets: ['localhost:8000']
```

### 9.6.2 Grafana å„€è¡¨æ¿

**å„€è¡¨æ¿é…ç½®ï¼ˆJSONï¼‰ï¼š**

```json
{
  "dashboard": {
    "title": "WebGuard Test Dashboard",
    "panels": [
      {
        "title": "Test Pass Rate",
        "targets": [
          {
            "expr": "sum(rate(webguard_test_executions_total{status=\"passed\"}[5m])) / sum(rate(webguard_test_executions_total[5m])) * 100"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Test Duration (P95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(webguard_test_duration_seconds_bucket[5m])) by (le, suite_name))"
          }
        ],
        "type": "graph"
      }
    ]
  }
}
```


## 9.7 API ç¶²é—œï¼ˆFastAPIï¼‰

### 9.7.1 RESTful API

```python
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional
from webguard.tasks import execute_test_suite
from webguard.database import Session
from webguard.models import TestSuite, TestExecution
import logging

logger = logging.getLogger(__name__)

app = FastAPI(title="WebGuard API", version="1.0.0")
security = HTTPBearer()


# Request/Response Models
class TestSuiteCreate(BaseModel):
    name: str
    description: Optional[str] = None
    schedule_cron: Optional[str] = None


class TestSuiteResponse(BaseModel):
    id: int
    name: str
    description: Optional[str]
    enabled: bool
    schedule_cron: Optional[str]

    class Config:
        from_attributes = True


class TestExecutionTrigger(BaseModel):
    suite_id: int
    triggered_by: str = 'api'


# ç°¡æ˜“ JWT èªè­‰ï¼ˆç”Ÿç”¢ç’°å¢ƒæ‡‰ä½¿ç”¨å®Œæ•´çš„ OAuth2ï¼‰
def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """é©—è­‰ JWT Token"""
    token = credentials.credentials

    # é€™è£¡æ‡‰è©²é©—è­‰ JWT token
    if token != "secret_token":  # ç¤ºç¯„ï¼Œå¯¦éš›æ‡‰ä½¿ç”¨ JWT åº«
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token"
        )
    return token


# API Endpoints
@app.get("/api/v1/suites", response_model=List[TestSuiteResponse])
async def list_test_suites(
    skip: int = 0,
    limit: int = 100,
    token: str = Depends(verify_token)
):
    """ç²å–æ¸¬è©¦å¥—ä»¶åˆ—è¡¨"""
    session = Session()
    try:
        suites = session.query(TestSuite).offset(skip).limit(limit).all()
        return suites
    finally:
        session.close()


@app.post("/api/v1/suites", response_model=TestSuiteResponse)
async def create_test_suite(
    suite: TestSuiteCreate,
    token: str = Depends(verify_token)
):
    """å‰µå»ºæ¸¬è©¦å¥—ä»¶"""
    session = Session()
    try:
        db_suite = TestSuite(**suite.dict())
        session.add(db_suite)
        session.commit()
        session.refresh(db_suite)
        return db_suite
    finally:
        session.close()


@app.post("/api/v1/executions/trigger")
async def trigger_test_execution(
    trigger: TestExecutionTrigger,
    token: str = Depends(verify_token)
):
    """è§¸ç™¼æ¸¬è©¦åŸ·è¡Œ"""
    # ç•°æ­¥åŸ·è¡Œï¼ˆä½¿ç”¨ Celeryï¼‰
    task = execute_test_suite.delay(
        trigger.suite_id,
        triggered_by=trigger.triggered_by
    )

    return {
        "task_id": task.id,
        "status": "queued",
        "message": f"Test suite {trigger.suite_id} execution queued"
    }


@app.get("/api/v1/executions/{execution_id}")
async def get_execution_results(
    execution_id: int,
    token: str = Depends(verify_token)
):
    """ç²å–åŸ·è¡Œçµæžœ"""
    session = Session()
    try:
        execution = session.query(TestExecution).filter_by(id=execution_id).first()

        if not execution:
            raise HTTPException(status_code=404, detail="Execution not found")

        return {
            "id": execution.id,
            "suite_id": execution.suite_id,
            "status": execution.status,
            "total_tests": execution.total_tests,
            "passed_tests": execution.passed_tests,
            "failed_tests": execution.failed_tests,
            "execution_time_ms": execution.execution_time_ms,
            "started_at": execution.started_at.isoformat(),
            "completed_at": execution.completed_at.isoformat() if execution.completed_at else None
        }
    finally:
        session.close()
```


## 9.8 éƒ¨ç½²æž¶æ§‹ï¼ˆDocker Composeï¼‰

### 9.8.1 å®Œæ•´éƒ¨ç½²é…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  # PostgreSQL æ•¸æ“šåº«
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: webguard
      POSTGRES_USER: webguard
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U webguard"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # MinIO (S3å…¼å®¹å­˜å„²)
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD}
    volumes:
      - minio_data:/data
    ports:
      - "9000:9000"
      - "9001:9001"

  # WebGuard API
  api:
    build:
      context: .
      dockerfile: Dockerfile
    command: uvicorn webguard.api:app --host 0.0.0.0 --port 8000
    environment:
      DATABASE_URL: postgresql://webguard:${POSTGRES_PASSWORD}@postgres:5432/webguard
      REDIS_URL: redis://redis:6379/0
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
    volumes:
      - ./webguard:/app/webguard
      - ./skills:/app/skills

  # Celery Worker (Browser Tests)
  celery-browser:
    build:
      context: .
      dockerfile: Dockerfile
    command: celery -A webguard.celery_config worker -Q browser -c 2 -n browser_worker@%h
    environment:
      DATABASE_URL: postgresql://webguard:${POSTGRES_PASSWORD}@postgres:5432/webguard
      REDIS_URL: redis://redis:6379/0
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
    depends_on:
      - postgres
      - redis
    volumes:
      - ./webguard:/app/webguard
      - ./skills:/app/skills

  # Celery Worker (API Tests)
  celery-api:
    build:
      context: .
      dockerfile: Dockerfile
    command: celery -A webguard.celery_config worker -Q api -c 10 -n api_worker@%h
    environment:
      DATABASE_URL: postgresql://webguard:${POSTGRES_PASSWORD}@postgres:5432/webguard
      REDIS_URL: redis://redis:6379/0
    depends_on:
      - redis

  # Celery Beat (å®šæ™‚ä»»å‹™èª¿åº¦å™¨)
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile
    command: celery -A webguard.celery_config beat --loglevel=info
    environment:
      DATABASE_URL: postgresql://webguard:${POSTGRES_PASSWORD}@postgres:5432/webguard
      REDIS_URL: redis://redis:6379/0
    depends_on:
      - redis

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"

  # Grafana
  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

volumes:
  postgres_data:
  redis_data:
  minio_data:
  prometheus_data:
  grafana_data:
```

**å•Ÿå‹•å®Œæ•´ç³»çµ±ï¼š**

```bash
# è¨­ç½®ç’°å¢ƒè®Šæ•¸
cat > .env <<EOF
POSTGRES_PASSWORD=secure_password
MINIO_PASSWORD=secure_password
GRAFANA_PASSWORD=admin_password
ANTHROPIC_API_KEY=your_api_key
EOF

# å•Ÿå‹•æ‰€æœ‰æœå‹™
docker-compose up -d

# æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f api

# åœæ­¢æ‰€æœ‰æœå‹™
docker-compose down
```


## 9.9 é«˜å¯ç”¨æ€§è¨­è¨ˆ

### 9.9.1 è² è¼‰å‡è¡¡

ä½¿ç”¨ Nginx ä½œç‚ºåå‘ä»£ç†ï¼š

```nginx
# nginx.conf
upstream webguard_api {
    least_conn;  # æœ€å°‘é€£æŽ¥æ•¸ç®—æ³•
    server api1:8000 weight=1 max_fails=3 fail_timeout=30s;
    server api2:8000 weight=1 max_fails=3 fail_timeout=30s;
    server api3:8000 weight=1 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name webguard.example.com;

    location /api/ {
        proxy_pass http://webguard_api;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

        # è¶…æ™‚è¨­ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        # å¥åº·æª¢æŸ¥ï¼ˆéœ€ nginx plus æˆ–ç¬¬ä¸‰æ–¹æ¨¡å¡Šï¼‰
        # health_check interval=10s fails=3 passes=2;
    }
}
```

### 9.9.2 æ•…éšœæ¢å¾©

**Celery ä»»å‹™é‡è©¦ï¼š**

```python
@celery_app.task(
    bind=True,
    autoretry_for=(ConnectionError, TimeoutError),
    retry_backoff=True,
    retry_backoff_max=600,  # æœ€å¤šé€€é¿åˆ° 10 åˆ†é˜
    retry_jitter=True,
    max_retries=3
)
def execute_test_suite(self, suite_id: int):
    # æ¸¬è©¦åŸ·è¡Œé‚è¼¯...
    pass
```


## 9.10 æœ¬ç« ç¸½çµ

æœ¬ç« å‘ˆç¾äº† WebGuard å®Œæ•´ç³»çµ±æž¶æ§‹ï¼š

**æ ¸å¿ƒçµ„ä»¶ï¼š**

- **å››å±¤æž¶æ§‹**ï¼šç·¨æŽ’å±¤ï¼ˆCelery/APSchedulerï¼‰ã€åŸ·è¡Œå±¤ï¼ˆSkillsï¼‰ã€å­˜å„²å±¤ï¼ˆPostgreSQL/Redis/MinIOï¼‰ã€å ±å‘Šå±¤ï¼ˆAllure/Grafanaï¼‰
- **æ•¸æ“šåº«è¨­è¨ˆ**ï¼šå®Œæ•´çš„ Schemaã€ç´¢å¼•å„ªåŒ–ã€çµ±è¨ˆè¦–åœ–
- **åˆ†å¸ƒå¼ä»»å‹™**ï¼šCelery éšŠåˆ—ã€Worker åˆ†é¡žã€ä»»å‹™è·¯ç”±
- **API ç¶²é—œ**ï¼šFastAPI RESTful APIã€JWT èªè­‰
- **ç›£æŽ§å‘Šè­¦**ï¼šPrometheus æŒ‡æ¨™ã€Grafana å„€è¡¨æ¿
- **é«˜å¯ç”¨æ€§**ï¼šè² è¼‰å‡è¡¡ã€æ•…éšœæ¢å¾©ã€æ©«å‘æ“´å±•

**ä¼æ¥­ç´šèƒ½åŠ›ï¼š**

- âœ… å–®ç¯€é»ž 50+ ä¸¦ç™¼ï¼Œé›†ç¾¤å¯æ“´å±•è‡³æ•¸ç™¾ä¸¦ç™¼
- âœ… 99.9% å¯ç”¨æ€§è¨­è¨ˆ
- âœ… å®Œæ•´çš„å¯è§€æ¸¬æ€§ï¼ˆæŒ‡æ¨™ã€æ—¥èªŒã€è¿½è¹¤ï¼‰
- âœ… ä¸€éµ Docker Compose éƒ¨ç½²
- âœ… æ”¯æŒå¤šç§Ÿæˆ¶ï¼ˆå¯æ“´å±•ï¼‰

**ä¸‹ä¸€ç« é å‘Šï¼š**

ç¬¬ 10 ç« ï¼ˆæœ€å¾Œä¸€ç« ï¼‰å°‡æŽ¢è¨Ž **ä¼æ¥­éƒ¨ç½²ã€å®‰å…¨èˆ‡ MCP ç”Ÿæ…‹**ï¼šKubernetes éƒ¨ç½²ã€å®‰å…¨æœ€ä½³å¯¦è¸ã€å¯†é‘°ç®¡ç†ã€MCP æ•´åˆã€æœªä¾†å±•æœ›ã€‚é€™å°‡å®Œæˆæ•´å€‹æŠ€è¡“æ›¸çš„æ’°å¯«ã€‚
