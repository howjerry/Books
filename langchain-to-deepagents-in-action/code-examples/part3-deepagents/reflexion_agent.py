"""
Chapter 9: è‡ªæˆ‘ä¿®æ­£æ¨¡å¼ (The Reflexion Pattern) - ç¨ç«‹ç¯„ä¾‹

Generator-Evaluator-Refiner æ¶æ§‹å¯¦ç¾
"""

import os
import re
from typing import TypedDict, Annotated, Literal
from pydantic import BaseModel, Field
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages


# ============================================================
# 1. è©•ä¼°çµæœçµæ§‹
# ============================================================

class QualityDimension(BaseModel):
    """å“è³ªç¶­åº¦è©•åˆ†"""
    name: str
    score: float = Field(ge=0, le=1)
    feedback: str


class EvaluationResult(BaseModel):
    """è©•ä¼°çµæœ"""
    overall_score: float = Field(ge=0, le=1, description="ç¶œåˆè©•åˆ† 0-1")
    dimensions: list[QualityDimension] = Field(description="å„ç¶­åº¦è©•åˆ†")
    issues: list[str] = Field(default_factory=list, description="ç™¼ç¾çš„å•é¡Œ")
    suggestions: list[str] = Field(default_factory=list, description="æ”¹é€²å»ºè­°")
    passed: bool = Field(description="æ˜¯å¦é€šéå“è³ªé–€æª»")


class Reflection(BaseModel):
    """åæ€çµæœ"""
    what_went_wrong: str = Field(description="å•é¡Œåˆ†æ")
    root_cause: str = Field(description="æ ¹æœ¬åŸå› ")
    improvement_strategy: str = Field(description="æ”¹é€²ç­–ç•¥")


# ============================================================
# 2. ç‹€æ…‹å®šç¾©
# ============================================================

class ReflexionState(TypedDict):
    messages: Annotated[list, add_messages]
    task: str
    task_type: str  # "code", "text", "analysis"
    current_output: str | None
    output_history: list[str]
    evaluation: EvaluationResult | None
    evaluation_history: list[dict]
    reflections: list[str]
    iteration: int
    max_iterations: int
    quality_threshold: float


# ============================================================
# 3. è©•ä¼°å™¨å¯¦ç¾
# ============================================================

class CodeEvaluator:
    """â€¹1â€º ç¨‹å¼ç¢¼å“è³ªè©•ä¼°å™¨"""

    def __init__(self, llm):
        self.llm = llm

    def evaluate(self, code: str, task: str) -> EvaluationResult:
        """è©•ä¼°ç¨‹å¼ç¢¼å“è³ª"""
        dimensions = []

        # â€¹2â€º èªæ³•æ­£ç¢ºæ€§æª¢æŸ¥
        syntax_score, syntax_feedback = self._check_syntax(code)
        dimensions.append(QualityDimension(
            name="èªæ³•æ­£ç¢ºæ€§",
            score=syntax_score,
            feedback=syntax_feedback
        ))

        # â€¹3â€º åŠŸèƒ½å®Œæ•´æ€§æª¢æŸ¥ï¼ˆä½¿ç”¨ LLMï¼‰
        completeness = self._check_completeness(code, task)
        dimensions.append(completeness)

        # â€¹4â€º ç¨‹å¼ç¢¼é¢¨æ ¼æª¢æŸ¥
        style = self._check_style(code)
        dimensions.append(style)

        # â€¹5â€º éŒ¯èª¤è™•ç†æª¢æŸ¥
        error_handling = self._check_error_handling(code)
        dimensions.append(error_handling)

        # è¨ˆç®—ç¶œåˆè©•åˆ†
        weights = [0.3, 0.4, 0.15, 0.15]
        overall_score = sum(
            d.score * w for d, w in zip(dimensions, weights)
        )

        # æ”¶é›†å•é¡Œå’Œå»ºè­°
        issues = []
        suggestions = []
        for dim in dimensions:
            if dim.score < 0.7:
                issues.append(f"{dim.name}: {dim.feedback}")
            if dim.score < 0.9:
                suggestions.append(f"æ”¹é€² {dim.name}")

        return EvaluationResult(
            overall_score=overall_score,
            dimensions=dimensions,
            issues=issues,
            suggestions=suggestions,
            passed=overall_score >= 0.8
        )

    def _check_syntax(self, code: str) -> tuple[float, str]:
        """æª¢æŸ¥ Python èªæ³•"""
        try:
            compile(code, "<string>", "exec")
            return 1.0, "èªæ³•æ­£ç¢º"
        except SyntaxError as e:
            return 0.0, f"èªæ³•éŒ¯èª¤ï¼š{e.msg} (è¡Œ {e.lineno})"

    def _check_completeness(self, code: str, task: str) -> QualityDimension:
        """ä½¿ç”¨ LLM æª¢æŸ¥åŠŸèƒ½å®Œæ•´æ€§"""
        prompt = f"""è©•ä¼°ä»¥ä¸‹ç¨‹å¼ç¢¼æ˜¯å¦å®Œæ•´å¯¦ç¾äº†ä»»å‹™è¦æ±‚ã€‚

ä»»å‹™ï¼š{task}

ç¨‹å¼ç¢¼ï¼š
```python
{code}
```

è«‹è©•ä¼°ï¼š
1. æ˜¯å¦å¯¦ç¾äº†æ‰€æœ‰è¦æ±‚çš„åŠŸèƒ½
2. æ˜¯å¦æœ‰éºæ¼çš„é‚Šç•Œæƒ…æ³
3. è¿”å›æ ¼å¼ï¼šåˆ†æ•¸(0-1)|è©•èª

ä¾‹å¦‚ï¼š0.8|åŸºæœ¬åŠŸèƒ½å®Œæ•´ï¼Œä½†ç¼ºå°‘ç©ºå€¼è™•ç†"""

        response = self.llm.invoke([HumanMessage(content=prompt)])
        try:
            parts = response.content.strip().split("|")
            score = float(parts[0])
            feedback = parts[1] if len(parts) > 1 else "è©•ä¼°å®Œæˆ"
        except (ValueError, IndexError):
            score = 0.7
            feedback = response.content[:100]

        return QualityDimension(
            name="åŠŸèƒ½å®Œæ•´æ€§",
            score=min(max(score, 0), 1),
            feedback=feedback
        )

    def _check_style(self, code: str) -> QualityDimension:
        """æª¢æŸ¥ç¨‹å¼ç¢¼é¢¨æ ¼"""
        issues = []

        # æª¢æŸ¥è¡Œé•·åº¦
        long_lines = [i for i, line in enumerate(code.split("\n"), 1)
                      if len(line) > 100]
        if long_lines:
            issues.append(f"è¡Œéé•·ï¼š{long_lines[:3]}")

        # æª¢æŸ¥å‘½åè¦ç¯„
        if re.search(r'\b[a-z][A-Z]', code):  # æ··åˆå‘½å
            issues.append("å»ºè­°ä½¿ç”¨ä¸€è‡´çš„å‘½åè¦ç¯„")

        # æª¢æŸ¥æ–‡æª”å­—ä¸²
        if 'def ' in code and '"""' not in code and "'''" not in code:
            issues.append("ç¼ºå°‘æ–‡æª”å­—ä¸²")

        score = max(0, 1 - len(issues) * 0.2)
        feedback = "ï¼›".join(issues) if issues else "é¢¨æ ¼è‰¯å¥½"

        return QualityDimension(
            name="ç¨‹å¼ç¢¼é¢¨æ ¼",
            score=score,
            feedback=feedback
        )

    def _check_error_handling(self, code: str) -> QualityDimension:
        """æª¢æŸ¥éŒ¯èª¤è™•ç†"""
        has_try = "try:" in code
        has_except = "except" in code
        has_validation = any(kw in code for kw in ["if not", "raise", "assert"])

        if has_try and has_except and has_validation:
            return QualityDimension(
                name="éŒ¯èª¤è™•ç†",
                score=1.0,
                feedback="æœ‰å®Œæ•´çš„éŒ¯èª¤è™•ç†"
            )
        elif has_try or has_validation:
            return QualityDimension(
                name="éŒ¯èª¤è™•ç†",
                score=0.6,
                feedback="æœ‰åŸºæœ¬éŒ¯èª¤è™•ç†ï¼Œå¯åŠ å¼·"
            )
        else:
            return QualityDimension(
                name="éŒ¯èª¤è™•ç†",
                score=0.3,
                feedback="ç¼ºå°‘éŒ¯èª¤è™•ç†æ©Ÿåˆ¶"
            )


# ============================================================
# 4. ç¯€é»å¯¦ç¾
# ============================================================

llm = ChatAnthropic(model="claude-sonnet-4-20250514", temperature=0.7)
evaluator = CodeEvaluator(ChatAnthropic(model="claude-sonnet-4-20250514", temperature=0))


def generator_node(state: ReflexionState) -> dict:
    """â€¹6â€º Generator ç¯€é»ï¼šç”Ÿæˆæˆ–æ”¹é€²è¼¸å‡º"""
    task = state["task"]
    iteration = state["iteration"]
    reflections = state["reflections"]
    current_output = state["current_output"]

    print(f"\nğŸ”„ è¿­ä»£ {iteration + 1}")

    if iteration == 0:
        # é¦–æ¬¡ç”Ÿæˆ
        system_prompt = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ Python é–‹ç™¼è€…ã€‚
è«‹æ ¹æ“šä»»å‹™è¦æ±‚ç”Ÿæˆé«˜å“è³ªçš„ç¨‹å¼ç¢¼ã€‚

è¦æ±‚ï¼š
1. ç¨‹å¼ç¢¼è¦å®Œæ•´å¯é‹è¡Œ
2. åŒ…å«é©ç•¶çš„éŒ¯èª¤è™•ç†
3. æ·»åŠ æ¸…æ™°çš„æ–‡æª”å­—ä¸²
4. éµå¾ª PEP8 é¢¨æ ¼æŒ‡å—"""

        response = llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ä»»å‹™ï¼š{task}\n\nè«‹ç”Ÿæˆç¨‹å¼ç¢¼ï¼š")
        ])
    else:
        # åŸºæ–¼åæ€æ”¹é€²
        system_prompt = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ Python é–‹ç™¼è€…ã€‚
è«‹æ ¹æ“šä¹‹å‰çš„åæ€å’Œå»ºè­°æ”¹é€²ç¨‹å¼ç¢¼ã€‚

é‡é»é—œæ³¨ä¹‹å‰è­˜åˆ¥å‡ºçš„å•é¡Œï¼Œç¢ºä¿é€™æ¬¡è§£æ±ºå®ƒå€‘ã€‚"""

        reflection_context = "\n".join([
            f"åæ€ {i+1}: {r}"
            for i, r in enumerate(reflections[-3:])  # æœ€è¿‘ 3 æ¬¡åæ€
        ])

        response = llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"""
ä»»å‹™ï¼š{task}

ä¹‹å‰çš„ç¨‹å¼ç¢¼ï¼š
```python
{current_output}
```

åæ€èˆ‡æ”¹é€²å»ºè­°ï¼š
{reflection_context}

è«‹ç”Ÿæˆæ”¹é€²å¾Œçš„ç¨‹å¼ç¢¼ï¼š
""")
        ])

    # æå–ç¨‹å¼ç¢¼
    output = response.content
    code_match = re.search(r'```python\n(.*?)\n```', output, re.DOTALL)
    if code_match:
        output = code_match.group(1)

    print(f"  ğŸ“ ç”Ÿæˆç¨‹å¼ç¢¼ï¼š{len(output)} å­—ç¬¦")

    return {
        "current_output": output,
        "output_history": [output]
    }


def evaluator_node(state: ReflexionState) -> dict:
    """â€¹7â€º Evaluator ç¯€é»ï¼šè©•ä¼°è¼¸å‡ºå“è³ª"""
    current_output = state["current_output"]
    task = state["task"]

    print("  ğŸ” è©•ä¼°å“è³ª...")

    evaluation = evaluator.evaluate(current_output, task)

    print(f"  ğŸ“Š ç¶œåˆè©•åˆ†ï¼š{evaluation.overall_score:.2f}")
    for dim in evaluation.dimensions:
        status = "âœ…" if dim.score >= 0.8 else "âš ï¸" if dim.score >= 0.5 else "âŒ"
        print(f"    {status} {dim.name}: {dim.score:.2f} - {dim.feedback}")

    if evaluation.issues:
        print(f"  âš ï¸ å•é¡Œï¼š{', '.join(evaluation.issues[:3])}")

    return {
        "evaluation": evaluation,
        "evaluation_history": [evaluation.model_dump()]
    }


def refiner_node(state: ReflexionState) -> dict:
    """â€¹8â€º Refiner ç¯€é»ï¼šåæ€ä¸¦ç”Ÿæˆæ”¹é€²ç­–ç•¥"""
    evaluation = state["evaluation"]
    current_output = state["current_output"]
    task = state["task"]

    print("  ğŸ’­ é€²è¡Œåæ€...")

    # ä½¿ç”¨ LLM é€²è¡Œæ·±åº¦åæ€
    reflection_prompt = f"""ä½œç‚ºä¸€ä½è³‡æ·±é–‹ç™¼è€…ï¼Œè«‹åæ€ä»¥ä¸‹ç¨‹å¼ç¢¼çš„å•é¡Œä¸¦æå‡ºæ”¹é€²ç­–ç•¥ã€‚

ä»»å‹™ï¼š{task}

ç¨‹å¼ç¢¼ï¼š
```python
{current_output}
```

è©•ä¼°çµæœï¼š
- ç¶œåˆè©•åˆ†ï¼š{evaluation.overall_score:.2f}
- å•é¡Œï¼š{', '.join(evaluation.issues)}
- å»ºè­°ï¼š{', '.join(evaluation.suggestions)}

è«‹é€²è¡Œåæ€ï¼Œæ ¼å¼ï¼š
1. å•é¡Œåˆ†æï¼šå…·é«”å“ªè£¡å‡ºäº†å•é¡Œ
2. æ ¹æœ¬åŸå› ï¼šç‚ºä»€éº¼æœƒå‡ºç¾é€™å€‹å•é¡Œ
3. æ”¹é€²ç­–ç•¥ï¼šä¸‹ä¸€æ¬¡è¿­ä»£æ‡‰è©²å¦‚ä½•æ”¹é€²"""

    structured_llm = llm.with_structured_output(Reflection)
    reflection = structured_llm.invoke([HumanMessage(content=reflection_prompt)])

    reflection_text = f"""
å•é¡Œï¼š{reflection.what_went_wrong}
åŸå› ï¼š{reflection.root_cause}
ç­–ç•¥ï¼š{reflection.improvement_strategy}
""".strip()

    print(f"  ğŸ“‹ åæ€ï¼š{reflection.improvement_strategy[:80]}...")

    return {
        "reflections": [reflection_text],
        "iteration": state["iteration"] + 1
    }


# ============================================================
# 5. è·¯ç”±å‡½æ•¸
# ============================================================

def should_continue(state: ReflexionState) -> Literal["refiner", "end"]:
    """â€¹9â€º æ±ºå®šæ˜¯å¦ç¹¼çºŒè¿­ä»£"""
    evaluation = state["evaluation"]
    iteration = state["iteration"]
    max_iterations = state["max_iterations"]
    quality_threshold = state["quality_threshold"]

    # é”åˆ°å“è³ªé–€æª»
    if evaluation.passed and evaluation.overall_score >= quality_threshold:
        print(f"\nâœ… å“è³ªé”æ¨™ï¼è©•åˆ†ï¼š{evaluation.overall_score:.2f}")
        return "end"

    # é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸
    if iteration >= max_iterations:
        print(f"\nâš ï¸ é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ ({max_iterations})")
        return "end"

    # ç¹¼çºŒæ”¹é€²
    print(f"  â¡ï¸ éœ€è¦æ”¹é€²ï¼Œé€²å…¥åæ€...")
    return "refiner"


# ============================================================
# 6. æ§‹å»ºåœ–
# ============================================================

def build_reflexion_graph() -> StateGraph:
    """æ§‹å»ºè‡ªæˆ‘ä¿®æ­£æ¨¡å¼åœ–"""
    graph = StateGraph(ReflexionState)

    # æ·»åŠ ç¯€é»
    graph.add_node("generator", generator_node)
    graph.add_node("evaluator", evaluator_node)
    graph.add_node("refiner", refiner_node)

    # æ·»åŠ é‚Š
    graph.add_edge(START, "generator")
    graph.add_edge("generator", "evaluator")
    graph.add_conditional_edges(
        "evaluator",
        should_continue,
        {
            "refiner": "refiner",
            "end": END
        }
    )
    graph.add_edge("refiner", "generator")

    return graph.compile()


# ============================================================
# 7. ä¸»ç¨‹å¼
# ============================================================

def main():
    """åŸ·è¡Œè‡ªæˆ‘ä¿®æ­£æ¨¡å¼ç¯„ä¾‹"""
    print("=" * 60)
    print("Chapter 9: è‡ªæˆ‘ä¿®æ­£æ¨¡å¼ (The Reflexion Pattern)")
    print("=" * 60)

    # æ§‹å»ºåœ–
    app = build_reflexion_graph()

    # æ¸¬è©¦ä»»å‹™
    tasks = [
        "å¯«ä¸€å€‹ Python å‡½æ•¸ `parse_date`ï¼Œå¯ä»¥è§£æå¤šç¨®æ—¥æœŸæ ¼å¼ï¼ˆå¦‚ '2024-01-15', 'Jan 15, 2024', '15/01/2024'ï¼‰ï¼Œè¿”å› datetime å°è±¡",
        "å¯¦ç¾ä¸€å€‹ `RateLimiter` é¡ï¼Œä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•æ§åˆ¶ API è«‹æ±‚é »ç‡ï¼Œæ”¯æŒæ¯ç§’æœ€å¤§è«‹æ±‚æ•¸é…ç½®",
    ]

    for i, task in enumerate(tasks, 1):
        print(f"\n{'='*60}")
        print(f"ä»»å‹™ {i}: {task}")
        print("=" * 60)

        initial_state = {
            "messages": [],
            "task": task,
            "task_type": "code",
            "current_output": None,
            "output_history": [],
            "evaluation": None,
            "evaluation_history": [],
            "reflections": [],
            "iteration": 0,
            "max_iterations": 3,
            "quality_threshold": 0.8
        }

        # åŸ·è¡Œåœ–
        result = app.invoke(initial_state)

        print(f"\n{'='*60}")
        print("æœ€çµ‚ç¨‹å¼ç¢¼ï¼š")
        print("=" * 60)
        print(result["current_output"])

        print(f"\nğŸ“Š è¿­ä»£çµ±è¨ˆï¼š")
        print(f"  - ç¸½è¿­ä»£æ¬¡æ•¸ï¼š{result['iteration']}")
        print(f"  - æœ€çµ‚è©•åˆ†ï¼š{result['evaluation'].overall_score:.2f}")
        print(f"  - åæ€æ¬¡æ•¸ï¼š{len(result['reflections'])}")

        if i < len(tasks):
            input("\næŒ‰ Enter ç¹¼çºŒä¸‹ä¸€å€‹ä»»å‹™...")


if __name__ == "__main__":
    main()
