# Chapter 1: å•Ÿç¨‹â€”â€”å¾ Chain åˆ° LCEL

> ã€Œæœ€å¥½çš„å­¸ç¿’æ–¹å¼æ˜¯å»ºé€ ã€‚ã€â€”â€”Seymour Papert

---

## æœ¬ç« å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

- ç†è§£ LangChain çš„æ ¸å¿ƒæŠ½è±¡èˆ‡è¨­è¨ˆå“²å­¸
- æŒæ¡ PromptTemplateã€LLM èˆ‡ OutputParser ä¸‰å¤§åŸºç¤å…ƒä»¶
- ä½¿ç”¨ LCEL (LangChain Expression Language) çµ„åˆå…ƒä»¶
- å®Œæˆ TechAssist v0.1ï¼šä¸€å€‹å¯é‹è¡Œçš„ CLI æ™ºèƒ½åŠ©ç†åŸå‹

---

## 1.1 å ´æ™¯å¼•å…¥ï¼šTechAssist çš„èª•ç”Ÿ

æƒ³åƒä½ æ˜¯ä¸€å®¶ç§‘æŠ€å…¬å¸çš„æŠ€è¡“è² è²¬äººã€‚æ¯å¤©ï¼Œä½ çš„åœ˜éšŠé¢è‡¨è‘—åŒæ¨£çš„æŒ‘æˆ°ï¼š

- æ–°é€²å·¥ç¨‹å¸«ä¸æ–·è©¢å•ã€Œé€™å€‹ API æ€éº¼ç”¨ï¼Ÿã€
- æŠ€è¡“æ–‡ä»¶æ•£è½åœ¨ Confluenceã€GitHub Wikiã€Notion å„è™•
- è³‡æ·±å·¥ç¨‹å¸«èŠ±è²»å¤§é‡æ™‚é–“å›ç­”é‡è¤‡æ€§å•é¡Œ

ä½ æ±ºå®šæ‰“é€ ä¸€å€‹å…§éƒ¨æ™ºèƒ½åŠ©ç†â€”â€”**TechAssist**ã€‚å®ƒéœ€è¦ï¼š

1. ç†è§£å·¥ç¨‹å¸«çš„è‡ªç„¶èªè¨€å•é¡Œ
2. åœ¨æŠ€è¡“æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸é—œç­”æ¡ˆ
3. ç”¨æ¸…æ™°çš„æ–¹å¼å›è¦†

é€™è½èµ·ä¾†æ˜¯å€‹ LLM çš„å®Œç¾æ‡‰ç”¨å ´æ™¯ã€‚ä½†å•é¡Œä¾†äº†ï¼š**å¦‚ä½•å¾ä¸€å€‹ç°¡å–®çš„ API èª¿ç”¨ï¼Œæ¼”é€²æˆä¸€å€‹å¯ç¶­è­·ã€å¯æ“´å±•çš„ç³»çµ±ï¼Ÿ**

é€™å°±æ˜¯æœ¬æ›¸è¦å¸¶ä½ èµ°çš„æ—…ç¨‹ã€‚è®“æˆ‘å€‘å¾æœ€åŸºç¤çš„æ§‹ä»¶é–‹å§‹ã€‚

---

## 1.2 ç‚ºä»€éº¼éœ€è¦ LangChainï¼Ÿ

### 1.2.1 åŸç”Ÿ API çš„ä¾·é™

è®“æˆ‘å€‘å…ˆçœ‹çœ‹ç›´æ¥ä½¿ç”¨ LLM API çš„æ–¹å¼ï¼š

```python
# ç›´æ¥èª¿ç”¨ Anthropic API
import anthropic

client = anthropic.Anthropic()
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "ä»€éº¼æ˜¯ Python çš„ GILï¼Ÿ"}
    ]
)
print(response.content[0].text)
```

é€™æ®µç¨‹å¼ç¢¼èƒ½é‹ä½œï¼Œä½†ç•¶ç³»çµ±è®Šè¤‡é›œæ™‚ï¼Œä½ æœƒé‡åˆ°ä»¥ä¸‹å•é¡Œï¼š

| æŒ‘æˆ° | èªªæ˜ |
|------|------|
| **Prompt ç®¡ç†** | å¦‚ä½•ç‰ˆæœ¬æ§åˆ¶ï¼Ÿå¦‚ä½•å‹•æ…‹æ’å…¥è®Šæ•¸ï¼Ÿ |
| **è¼¸å‡ºè§£æ** | å¦‚ä½•ç¢ºä¿ LLM è¼¸å‡ºç¬¦åˆé æœŸæ ¼å¼ï¼Ÿ |
| **æ¨¡å‹åˆ‡æ›** | æƒ³å¾ Claude æ›æˆ GPT-4o éœ€è¦æ”¹å¤šå°‘ç¨‹å¼ç¢¼ï¼Ÿ |
| **éˆå¼èª¿ç”¨** | å¦‚ä½•å°‡å¤šå€‹æ­¥é©Ÿçµ„åˆæˆæµç¨‹ï¼Ÿ |
| **éŒ¯èª¤è™•ç†** | å¦‚ä½•å„ªé›…åœ°è™•ç† API éŒ¯èª¤èˆ‡é‡è©¦ï¼Ÿ |

LangChain æ­£æ˜¯ç‚ºäº†è§£æ±ºé€™äº›å•é¡Œè€Œç”Ÿã€‚

### 1.2.2 LangChain çš„è¨­è¨ˆå“²å­¸

LangChain çš„æ ¸å¿ƒç†å¿µæ˜¯**çµ„åˆæ€§ (Composability)**ã€‚å®ƒå°‡ LLM æ‡‰ç”¨æ‹†è§£æˆæ¨™æº–åŒ–çš„æ§‹ä»¶ï¼Œè®“ä½ èƒ½å¤ ï¼š

```mermaid
graph LR
    A[PromptTemplate] --> B[LLM]
    B --> C[OutputParser]
    C --> D[ä¸‹ä¸€å€‹æ­¥é©Ÿ]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
```

æ¯å€‹æ§‹ä»¶éƒ½æœ‰æ˜ç¢ºçš„è·è²¬ï¼š

- **PromptTemplate**ï¼šç®¡ç†æç¤ºè©æ¨¡æ¿
- **LLM/ChatModel**ï¼šå°è£æ¨¡å‹èª¿ç”¨
- **OutputParser**ï¼šè§£æèˆ‡é©—è­‰è¼¸å‡º

é€™ç¨®è¨­è¨ˆè®“ä½ èƒ½å¤ åƒå †ç–Šæ¨‚é«˜ä¸€æ¨£çµ„åˆåŠŸèƒ½ã€‚

---

## 1.3 ç’°å¢ƒæº–å‚™

åœ¨é–‹å§‹ç·¨å¯«ç¨‹å¼ç¢¼ä¹‹å‰ï¼Œè®“æˆ‘å€‘è¨­ç½®å¥½é–‹ç™¼ç’°å¢ƒã€‚

### 1.3.1 å»ºç«‹å°ˆæ¡ˆç›®éŒ„

```bash
mkdir techassist && cd techassist
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 1.3.2 å®‰è£ä¾è³´

```bash
pip install langchain langchain-anthropic langchain-openai python-dotenv
```

### 1.3.3 è¨­å®š API Key

å»ºç«‹ `.env` æª”æ¡ˆï¼š

```bash
# .env
ANTHROPIC_API_KEY=your-api-key-here
# OPENAI_API_KEY=your-openai-key  # å‚™ç”¨
```

### 1.3.4 é©—è­‰å®‰è£

```python
# verify_setup.py
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic

load_dotenv()

llm = ChatAnthropic(model="claude-3-5-sonnet-20241022")
response = llm.invoke("èªª 'Hello, TechAssist!'")
print(response.content)
```

å¦‚æœçœ‹åˆ°å›æ‡‰ï¼Œæ­å–œä½ ï¼Œç’°å¢ƒå·²æº–å‚™å°±ç·’ï¼

---

## 1.4 æ ¸å¿ƒæ§‹ä»¶ä¸€ï¼šPromptTemplate

### 1.4.1 ç‚ºä»€éº¼éœ€è¦æ¨¡æ¿ï¼Ÿ

ç¡¬ç·¨ç¢¼çš„ prompt å­˜åœ¨å¹¾å€‹å•é¡Œï¼š

```python
# âŒ ä¸å¥½çš„åšæ³•ï¼šç¡¬ç·¨ç¢¼
prompt = f"ç”¨æˆ¶å•é¡Œï¼š{user_question}\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"
```

- é›£ä»¥è¤‡ç”¨
- é›£ä»¥æ¸¬è©¦
- é›£ä»¥ç‰ˆæœ¬æ§åˆ¶

PromptTemplate è§£æ±ºäº†é€™äº›å•é¡Œï¼š

```python
# âœ… å¥½çš„åšæ³•ï¼šä½¿ç”¨æ¨¡æ¿
from langchain_core.prompts import PromptTemplate

template = PromptTemplate.from_template(
    """ä½ æ˜¯ TechAssistï¼Œä¸€å€‹å°ˆæ¥­çš„æŠ€è¡“åŠ©ç†ã€‚

ç”¨æˆ¶å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡ã€ä»¥æ¸…æ™°æ˜“æ‡‚çš„æ–¹å¼å›ç­”ã€‚"""
)

# å‹•æ…‹å¡«å……è®Šæ•¸
formatted = template.format(question="ä»€éº¼æ˜¯ REST APIï¼Ÿ")
print(formatted)
```

### 1.4.2 ChatPromptTemplateï¼šå°è©±å ´æ™¯çš„æ¨¡æ¿

åœ¨å°è©±æ‡‰ç”¨ä¸­ï¼Œæˆ‘å€‘éœ€è¦å€åˆ†ä¸åŒè§’è‰²çš„è¨Šæ¯ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate

# â€¹1â€º å®šç¾©å¤šè§’è‰²å°è©±æ¨¡æ¿
chat_template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ TechAssistï¼Œä¸€å€‹å°ˆæ¥­çš„æŠ€è¡“åŠ©ç†ã€‚
ä½ çš„ç‰¹é»ï¼š
- å›ç­”æº–ç¢ºã€ç°¡æ½”
- ä½¿ç”¨ç¹é«”ä¸­æ–‡
- é©æ™‚æä¾›ç¨‹å¼ç¢¼ç¯„ä¾‹"""),

    ("human", "{question}")  # â€¹2â€º ä½¿ç”¨è€…è¼¸å…¥çš„ä½”ä½ç¬¦
])

# â€¹3â€º æ ¼å¼åŒ–ç‚ºè¨Šæ¯åˆ—è¡¨
messages = chat_template.format_messages(question="è§£é‡‹ Python è£é£¾å™¨")
for msg in messages:
    print(f"[{msg.type}] {msg.content[:50]}...")
```

**ç¨‹å¼ç¢¼è§£æï¼š**

- â€¹1â€º `from_messages` æ¥å— (è§’è‰², å…§å®¹) å…ƒçµ„åˆ—è¡¨
- â€¹2â€º èŠ±æ‹¬è™Ÿ `{question}` å®šç¾©è®Šæ•¸ä½”ä½ç¬¦
- â€¹3â€º `format_messages` è¿”å› `BaseMessage` ç‰©ä»¶åˆ—è¡¨ï¼Œå¯ç›´æ¥å‚³çµ¦ ChatModel

### 1.4.3 é€²éšæ¨¡æ¿ï¼šFew-Shot ç¯„ä¾‹

æœ‰æ™‚å€™ï¼Œçµ¦ LLM çœ‹å¹¾å€‹ç¯„ä¾‹æ¯”è©³ç´°æè¿°æ›´æœ‰æ•ˆï¼š

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate

# å®šç¾©ç¯„ä¾‹
examples = [
    {"input": "ä»€éº¼æ˜¯è®Šæ•¸ï¼Ÿ", "output": "è®Šæ•¸æ˜¯ç”¨ä¾†å„²å­˜è³‡æ–™çš„å®¹å™¨ã€‚åœ¨ Python ä¸­ï¼š`x = 10`"},
    {"input": "for è¿´åœˆæ€éº¼ç”¨ï¼Ÿ", "output": "for è¿´åœˆç”¨æ–¼é‡è¤‡åŸ·è¡Œç¨‹å¼ç¢¼ï¼š`for i in range(5): print(i)`"},
]

# ç¯„ä¾‹æ¨¡æ¿
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}"),
])

# Few-Shot æ¨¡æ¿
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

# çµ„åˆå®Œæ•´æ¨¡æ¿
final_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ TechAssistï¼Œè«‹åƒè€ƒä»¥ä¸‹ç¯„ä¾‹çš„å›ç­”é¢¨æ ¼ï¼š"),
    few_shot_prompt,
    ("human", "{question}"),
])
```

---

## 1.5 æ ¸å¿ƒæ§‹ä»¶äºŒï¼šLLM èˆ‡ ChatModel

### 1.5.1 LLM vs ChatModel

LangChain å€åˆ†å…©ç¨®æ¨¡å‹ä»‹é¢ï¼š

| é¡å‹ | è¼¸å…¥ | è¼¸å‡º | é©ç”¨å ´æ™¯ |
|------|------|------|----------|
| **LLM** | å­—ä¸² | å­—ä¸² | æ–‡æœ¬è£œå…¨ |
| **ChatModel** | è¨Šæ¯åˆ—è¡¨ | è¨Šæ¯ | å°è©±æ‡‰ç”¨ |

ç¾ä»£æ‡‰ç”¨å¹¾ä¹éƒ½ä½¿ç”¨ ChatModelï¼š

```python
from langchain_anthropic import ChatAnthropic

# â€¹1â€º åˆå§‹åŒ– ChatModel
llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.7,  # â€¹2â€º æ§åˆ¶å‰µæ„ç¨‹åº¦
    max_tokens=1024,
)

# â€¹3â€º ç›´æ¥èª¿ç”¨
response = llm.invoke("ä»€éº¼æ˜¯ Dockerï¼Ÿ")
print(response.content)
```

**åƒæ•¸èªªæ˜ï¼š**

- â€¹1â€º `ChatAnthropic` å°è£äº† Anthropic API
- â€¹2â€º `temperature`ï¼š0 = ç¢ºå®šæ€§è¼¸å‡ºï¼Œ1 = æ›´æœ‰å‰µæ„
- â€¹3â€º `invoke()` æ˜¯ LangChain çš„æ¨™æº–èª¿ç”¨æ–¹æ³•

### 1.5.2 æ¨¡å‹åˆ‡æ›çš„å„ªé›…æ–¹å¼

LangChain çš„æŠ½è±¡è®“æ¨¡å‹åˆ‡æ›è®Šå¾—ç°¡å–®ï¼š

```python
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

def get_llm(provider: str = "anthropic"):
    """å·¥å» å‡½æ•¸ï¼šæ ¹æ“šé…ç½®è¿”å›å°æ‡‰çš„ LLM"""
    if provider == "anthropic":
        return ChatAnthropic(model="claude-3-5-sonnet-20241022")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o")
    else:
        raise ValueError(f"æœªæ”¯æ´çš„ provider: {provider}")

# ä½¿ç”¨
llm = get_llm("anthropic")
response = llm.invoke("Hello!")
```

é€™ç¨®è¨­è¨ˆè®“ä½ èƒ½å¤ ï¼š

- åœ¨ä¸åŒç’°å¢ƒä½¿ç”¨ä¸åŒæ¨¡å‹ï¼ˆé–‹ç™¼ç”¨ä¾¿å®œçš„ï¼Œç”Ÿç”¢ç”¨å¼·å¤§çš„ï¼‰
- å¯¦ç¾ Fallback æ©Ÿåˆ¶ï¼ˆä¸»æ¨¡å‹å¤±æ•—æ™‚åˆ‡æ›å‚™ç”¨ï¼‰
- A/B æ¸¬è©¦ä¸åŒæ¨¡å‹çš„æ•ˆæœ

---

## 1.6 æ ¸å¿ƒæ§‹ä»¶ä¸‰ï¼šOutputParser

### 1.6.1 ç‚ºä»€éº¼éœ€è¦è§£æè¼¸å‡ºï¼Ÿ

LLM çš„è¼¸å‡ºæ˜¯è‡ªç”±æ–‡æœ¬ï¼Œä½†æˆ‘å€‘çš„ç¨‹å¼éœ€è¦çµæ§‹åŒ–è³‡æ–™ï¼š

```python
# LLM å›ç­”ï¼š"Python æ˜¯ä¸€ç¨®ç¨‹å¼èªè¨€ï¼Œç”± Guido van Rossum æ–¼ 1991 å¹´å‰µå»º..."
# æˆ‘å€‘éœ€è¦ï¼š{"language": "Python", "creator": "Guido van Rossum", "year": 1991}
```

OutputParser è² è²¬å°‡è‡ªç”±æ–‡æœ¬è½‰æ›æˆç¨‹å¼å¯ç”¨çš„æ ¼å¼ã€‚

### 1.6.2 StrOutputParserï¼šæœ€ç°¡å–®çš„è§£æå™¨

```python
from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()

# å¾ AIMessage æå–ç´”æ–‡å­—
from langchain_core.messages import AIMessage
message = AIMessage(content="é€™æ˜¯å›ç­”å…§å®¹")
result = parser.invoke(message)
print(result)  # "é€™æ˜¯å›ç­”å…§å®¹"
print(type(result))  # <class 'str'>
```

### 1.6.3 PydanticOutputParserï¼šçµæ§‹åŒ–è¼¸å‡º

ç•¶ä½ éœ€è¦çµæ§‹åŒ–è³‡æ–™æ™‚ï¼ŒPydantic è§£æå™¨æ˜¯æœ€ä½³é¸æ“‡ï¼š

```python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# â€¹1â€º å®šç¾©è¼¸å‡ºçµæ§‹
class TechAnswer(BaseModel):
    summary: str = Field(description="ç°¡çŸ­æ‘˜è¦ï¼Œä¸è¶…é 50 å­—")
    explanation: str = Field(description="è©³ç´°è§£é‡‹")
    code_example: str | None = Field(description="ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼ˆå¦‚é©ç”¨ï¼‰")
    difficulty: str = Field(description="é›£åº¦ï¼šåˆç´š/ä¸­ç´š/é«˜ç´š")

# â€¹2â€º å»ºç«‹è§£æå™¨
parser = PydanticOutputParser(pydantic_object=TechAnswer)

# â€¹3â€º ç²å–æ ¼å¼èªªæ˜ï¼ˆç”¨æ–¼ promptï¼‰
print(parser.get_format_instructions())
```

è¼¸å‡ºçš„æ ¼å¼èªªæ˜æœƒæŒ‡å° LLM è¼¸å‡ºæ­£ç¢ºçš„ JSON æ ¼å¼ã€‚

### 1.6.4 æ•´åˆè§£æå™¨åˆ° Prompt

```python
from langchain_core.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ TechAssist æŠ€è¡“åŠ©ç†ã€‚
è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

{format_instructions}"""),
    ("human", "{question}")
])

# å°‡æ ¼å¼èªªæ˜æ³¨å…¥æ¨¡æ¿
prompt_with_parser = template.partial(
    format_instructions=parser.get_format_instructions()
)
```

---

## 1.7 LCELï¼šçµ„åˆçš„è—è¡“

### 1.7.1 ä»€éº¼æ˜¯ LCELï¼Ÿ

**LCEL (LangChain Expression Language)** æ˜¯ LangChain çš„è²æ˜å¼çµ„åˆèªæ³•ã€‚å®ƒä½¿ç”¨ç®¡é“é‹ç®—ç¬¦ `|` å°‡å…ƒä»¶ä¸²é€£ï¼š

```python
chain = prompt | llm | parser
```

é€™è¡Œç¨‹å¼ç¢¼è¡¨ç¤ºï¼šå°‡ prompt çš„è¼¸å‡ºå‚³çµ¦ llmï¼Œå†å°‡ llm çš„è¼¸å‡ºå‚³çµ¦ parserã€‚

### 1.7.2 LCEL çš„å„ªå‹¢

| ç‰¹æ€§ | èªªæ˜ |
|------|------|
| **å¯è®€æ€§** | è³‡æ–™æµæ¸…æ™°å¯è¦‹ |
| **æ¨™æº–ä»‹é¢** | æ‰€æœ‰å…ƒä»¶éƒ½æ”¯æ´ `invoke()`, `stream()`, `batch()` |
| **è‡ªå‹•ä¸²æµ** | æ”¯æ´é€ token è¼¸å‡º |
| **ä¸¦è¡Œè™•ç†** | å¯æ‰¹æ¬¡è™•ç†å¤šå€‹è«‹æ±‚ |
| **è¿½è¹¤æ•´åˆ** | è‡ªå‹•æ”¯æ´ LangSmith è¿½è¹¤ |

### 1.7.3 ç¬¬ä¸€å€‹ LCEL Chain

è®“æˆ‘å€‘å°‡å‰é¢å­¸åˆ°çš„å…ƒä»¶çµ„åˆèµ·ä¾†ï¼š

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# â€¹1â€º å®šç¾©å…ƒä»¶
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ TechAssistï¼Œå°ˆæ¥­çš„æŠ€è¡“åŠ©ç†ã€‚ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"),
    ("human", "{question}")
])

llm = ChatAnthropic(model="claude-3-5-sonnet-20241022")

parser = StrOutputParser()

# â€¹2â€º ä½¿ç”¨ LCEL çµ„åˆ
chain = prompt | llm | parser

# â€¹3â€º èª¿ç”¨ Chain
answer = chain.invoke({"question": "ä»€éº¼æ˜¯ APIï¼Ÿ"})
print(answer)
```

**åŸ·è¡Œæµç¨‹ï¼š**

1. `prompt` æ¥æ”¶ `{"question": "ä»€éº¼æ˜¯ APIï¼Ÿ"}`ï¼Œè¼¸å‡ºæ ¼å¼åŒ–çš„è¨Šæ¯åˆ—è¡¨
2. `llm` æ¥æ”¶è¨Šæ¯åˆ—è¡¨ï¼Œè¼¸å‡º `AIMessage`
3. `parser` æ¥æ”¶ `AIMessage`ï¼Œè¼¸å‡ºç´”æ–‡å­—å­—ä¸²

### 1.7.4 ä¸²æµè¼¸å‡º

LCEL å…§å»ºä¸²æµæ”¯æ´ï¼Œè®“ä½¿ç”¨è€…é«”é©—æ›´å¥½ï¼š

```python
# ä¸²æµè¼¸å‡ºï¼ˆé€ tokenï¼‰
for chunk in chain.stream({"question": "è§£é‡‹ç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆ"}):
    print(chunk, end="", flush=True)
```

### 1.7.5 æ‰¹æ¬¡è™•ç†

ç•¶ä½ æœ‰å¤šå€‹å•é¡Œéœ€è¦è™•ç†ï¼š

```python
questions = [
    {"question": "ä»€éº¼æ˜¯ REST APIï¼Ÿ"},
    {"question": "ä»€éº¼æ˜¯ GraphQLï¼Ÿ"},
    {"question": "REST å’Œ GraphQL çš„å·®ç•°ï¼Ÿ"},
]

# æ‰¹æ¬¡èª¿ç”¨ï¼ˆè‡ªå‹•ä¸¦è¡Œï¼‰
answers = chain.batch(questions)
for q, a in zip(questions, answers):
    print(f"Q: {q['question']}\nA: {a[:100]}...\n")
```

---

## 1.8 å¯¦ä½œï¼šTechAssist v0.1

ç¾åœ¨ï¼Œè®“æˆ‘å€‘æŠŠæ‰€æœ‰å­¸åˆ°çš„çŸ¥è­˜æ•´åˆï¼Œæ‰“é€  TechAssist çš„ç¬¬ä¸€å€‹ç‰ˆæœ¬ã€‚

### 1.8.1 å°ˆæ¡ˆçµæ§‹

```
techassist/
â”œâ”€â”€ .env                    # API Keys
â”œâ”€â”€ requirements.txt        # ä¾è³´
â”œâ”€â”€ techassist/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ prompts.py         # Prompt æ¨¡æ¿
â”‚   â”œâ”€â”€ chains.py          # Chain å®šç¾©
â”‚   â””â”€â”€ cli.py             # CLI ä»‹é¢
â””â”€â”€ main.py                # å…¥å£
```

### 1.8.2 é…ç½®ç®¡ç†

```python
# techassist/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    """æ‡‰ç”¨é…ç½®"""
    anthropic_api_key: str
    model_name: str = "claude-3-5-sonnet-20241022"
    temperature: float = 0.7
    max_tokens: int = 1024

    class Config:
        env_file = ".env"

settings = Settings()
```

### 1.8.3 Prompt æ¨¡æ¿

```python
# techassist/prompts.py
from langchain_core.prompts import ChatPromptTemplate

# TechAssist ç³»çµ± Prompt
SYSTEM_PROMPT = """ä½ æ˜¯ TechAssistï¼Œä¸€å€‹å°ˆæ¥­çš„æŠ€è¡“åŠ©ç†ã€‚

## ä½ çš„ç‰¹é»
- ç²¾é€šå„ç¨®ç¨‹å¼èªè¨€å’ŒæŠ€è¡“æ¦‚å¿µ
- å›ç­”æº–ç¢ºã€ç°¡æ½”ã€å¯¦ç”¨
- ä½¿ç”¨ç¹é«”ä¸­æ–‡
- é©æ™‚æä¾›ç¨‹å¼ç¢¼ç¯„ä¾‹

## å›ç­”åŸå‰‡
1. å…ˆçµ¦å‡ºç°¡çŸ­çš„ç›´æ¥å›ç­”
2. å†æä¾›å¿…è¦çš„è©³ç´°è§£é‡‹
3. å¦‚æœ‰ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼Œç¢ºä¿å¯ç›´æ¥é‹è¡Œ
4. å¦‚æœä¸ç¢ºå®šï¼Œèª å¯¦èªªæ˜

## æ ¼å¼è¦æ±‚
- ä½¿ç”¨ Markdown æ ¼å¼
- ç¨‹å¼ç¢¼ä½¿ç”¨èªæ³•é«˜äº®æ¨™è¨˜"""

def get_qa_prompt() -> ChatPromptTemplate:
    """å–å¾—å•ç­” Prompt æ¨¡æ¿"""
    return ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        ("human", "{question}")
    ])
```

### 1.8.4 Chain å®šç¾©

```python
# techassist/chains.py
from langchain_anthropic import ChatAnthropic
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from .config import settings
from .prompts import get_qa_prompt

def create_qa_chain():
    """å»ºç«‹å•ç­” Chain

    Returns:
        ä¸€å€‹æ¥å— question ä¸¦è¿”å›å›ç­”çš„ Chain
    """
    # â€¹1â€º åˆå§‹åŒ– LLM
    llm = ChatAnthropic(
        model=settings.model_name,
        temperature=settings.temperature,
        max_tokens=settings.max_tokens,
    )

    # â€¹2â€º çµ„åˆ Chain
    chain = (
        get_qa_prompt()  # Prompt æ¨¡æ¿
        | llm            # LLM èª¿ç”¨
        | StrOutputParser()  # è¼¸å‡ºè§£æ
    )

    return chain
```

### 1.8.5 CLI ä»‹é¢

```python
# techassist/cli.py
import sys
from .chains import create_qa_chain

def run_cli():
    """åŸ·è¡Œ CLI äº’å‹•ä»‹é¢"""
    print("=" * 50)
    print("ğŸ¤– TechAssist v0.1 - æŠ€è¡“åŠ©ç†")
    print("=" * 50)
    print("è¼¸å…¥æŠ€è¡“å•é¡Œï¼Œæˆ‘æœƒç‚ºä½ è§£ç­”ã€‚")
    print("è¼¸å…¥ 'quit' æˆ– 'exit' é›¢é–‹ã€‚")
    print("-" * 50)

    chain = create_qa_chain()

    while True:
        try:
            question = input("\nğŸ“ ä½ çš„å•é¡Œï¼š").strip()

            if not question:
                continue

            if question.lower() in ('quit', 'exit', 'q'):
                print("\nğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ TechAssistï¼Œå†è¦‹ï¼")
                break

            print("\nğŸ’­ æ€è€ƒä¸­...\n")

            # ä¸²æµè¼¸å‡º
            print("ğŸ“– å›ç­”ï¼š")
            for chunk in chain.stream({"question": question}):
                print(chunk, end="", flush=True)
            print("\n")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ TechAssistï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    run_cli()
```

### 1.8.6 ä¸»ç¨‹å¼å…¥å£

```python
# main.py
from dotenv import load_dotenv

def main():
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    load_dotenv()

    # å•Ÿå‹• CLI
    from techassist.cli import run_cli
    run_cli()

if __name__ == "__main__":
    main()
```

### 1.8.7 åŸ·è¡Œæ¸¬è©¦

```bash
# å®‰è£ä¾è³´
pip install -r requirements.txt

# åŸ·è¡Œ
python main.py
```

æ¸¬è©¦å°è©±ç¯„ä¾‹ï¼š

```
ğŸ“ ä½ çš„å•é¡Œï¼šä»€éº¼æ˜¯ Python çš„åˆ—è¡¨æ¨å°å¼ï¼Ÿ

ğŸ’­ æ€è€ƒä¸­...

ğŸ“– å›ç­”ï¼š
åˆ—è¡¨æ¨å°å¼ï¼ˆList Comprehensionï¼‰æ˜¯ Python ä¸­ä¸€ç¨®ç°¡æ½”å„ªé›…çš„èªæ³•ï¼Œ
ç”¨æ–¼å¿«é€Ÿå»ºç«‹åˆ—è¡¨ã€‚

## åŸºæœ¬èªæ³•
```python
[expression for item in iterable if condition]
```

## ç¯„ä¾‹
```python
# å»ºç«‹ 1-10 çš„å¹³æ–¹æ•¸åˆ—è¡¨
squares = [x**2 for x in range(1, 11)]
# çµæœï¼š[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]

# ç¯©é¸å¶æ•¸
evens = [x for x in range(10) if x % 2 == 0]
# çµæœï¼š[0, 2, 4, 6, 8]
```

ç›¸æ¯”å‚³çµ± for è¿´åœˆï¼Œåˆ—è¡¨æ¨å°å¼æ›´ç°¡æ½”ï¼Œé€šå¸¸ä¹Ÿæ›´å¿«ã€‚
```

---

## 1.9 æœ¬ç« å›é¡§

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘å€‘å­¸ç¿’äº†ï¼š

### æ ¸å¿ƒæ¦‚å¿µ

| å…ƒä»¶ | è·è²¬ | é—œéµæ–¹æ³• |
|------|------|----------|
| **PromptTemplate** | ç®¡ç†æç¤ºè©æ¨¡æ¿ | `format()`, `format_messages()` |
| **ChatModel** | å°è£ LLM èª¿ç”¨ | `invoke()`, `stream()`, `batch()` |
| **OutputParser** | è§£æè¼¸å‡ºæ ¼å¼ | `invoke()`, `get_format_instructions()` |
| **LCEL** | çµ„åˆå…ƒä»¶ | `\|` (ç®¡é“é‹ç®—ç¬¦) |

### è¨­è¨ˆåŸå‰‡

1. **çµ„åˆå„ªæ–¼ç¹¼æ‰¿**ï¼šä½¿ç”¨ LCEL å°‡å°å…ƒä»¶çµ„åˆæˆå¤§åŠŸèƒ½
2. **é—œæ³¨é»åˆ†é›¢**ï¼šæ¯å€‹å…ƒä»¶åªåšä¸€ä»¶äº‹
3. **å¯æ¸¬è©¦æ€§**ï¼šæ¯å€‹å…ƒä»¶å¯ä»¥ç¨ç«‹æ¸¬è©¦

### TechAssist é‡Œç¨‹ç¢‘

- âœ… v0.1ï¼šåŸºæ–¼ Chain çš„ç°¡å–®å•ç­”æ©Ÿå™¨äºº

---

## 1.10 ä¸‹ä¸€ç« é å‘Š

TechAssist v0.1 èƒ½å›ç­”å•é¡Œï¼Œä½†å®ƒæœ‰æ˜é¡¯çš„ä¸è¶³ï¼š

- ç„¡æ³•æ§åˆ¶è¼¸å‡ºæ ¼å¼ï¼ˆæœ‰æ™‚å¤ªé•·ï¼Œæœ‰æ™‚å¤ªçŸ­ï¼‰
- ç„¡æ³•åˆ†é¡ä½¿ç”¨è€…æ„åœ–ï¼ˆæ˜¯æŠ€è¡“å•é¡Œï¼Ÿé‚„æ˜¯é–’èŠï¼Ÿï¼‰
- è¼¸å‡ºçµæ§‹åŒ–ç¨‹åº¦ä¸å¤ ï¼ˆé›£ä»¥è¢«å…¶ä»–ç³»çµ±æ¶ˆè²»ï¼‰

åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘å€‘å°‡æ·±å…¥ **Prompt å·¥ç¨‹èˆ‡çµæ§‹åŒ–è¼¸å‡º**ï¼Œå­¸ç¿’å¦‚ä½•ï¼š

- ä½¿ç”¨é€²éš Prompt æŠ€å·§ï¼ˆChain-of-Thought, Few-Shotï¼‰
- ç”¨ Pydantic å¼·åˆ¶ LLM è¼¸å‡ºçµæ§‹åŒ–è³‡æ–™
- å»ºç«‹æ„åœ–åˆ†é¡å™¨ï¼Œè®“ TechAssist æ›´è°æ˜

---

## ç·´ç¿’é¡Œ

1. **åŸºç¤ç·´ç¿’**ï¼šä¿®æ”¹ TechAssist çš„ç³»çµ± Promptï¼Œè®“å®ƒå°ˆæ³¨æ–¼ Python é ˜åŸŸçš„å•é¡Œã€‚

2. **é€²éšç·´ç¿’**ï¼šæ–°å¢ä¸€å€‹ `/help` æŒ‡ä»¤ï¼Œç•¶ä½¿ç”¨è€…è¼¸å…¥æ™‚ï¼Œé¡¯ç¤ºå¯ç”¨æŒ‡ä»¤åˆ—è¡¨ã€‚

3. **æŒ‘æˆ°ç·´ç¿’**ï¼šå¯¦ä½œå°è©±æ­·å²åŠŸèƒ½ï¼Œè®“ TechAssist èƒ½è¨˜ä½ä¹‹å‰çš„å°è©±ï¼ˆæç¤ºï¼šä¿®æ”¹ Prompt æ¨¡æ¿ï¼ŒåŠ å…¥ `MessagesPlaceholder`ï¼‰ã€‚

---

## å»¶ä¼¸é–±è®€

- [LangChain å®˜æ–¹æ–‡ä»¶ï¼šLCEL](https://python.langchain.com/docs/expression_language/)
- [Anthropic Claude API æ–‡ä»¶](https://docs.anthropic.com/)
- [Pydantic å®˜æ–¹æ–‡ä»¶](https://docs.pydantic.dev/)
