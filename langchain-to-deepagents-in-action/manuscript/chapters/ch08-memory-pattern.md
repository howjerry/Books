# Chapter 8: è¨˜æ†¶æ¨¡å¼ (The Memory Pattern)

> ã€Œè¨˜æ†¶ä¸æ˜¯éå»çš„å®¹å™¨ï¼Œè€Œæ˜¯æœªä¾†çš„æŒ‡å—ã€‚ã€

---

## æœ¬ç« å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

- ç†è§£ Agent è¨˜æ†¶ç³»çµ±çš„æ¶æ§‹
- å€åˆ†çŸ­æœŸè¨˜æ†¶èˆ‡é•·æœŸè¨˜æ†¶çš„ä½¿ç”¨å ´æ™¯
- å¯¦ç¾ Snapshot Pattern é€²è¡Œç‹€æ…‹å›æº¯
- å¯¦ç¾ Semantic Injection å‹•æ…‹æ³¨å…¥ç›¸é—œè¨˜æ†¶
- å®Œæˆ TechAssist v0.9ï¼šå…·å‚™é•·æœŸè¨˜æ†¶çš„åŠ©ç†

---

## 8.1 å ´æ™¯å¼•å…¥ï¼šè¨˜æ†¶çš„åƒ¹å€¼

æƒ³åƒé€™å€‹å°è©±ï¼š

```
ç”¨æˆ¶ï¼ˆé€±ä¸€ï¼‰ï¼šæˆ‘å–œæ­¡ç”¨ TypeScriptï¼Œä¸å–œæ­¡ JavaScript
TechAssistï¼šå¥½çš„ï¼Œæˆ‘è¨˜ä½äº†ï¼

ç”¨æˆ¶ï¼ˆé€±ä¸‰ï¼‰ï¼šå¹«æˆ‘å¯«ä¸€å€‹å‰ç«¯çµ„ä»¶
TechAssistï¼šå¥½çš„ï¼Œé€™æ˜¯ JavaScript ç¨‹å¼ç¢¼...

ç”¨æˆ¶ï¼šğŸ˜¤ æˆ‘èªªéæˆ‘å–œæ­¡ TypeScriptï¼
```

å•é¡Œåœ¨æ–¼ï¼š**TechAssist æ²’æœ‰è¨˜æ†¶**ã€‚

### 8.1.1 è¨˜æ†¶çš„é¡å‹

äººé¡çš„è¨˜æ†¶ç³»çµ±ï¼š

| é¡å‹ | æŒçºŒæ™‚é–“ | å®¹é‡ | ä¾‹å­ |
|------|----------|------|------|
| **æ„Ÿè¦ºè¨˜æ†¶** | æ¯«ç§’ | å¤§ | å‰›çœ‹åˆ°çš„ç•«é¢ |
| **çŸ­æœŸè¨˜æ†¶** | ç§’-åˆ†é˜ | å°ï¼ˆ7Â±2 é …ï¼‰ | æ­£åœ¨æ€è€ƒçš„å…§å®¹ |
| **é•·æœŸè¨˜æ†¶** | æ°¸ä¹… | ç„¡é™ | å­¸éçš„çŸ¥è­˜ã€ç¶“æ­· |

Agent çš„è¨˜æ†¶ç³»çµ±ï¼š

| é¡å‹ | å¯¦ç¾ | ç”¨é€” |
|------|------|------|
| **å³æ™‚è¨˜æ†¶** | ç•¶å‰å°è©± messages | ç¶­æŒå°è©±é€£è²« |
| **æœƒè©±è¨˜æ†¶** | Checkpointer | å–®æ¬¡æœƒè©±çš„ç‹€æ…‹ |
| **é•·æœŸè¨˜æ†¶** | å‘é‡è³‡æ–™åº« | è·¨æœƒè©±çš„çŸ¥è­˜ |
| **ç¨‹åºè¨˜æ†¶** | ä¿å­˜çš„è¨ˆåŠƒ/æ¨¡æ¿ | å­¸ç¿’åˆ°çš„æŠ€èƒ½ |

### 8.1.2 ç‚ºä»€éº¼ LLM éœ€è¦è¨˜æ†¶ç³»çµ±ï¼Ÿ

LLM æœ¬èº«çš„é™åˆ¶ï¼š

```mermaid
graph LR
    A[è¼¸å…¥ Token] --> B[LLM]
    B --> C[è¼¸å‡º Token]

    style B fill:#f9f,stroke:#333
```

- **ç„¡ç‹€æ…‹**ï¼šæ¯æ¬¡èª¿ç”¨éƒ½æ˜¯ç¨ç«‹çš„
- **ä¸Šä¸‹æ–‡çª—å£æœ‰é™**ï¼šClaude 128K tokensï¼Œä½†ä»æœ‰é™
- **ç„¡æŒä¹…åŒ–**ï¼šå°è©±çµæŸå¾Œå…¨éƒ¨éºå¿˜

è¨˜æ†¶ç³»çµ±çš„ä½œç”¨ï¼š

```mermaid
graph TB
    subgraph "è¨˜æ†¶ç³»çµ±"
        M1[çŸ­æœŸè¨˜æ†¶]
        M2[é•·æœŸè¨˜æ†¶]
        M3[ç¨‹åºè¨˜æ†¶]
    end

    A[è¼¸å…¥] --> R[è¨˜æ†¶æª¢ç´¢]
    R --> M1
    R --> M2
    R --> M3
    M1 --> E[å¢å¼·çš„ä¸Šä¸‹æ–‡]
    M2 --> E
    M3 --> E
    E --> L[LLM]
    L --> O[è¼¸å‡º]
    O --> S[è¨˜æ†¶å­˜å„²]
    S --> M1
    S --> M2
```

---

## 8.2 çŸ­æœŸè¨˜æ†¶ï¼šå°è©±ä¸Šä¸‹æ–‡

### 8.2.1 ä½¿ç”¨ LangGraph çš„ Messages

æœ€åŸºæœ¬çš„çŸ­æœŸè¨˜æ†¶ï¼š

```python
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages

class ConversationState(TypedDict):
    """å°è©±ç‹€æ…‹"""
    messages: Annotated[list, add_messages]

# add_messages ç¢ºä¿æ–°è¨Šæ¯è¿½åŠ è€Œéè¦†è“‹
```

### 8.2.2 å°è©±æ­·å²ç®¡ç†

ç•¶å°è©±éé•·æ™‚ï¼Œéœ€è¦ç®¡ç†æ­·å²ï¼š

```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

def trim_messages(messages: list, max_tokens: int = 4000) -> list:
    """ä¿®å‰ªè¨Šæ¯æ­·å²ï¼Œä¿æŒåœ¨ token é™åˆ¶å…§

    ç­–ç•¥ï¼š
    1. ä¿ç•™ system message
    2. ä¿ç•™æœ€è¿‘çš„è¨Šæ¯
    3. å¦‚æœ‰å¿…è¦ï¼Œæ‘˜è¦èˆŠè¨Šæ¯
    """
    # ç°¡åŒ–çš„ token è¨ˆç®—ï¼ˆå¯¦éš›æ‡‰ä½¿ç”¨ tiktokenï¼‰
    def estimate_tokens(msg) -> int:
        return len(msg.content) // 4

    # åˆ†é›¢ç³»çµ±è¨Šæ¯å’Œå°è©±è¨Šæ¯
    system_msgs = [m for m in messages if isinstance(m, SystemMessage)]
    chat_msgs = [m for m in messages if not isinstance(m, SystemMessage)]

    # è¨ˆç®—ç³»çµ±è¨Šæ¯çš„ token
    system_tokens = sum(estimate_tokens(m) for m in system_msgs)
    available_tokens = max_tokens - system_tokens

    # å¾æœ€æ–°é–‹å§‹ä¿ç•™
    kept_msgs = []
    current_tokens = 0

    for msg in reversed(chat_msgs):
        msg_tokens = estimate_tokens(msg)
        if current_tokens + msg_tokens > available_tokens:
            break
        kept_msgs.insert(0, msg)
        current_tokens += msg_tokens

    return system_msgs + kept_msgs


def summarize_old_messages(messages: list, llm) -> str:
    """æ‘˜è¦èˆŠçš„å°è©±å…§å®¹"""
    summary_prompt = """è«‹æ‘˜è¦ä»¥ä¸‹å°è©±çš„é—œéµè³‡è¨Šï¼š

{conversation}

æ‘˜è¦æ‡‰åŒ…å«ï¼š
1. è¨è«–çš„ä¸»è¦è©±é¡Œ
2. ç”¨æˆ¶çš„åå¥½æˆ–éœ€æ±‚
3. å·²ç¶“å®Œæˆçš„ä»»å‹™
4. å¾…è™•ç†çš„äº‹é …

è«‹ç”¨ 3-5 å¥è©±æ‘˜è¦ã€‚"""

    conversation = "\n".join([
        f"{'ç”¨æˆ¶' if isinstance(m, HumanMessage) else 'AI'}: {m.content}"
        for m in messages
    ])

    response = llm.invoke(summary_prompt.format(conversation=conversation))
    return response.content
```

### 8.2.3 æ»‘å‹•çª—å£æ¨¡å¼

```python
class SlidingWindowMemory:
    """æ»‘å‹•çª—å£è¨˜æ†¶ç®¡ç†"""

    def __init__(self, window_size: int = 10):
        self.window_size = window_size
        self.messages = []

    def add(self, message):
        """æ·»åŠ è¨Šæ¯"""
        self.messages.append(message)
        # ä¿æŒçª—å£å¤§å°
        if len(self.messages) > self.window_size:
            self.messages = self.messages[-self.window_size:]

    def get_context(self) -> list:
        """ç²å–ç•¶å‰ä¸Šä¸‹æ–‡"""
        return self.messages.copy()

    def clear(self):
        """æ¸…é™¤è¨˜æ†¶"""
        self.messages = []
```

---

## 8.3 æœƒè©±è¨˜æ†¶ï¼šCheckpointer

### 8.3.1 Snapshot Pattern

Checkpointer å¯¦ç¾äº†ç‹€æ…‹å¿«ç…§ï¼Œå…è¨±ï¼š
- æ–·é»æ¢å¾©
- ç‹€æ…‹å›æº¯
- åˆ†æ”¯æ¢ç´¢

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver

# è¨˜æ†¶é«” Checkpointerï¼ˆé–‹ç™¼ç”¨ï¼‰
memory_saver = MemorySaver()

# SQLite Checkpointerï¼ˆç”Ÿç”¢ç”¨ï¼‰
import sqlite3
conn = sqlite3.connect("memory.db", check_same_thread=False)
sqlite_saver = SqliteSaver(conn)

# ç·¨è­¯æ™‚æŒ‡å®š
app = graph.compile(checkpointer=sqlite_saver)
```

### 8.3.2 å¤šæœƒè©±ç®¡ç†

```python
class SessionManager:
    """æœƒè©±ç®¡ç†å™¨"""

    def __init__(self, app, checkpointer):
        self.app = app
        self.checkpointer = checkpointer

    def create_session(self, user_id: str) -> str:
        """å‰µå»ºæ–°æœƒè©±"""
        session_id = f"{user_id}-{int(time.time())}"
        return session_id

    def get_config(self, session_id: str) -> dict:
        """ç²å–æœƒè©±é…ç½®"""
        return {"configurable": {"thread_id": session_id}}

    def list_sessions(self, user_id: str) -> list[str]:
        """åˆ—å‡ºç”¨æˆ¶çš„æ‰€æœ‰æœƒè©±"""
        # å¯¦ç¾ä¾è³´æ–¼ checkpointer çš„èƒ½åŠ›
        pass

    def resume_session(self, session_id: str, new_input: str):
        """æ¢å¾©ä¸¦ç¹¼çºŒæœƒè©±"""
        config = self.get_config(session_id)

        # ç²å–ä¹‹å‰çš„ç‹€æ…‹
        snapshot = self.app.get_state(config)

        if snapshot.values:
            # æœ‰æ­·å²ç‹€æ…‹ï¼Œç¹¼çºŒå°è©±
            return self.app.invoke(
                {"messages": [HumanMessage(content=new_input)]},
                config=config
            )
        else:
            # æ–°æœƒè©±
            return self.app.invoke(
                {"messages": [HumanMessage(content=new_input)]},
                config=config
            )

    def rollback(self, session_id: str, steps: int = 1):
        """å›æ»¾åˆ°ä¹‹å‰çš„ç‹€æ…‹"""
        config = self.get_config(session_id)

        history = list(self.app.get_state_history(config))
        if len(history) > steps:
            target = history[steps]
            return self.app.update_state(
                config,
                target.values,
                as_node=target.next[0] if target.next else None
            )
```

### 8.3.3 ç‹€æ…‹åˆ†æ”¯

```python
def explore_alternatives(app, config, alternatives: list[str]):
    """æ¢ç´¢ä¸åŒçš„å›æ‡‰åˆ†æ”¯"""
    results = []

    # ç²å–ç•¶å‰ç‹€æ…‹
    snapshot = app.get_state(config)
    base_checkpoint = snapshot.config["configurable"]["checkpoint_id"]

    for i, alternative in enumerate(alternatives):
        # ç‚ºæ¯å€‹æ›¿ä»£æ–¹æ¡ˆå‰µå»ºæ–°çš„åˆ†æ”¯
        branch_config = {
            "configurable": {
                "thread_id": f"{config['configurable']['thread_id']}-branch-{i}",
                "checkpoint_id": base_checkpoint
            }
        }

        # å¾ç›¸åŒèµ·é»åŸ·è¡Œä¸åŒè¼¸å…¥
        result = app.invoke(
            {"messages": [HumanMessage(content=alternative)]},
            config=branch_config
        )
        results.append({
            "input": alternative,
            "output": result
        })

    return results
```

---

## 8.4 é•·æœŸè¨˜æ†¶ï¼šå‘é‡è³‡æ–™åº«

### 8.4.1 è¨˜æ†¶æ¶æ§‹

```mermaid
graph TB
    subgraph "å¯«å…¥æµç¨‹"
        I[å°è©±/ç¶“é©—] --> E[Embedding]
        E --> V[å‘é‡è³‡æ–™åº«]
    end

    subgraph "è®€å–æµç¨‹"
        Q[æŸ¥è©¢] --> QE[Query Embedding]
        QE --> S[ç›¸ä¼¼åº¦æœå°‹]
        V --> S
        S --> R[ç›¸é—œè¨˜æ†¶]
    end
```

### 8.4.2 å¯¦ç¾é•·æœŸè¨˜æ†¶å­˜å„²

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from pydantic import BaseModel
from datetime import datetime

class Memory(BaseModel):
    """è¨˜æ†¶æ¢ç›®"""
    id: str
    content: str
    memory_type: str  # "fact", "preference", "experience", "skill"
    source: str  # ä¾†æºï¼ˆå°è©± IDã€æ–‡ä»¶ç­‰ï¼‰
    created_at: datetime
    importance: float  # é‡è¦æ€§åˆ†æ•¸ 0-1
    access_count: int = 0
    last_accessed: datetime | None = None


class LongTermMemory:
    """é•·æœŸè¨˜æ†¶ç³»çµ±"""

    def __init__(self, collection_name: str = "techassist_memory"):
        self.embeddings = OpenAIEmbeddings()
        self.client = QdrantClient(":memory:")  # æˆ–ä½¿ç”¨æŒä¹…åŒ–å­˜å„²
        self.collection_name = collection_name
        self._init_collection()

    def _init_collection(self):
        """åˆå§‹åŒ–å‘é‡é›†åˆ"""
        from qdrant_client.models import Distance, VectorParams

        self.client.recreate_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=1536,  # OpenAI embedding ç¶­åº¦
                distance=Distance.COSINE
            )
        )

    def store(self, memory: Memory):
        """å­˜å„²è¨˜æ†¶"""
        # ç”Ÿæˆ embedding
        vector = self.embeddings.embed_query(memory.content)

        # å­˜å„²åˆ°å‘é‡è³‡æ–™åº«
        self.client.upsert(
            collection_name=self.collection_name,
            points=[{
                "id": memory.id,
                "vector": vector,
                "payload": memory.model_dump()
            }]
        )

    def search(self, query: str, top_k: int = 5, memory_type: str | None = None) -> list[Memory]:
        """æœå°‹ç›¸é—œè¨˜æ†¶"""
        query_vector = self.embeddings.embed_query(query)

        # æ§‹å»ºéæ¿¾æ¢ä»¶
        filter_conditions = None
        if memory_type:
            filter_conditions = {
                "must": [{"key": "memory_type", "match": {"value": memory_type}}]
            }

        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=top_k,
            query_filter=filter_conditions
        )

        memories = []
        for result in results:
            memory = Memory(**result.payload)
            memory.access_count += 1
            memory.last_accessed = datetime.now()
            memories.append(memory)

        return memories

    def forget(self, memory_id: str):
        """åˆªé™¤è¨˜æ†¶"""
        self.client.delete(
            collection_name=self.collection_name,
            points_selector={"points": [memory_id]}
        )

    def consolidate(self, threshold_days: int = 30):
        """è¨˜æ†¶æ•´åˆï¼šæ¸…ç†èˆŠçš„ã€ä¸é‡è¦çš„è¨˜æ†¶"""
        # å¯¦ç¾è¨˜æ†¶éºå¿˜æ›²ç·š
        pass
```

### 8.4.3 è¨˜æ†¶é¡å‹

```python
class MemoryTypes:
    """è¨˜æ†¶é¡å‹å®šç¾©"""

    FACT = "fact"           # äº‹å¯¦ï¼šç”¨æˆ¶çš„è³‡è¨Šã€åå¥½
    PREFERENCE = "preference"  # åå¥½ï¼šå–œæ­¡ä»€éº¼ã€ä¸å–œæ­¡ä»€éº¼
    EXPERIENCE = "experience"  # ç¶“é©—ï¼šè™•ç†éçš„ä»»å‹™ã€è§£æ±ºæ–¹æ¡ˆ
    SKILL = "skill"         # æŠ€èƒ½ï¼šå­¸åˆ°çš„æ¨¡å¼ã€æ¨¡æ¿


# è¨˜æ†¶æå– Prompt
MEMORY_EXTRACTION_PROMPT = """åˆ†æä»¥ä¸‹å°è©±ï¼Œæå–æ‡‰è©²è¨˜ä½çš„è³‡è¨Šã€‚

å°è©±ï¼š
{conversation}

è«‹è­˜åˆ¥ä¸¦æå–ï¼š
1. äº‹å¯¦ï¼ˆFACTï¼‰ï¼šç”¨æˆ¶æåˆ°çš„å®¢è§€è³‡è¨Š
2. åå¥½ï¼ˆPREFERENCEï¼‰ï¼šç”¨æˆ¶è¡¨é”çš„å–œå¥½
3. ç¶“é©—ï¼ˆEXPERIENCEï¼‰ï¼šè§£æ±ºå•é¡Œçš„éç¨‹å’Œçµæœ
4. æŠ€èƒ½ï¼ˆSKILLï¼‰ï¼šå¯ä»¥è¤‡ç”¨çš„æ¨¡å¼æˆ–æ–¹æ³•

è¼¸å‡ºæ ¼å¼ï¼ˆJSON åˆ—è¡¨ï¼‰ï¼š
[
    {{"type": "fact", "content": "...", "importance": 0.8}},
    ...
]
"""


def extract_memories(conversation: str, llm) -> list[dict]:
    """å¾å°è©±ä¸­æå–è¨˜æ†¶"""
    response = llm.invoke(MEMORY_EXTRACTION_PROMPT.format(conversation=conversation))

    try:
        import json
        memories = json.loads(response.content)
        return memories
    except:
        return []
```

---

## 8.5 Semantic Injectionï¼šå‹•æ…‹è¨˜æ†¶æ³¨å…¥

### 8.5.1 è¨­è¨ˆåŸå‰‡

åœ¨ LLM èª¿ç”¨å‰ï¼Œæ ¹æ“šç•¶å‰ä¸Šä¸‹æ–‡æ³¨å…¥ç›¸é—œè¨˜æ†¶ï¼š

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ¶
    participant A as Agent
    participant M as è¨˜æ†¶ç³»çµ±
    participant L as LLM

    U->>A: è¼¸å…¥
    A->>M: æœå°‹ç›¸é—œè¨˜æ†¶
    M-->>A: ç›¸é—œè¨˜æ†¶åˆ—è¡¨
    A->>A: æ§‹å»ºå¢å¼· Prompt
    A->>L: å¢å¼·çš„è¼¸å…¥
    L-->>A: å›æ‡‰
    A->>M: å­˜å„²æ–°è¨˜æ†¶
    A->>U: å›æ‡‰
```

### 8.5.2 å¯¦ç¾ Memory Injection Node

```python
from langchain_core.messages import SystemMessage

def create_memory_injection_node(memory_system: LongTermMemory):
    """å‰µå»ºè¨˜æ†¶æ³¨å…¥ç¯€é»"""

    def memory_injection_node(state: AgentState) -> dict:
        """åœ¨è™•ç†å‰æ³¨å…¥ç›¸é—œè¨˜æ†¶"""
        # ç²å–æœ€è¿‘çš„ç”¨æˆ¶è¼¸å…¥
        last_user_message = None
        for msg in reversed(state["messages"]):
            if isinstance(msg, HumanMessage):
                last_user_message = msg.content
                break

        if not last_user_message:
            return {}

        # æœå°‹ç›¸é—œè¨˜æ†¶
        relevant_memories = memory_system.search(
            query=last_user_message,
            top_k=5
        )

        if not relevant_memories:
            return {}

        # æ§‹å»ºè¨˜æ†¶ä¸Šä¸‹æ–‡
        memory_context = "## ç›¸é—œè¨˜æ†¶\n\n"
        for mem in relevant_memories:
            memory_context += f"- [{mem.memory_type}] {mem.content}\n"

        # æ³¨å…¥ç‚ºç³»çµ±è¨Šæ¯
        memory_message = SystemMessage(content=memory_context)

        return {
            "injected_memories": relevant_memories,
            "messages": [memory_message]
        }

    return memory_injection_node
```

### 8.5.3 è¨˜æ†¶æ„ŸçŸ¥çš„ Prompt

```python
MEMORY_AWARE_SYSTEM_PROMPT = """ä½ æ˜¯ TechAssistï¼Œä¸€å€‹å…·æœ‰è¨˜æ†¶èƒ½åŠ›çš„æŠ€è¡“åŠ©ç†ã€‚

## ä½ çš„è¨˜æ†¶

ä»¥ä¸‹æ˜¯é—œæ–¼ç”¨æˆ¶å’Œä¹‹å‰äº’å‹•çš„é‡è¦è³‡è¨Šï¼š

{memory_context}

## ä½¿ç”¨è¨˜æ†¶çš„åŸå‰‡

1. **å€‹äººåŒ–**ï¼šæ ¹æ“šç”¨æˆ¶åå¥½èª¿æ•´å›ç­”
2. **é€£è²«æ€§**ï¼šåƒè€ƒä¹‹å‰çš„å°è©±å’Œæ±ºå®š
3. **å­¸ç¿’**ï¼šæ‡‰ç”¨ä¹‹å‰çš„ç¶“é©—å’Œè§£æ±ºæ–¹æ¡ˆ
4. **ä½†ä¸è¦**ï¼šéåº¦ä¾è³´èˆŠè³‡è¨Šï¼Œå¿½ç•¥æ–°çš„ä¸Šä¸‹æ–‡

## ç•¶å‰å°è©±

è«‹æ ¹æ“šä¸Šè¿°è¨˜æ†¶å’Œç”¨æˆ¶çš„æ–°è¼¸å…¥é€²è¡Œå›æ‡‰ã€‚
"""


def build_memory_aware_prompt(memories: list[Memory]) -> str:
    """æ§‹å»ºè¨˜æ†¶æ„ŸçŸ¥çš„ Prompt"""
    if not memories:
        return MEMORY_AWARE_SYSTEM_PROMPT.replace("{memory_context}", "ï¼ˆç„¡ç›¸é—œè¨˜æ†¶ï¼‰")

    memory_sections = {
        "fact": [],
        "preference": [],
        "experience": [],
        "skill": []
    }

    for mem in memories:
        memory_sections[mem.memory_type].append(mem.content)

    context_parts = []

    if memory_sections["fact"]:
        context_parts.append("### ç”¨æˆ¶è³‡è¨Š\n" + "\n".join(f"- {f}" for f in memory_sections["fact"]))

    if memory_sections["preference"]:
        context_parts.append("### ç”¨æˆ¶åå¥½\n" + "\n".join(f"- {p}" for p in memory_sections["preference"]))

    if memory_sections["experience"]:
        context_parts.append("### ç›¸é—œç¶“é©—\n" + "\n".join(f"- {e}" for e in memory_sections["experience"]))

    if memory_sections["skill"]:
        context_parts.append("### å¯ç”¨æŠ€èƒ½\n" + "\n".join(f"- {s}" for s in memory_sections["skill"]))

    memory_context = "\n\n".join(context_parts)

    return MEMORY_AWARE_SYSTEM_PROMPT.replace("{memory_context}", memory_context)
```

---

## 8.6 å¯¦ä½œï¼šTechAssist v0.9

### 8.6.1 è¨˜æ†¶å¢å¼·çš„ç‹€æ…‹

```python
class MemoryEnhancedState(TypedDict):
    """TechAssist v0.9 ç‹€æ…‹"""
    messages: Annotated[list, add_messages]

    # ç”¨æˆ¶è³‡è¨Š
    user_id: str
    session_id: str

    # è¨˜æ†¶
    injected_memories: list[Memory]
    memories_to_store: list[Memory]

    # åŸæœ‰åŠŸèƒ½
    intent: str | None
    plan: Plan | None
    iteration: int
```

### 8.6.2 å®Œæ•´çš„è¨˜æ†¶ç®¡é“

```python
class TechAssistV9:
    """TechAssist v0.9 - å…·å‚™é•·æœŸè¨˜æ†¶"""

    def __init__(self, user_id: str):
        self.user_id = user_id
        self.memory = LongTermMemory(collection_name=f"user_{user_id}")
        self.graph = self._build_graph()

    def _build_graph(self):
        graph = StateGraph(MemoryEnhancedState)

        # è¨˜æ†¶ç›¸é—œç¯€é»
        graph.add_node("recall", self._recall_node)
        graph.add_node("process", self._process_node)
        graph.add_node("memorize", self._memorize_node)

        # æµç¨‹
        graph.add_edge(START, "recall")
        graph.add_edge("recall", "process")
        graph.add_edge("process", "memorize")
        graph.add_edge("memorize", END)

        return graph.compile(checkpointer=MemorySaver())

    def _recall_node(self, state: MemoryEnhancedState) -> dict:
        """å›æ†¶ç›¸é—œè¨˜æ†¶"""
        last_message = state["messages"][-1].content

        memories = self.memory.search(last_message, top_k=5)

        return {"injected_memories": memories}

    def _process_node(self, state: MemoryEnhancedState) -> dict:
        """è™•ç†è«‹æ±‚ï¼ˆå¸¶è¨˜æ†¶ä¸Šä¸‹æ–‡ï¼‰"""
        # æ§‹å»ºè¨˜æ†¶æ„ŸçŸ¥çš„ prompt
        system_prompt = build_memory_aware_prompt(state["injected_memories"])

        messages = [
            SystemMessage(content=system_prompt),
            *state["messages"]
        ]

        response = llm.invoke(messages)

        return {"messages": [response]}

    def _memorize_node(self, state: MemoryEnhancedState) -> dict:
        """å­˜å„²æ–°è¨˜æ†¶"""
        # æå–å°è©±ä¸­çš„é‡è¦è³‡è¨Š
        conversation = "\n".join([
            f"{'User' if isinstance(m, HumanMessage) else 'AI'}: {m.content}"
            for m in state["messages"][-4:]  # æœ€è¿‘å¹¾è¼ª
        ])

        new_memories = extract_memories(conversation, llm)

        for mem_data in new_memories:
            memory = Memory(
                id=f"{self.user_id}-{int(time.time())}-{random.randint(1000, 9999)}",
                content=mem_data["content"],
                memory_type=mem_data["type"],
                source=state["session_id"],
                created_at=datetime.now(),
                importance=mem_data.get("importance", 0.5)
            )
            self.memory.store(memory)

        return {"memories_to_store": new_memories}

    def chat(self, message: str, session_id: str | None = None) -> str:
        """å°è©±"""
        session_id = session_id or f"{self.user_id}-{int(time.time())}"

        config = {"configurable": {"thread_id": session_id}}

        initial = {
            "messages": [HumanMessage(content=message)],
            "user_id": self.user_id,
            "session_id": session_id,
            "injected_memories": [],
            "memories_to_store": [],
            "intent": None,
            "plan": None,
            "iteration": 0,
        }

        result = self.graph.invoke(initial, config=config)
        return result["messages"][-1].content
```

### 8.6.3 ä½¿ç”¨ç¯„ä¾‹

```python
# å‰µå»ºç”¨æˆ¶å°ˆå±¬çš„åŠ©ç†
assistant = TechAssistV9(user_id="user_001")

# ç¬¬ä¸€æ¬¡å°è©±
response1 = assistant.chat("æˆ‘æ˜¯ä¸€å€‹ Python é–‹ç™¼è€…ï¼Œå–œæ­¡ç”¨ FastAPI")
print(response1)
# "å¾ˆé«˜èˆˆèªè­˜ä½ ï¼æˆ‘æœƒè¨˜ä½ä½ ä½¿ç”¨ Python å’Œ FastAPI..."

# ä¸€æ®µæ™‚é–“å¾Œ...
response2 = assistant.chat("å¹«æˆ‘å¯«ä¸€å€‹ REST API")
print(response2)
# "å¥½çš„ï¼æ ¹æ“šä½ ä¹‹å‰æåˆ°çš„åå¥½ï¼Œæˆ‘æœƒä½¿ç”¨ FastAPI ä¾†å¯¦ç¾..."
# ï¼ˆè¨˜æ†¶ç”Ÿæ•ˆï¼ï¼‰

# æŸ¥çœ‹å­˜å„²çš„è¨˜æ†¶
memories = assistant.memory.search("Python", top_k=10)
for mem in memories:
    print(f"[{mem.memory_type}] {mem.content}")
```

---

## 8.7 é€²éšæŠ€å·§

### 8.7.1 è¨˜æ†¶è¡°æ¸›

æ¨¡æ“¬äººé¡çš„éºå¿˜æ›²ç·šï¼š

```python
import math

def calculate_memory_strength(memory: Memory, current_time: datetime) -> float:
    """è¨ˆç®—è¨˜æ†¶å¼·åº¦ï¼ˆåŸºæ–¼éºå¿˜æ›²ç·šï¼‰"""
    # è‰¾è³“æµ©æ–¯éºå¿˜æ›²ç·š
    hours_since_creation = (current_time - memory.created_at).total_seconds() / 3600

    # åŸºç¤è¡°æ¸›
    base_retention = math.exp(-hours_since_creation / 168)  # 168 å°æ™‚ = 1 é€±

    # é‡è¦æ€§åŠ æˆ
    importance_bonus = memory.importance * 0.3

    # è¨ªå•æ¬¡æ•¸åŠ æˆ
    access_bonus = min(memory.access_count * 0.05, 0.2)

    return min(base_retention + importance_bonus + access_bonus, 1.0)


def prune_weak_memories(memory_system: LongTermMemory, threshold: float = 0.1):
    """æ¸…ç†å¼±è¨˜æ†¶"""
    all_memories = memory_system.get_all()
    current_time = datetime.now()

    for memory in all_memories:
        strength = calculate_memory_strength(memory, current_time)
        if strength < threshold:
            memory_system.forget(memory.id)
```

### 8.7.2 è¨˜æ†¶æ•´åˆ

å°‡å¤šå€‹ç›¸é—œè¨˜æ†¶åˆä½µï¼š

```python
def consolidate_memories(memories: list[Memory], llm) -> Memory:
    """æ•´åˆç›¸é—œè¨˜æ†¶"""
    contents = "\n".join([m.content for m in memories])

    prompt = f"""ä»¥ä¸‹æ˜¯å¤šå€‹ç›¸é—œçš„è¨˜æ†¶ç‰‡æ®µï¼Œè«‹æ•´åˆç‚ºä¸€å€‹é€£è²«çš„æ‘˜è¦ï¼š

{contents}

æ•´åˆå¾Œçš„è¨˜æ†¶æ‡‰è©²ï¼š
1. ä¿ç•™æ‰€æœ‰é‡è¦è³‡è¨Š
2. å»é™¤é‡è¤‡
3. ä¿æŒé‚è¼¯é€£è²«
"""

    response = llm.invoke(prompt)

    # å‰µå»ºæ•´åˆå¾Œçš„è¨˜æ†¶
    return Memory(
        id=f"consolidated-{int(time.time())}",
        content=response.content,
        memory_type=memories[0].memory_type,
        source="consolidation",
        created_at=datetime.now(),
        importance=max(m.importance for m in memories)
    )
```

### 8.7.3 è¨˜æ†¶ç´¢å¼•å„ªåŒ–

```python
class OptimizedMemoryIndex:
    """å„ªåŒ–çš„è¨˜æ†¶ç´¢å¼•"""

    def __init__(self):
        self.by_type = {}      # æŒ‰é¡å‹ç´¢å¼•
        self.by_time = []      # æŒ‰æ™‚é–“æ’åº
        self.by_importance = []  # æŒ‰é‡è¦æ€§æ’åº

    def add(self, memory: Memory):
        # æŒ‰é¡å‹
        if memory.memory_type not in self.by_type:
            self.by_type[memory.memory_type] = []
        self.by_type[memory.memory_type].append(memory)

        # æŒ‰æ™‚é–“
        self.by_time.append(memory)
        self.by_time.sort(key=lambda m: m.created_at, reverse=True)

        # æŒ‰é‡è¦æ€§
        self.by_importance.append(memory)
        self.by_importance.sort(key=lambda m: m.importance, reverse=True)

    def get_recent(self, n: int = 10) -> list[Memory]:
        return self.by_time[:n]

    def get_important(self, n: int = 10) -> list[Memory]:
        return self.by_importance[:n]

    def get_by_type(self, memory_type: str) -> list[Memory]:
        return self.by_type.get(memory_type, [])
```

---

## 8.8 æœ¬ç« å›é¡§

### æ ¸å¿ƒæ¦‚å¿µ

| æ¦‚å¿µ | èªªæ˜ | å¯¦ç¾ |
|------|------|------|
| **çŸ­æœŸè¨˜æ†¶** | ç•¶å‰å°è©±ä¸Šä¸‹æ–‡ | messages + trim |
| **æœƒè©±è¨˜æ†¶** | å–®æ¬¡æœƒè©±ç‹€æ…‹ | Checkpointer |
| **é•·æœŸè¨˜æ†¶** | è·¨æœƒè©±çŸ¥è­˜ | å‘é‡è³‡æ–™åº« |
| **è¨˜æ†¶æ³¨å…¥** | å‹•æ…‹å¢å¼·ä¸Šä¸‹æ–‡ | Semantic Injection |

### è¨˜æ†¶ç®¡é“

```mermaid
graph LR
    A[è¼¸å…¥] --> B[Recall]
    B --> C[Inject]
    C --> D[Process]
    D --> E[Extract]
    E --> F[Store]
    F --> G[è¼¸å‡º]
```

### TechAssist é‡Œç¨‹ç¢‘

- âœ… v0.8ï¼šPlanning Pattern
- âœ… v0.9ï¼šMemory Patternï¼ˆé•·æœŸè¨˜æ†¶ï¼‰

---

## 8.9 ä¸‹ä¸€ç« é å‘Š

TechAssist v0.9 æœ‰äº†è¨˜æ†¶ï¼Œä½†å®ƒé‚„ä¸èƒ½**å¾éŒ¯èª¤ä¸­å­¸ç¿’**ã€‚ç•¶å®ƒçµ¦å‡ºéŒ¯èª¤çš„å›ç­”æ™‚ï¼Œå®ƒä¸æœƒè‡ªæˆ‘åçœå’Œæ”¹é€²ã€‚

åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ **è‡ªæˆ‘ä¿®æ­£æ¨¡å¼ (The Reflexion Pattern)**ï¼š

- é›™è¿´åœˆå­¸ç¿’åŸç†
- Generator-Evaluator-Refiner æ¶æ§‹
- è‡ªå‹•éŒ¯èª¤æª¢æ¸¬èˆ‡ä¿®å¾©
- TechAssist v1.0ï¼šèƒ½è‡ªæˆ‘æ”¹é€²çš„åŠ©ç†

---

## ç·´ç¿’é¡Œ

1. **åŸºç¤ç·´ç¿’**ï¼šå¯¦ç¾ä¸€å€‹ã€Œè¨˜æ†¶é¢æ¿ã€ï¼Œè®“ç”¨æˆ¶å¯ä»¥æŸ¥çœ‹å’Œç®¡ç† TechAssist å­˜å„²çš„é—œæ–¼ä»–å€‘çš„è¨˜æ†¶ã€‚

2. **é€²éšç·´ç¿’**ï¼šå¯¦ç¾ã€Œè¨˜æ†¶è¡çªè§£æ±ºã€ï¼šç•¶æ–°è¨˜æ†¶èˆ‡èˆŠè¨˜æ†¶çŸ›ç›¾æ™‚ï¼ˆä¾‹å¦‚ã€Œç”¨æˆ¶å–œæ­¡ Pythonã€vsã€Œç”¨æˆ¶ç¾åœ¨å–œæ­¡ Rustã€ï¼‰ï¼Œè‡ªå‹•è™•ç†ã€‚

3. **æŒ‘æˆ°ç·´ç¿’**ï¼šå¯¦ç¾ã€Œè¨˜æ†¶åˆ†äº«ã€ï¼šå…è¨±å¤šå€‹ç”¨æˆ¶å…±äº«æŸäº›è¨˜æ†¶ï¼ˆä¾‹å¦‚åœ˜éšŠçŸ¥è­˜åº«ï¼‰ï¼ŒåŒæ™‚ä¿æŒå€‹äººè¨˜æ†¶çš„ç§å¯†æ€§ã€‚

---

## å»¶ä¼¸é–±è®€

- [LangChainï¼šMemory](https://python.langchain.com/docs/how_to/chatbots_memory/)
- [å‘é‡è³‡æ–™åº«æ¯”è¼ƒ](https://www.pinecone.io/learn/vector-database/)
- [MemGPTï¼šLong-term Memory for LLMs](https://arxiv.org/abs/2310.08560)
