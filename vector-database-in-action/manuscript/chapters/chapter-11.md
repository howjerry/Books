# Chapter 11ï¼šæ€§èƒ½å„ªåŒ–èˆ‡ç›£æ§

> ã€Œå„ªåŒ–ä¸æ˜¯è®“ç³»çµ±è®Šå¿«ï¼Œè€Œæ˜¯æ‰¾å‡ºè®“ç³»çµ±è®Šæ…¢çš„åŸå› ä¸¦æ¶ˆé™¤å®ƒã€‚ã€

## å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

- è­˜åˆ¥å‘é‡æœå°‹ç³»çµ±çš„æ•ˆèƒ½ç“¶é ¸
- æ‡‰ç”¨å¤šç¨®å„ªåŒ–æŠ€è¡“æå‡æœå°‹æ•ˆèƒ½
- å»ºç«‹å®Œæ•´çš„ç›£æ§å’Œå‘Šè­¦ç³»çµ±
- é€²è¡Œæ•ˆèƒ½åŸºæº–æ¸¬è©¦å’Œå®¹é‡è¦åŠƒ
- å¯¦ä½œç”Ÿç”¢ç’°å¢ƒçš„æ•ˆèƒ½èª¿å„ª

---

## 11.1 æ•ˆèƒ½æŒ‡æ¨™é«”ç³»

### 11.1.1 æ ¸å¿ƒæ•ˆèƒ½æŒ‡æ¨™

```python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from enum import Enum
import time
import numpy as np

class PerformanceMetric(Enum):
    """æ•ˆèƒ½æŒ‡æ¨™é¡å‹"""
    LATENCY = "latency"           # å»¶é²
    THROUGHPUT = "throughput"     # ååé‡
    ACCURACY = "accuracy"         # ç²¾ç¢ºåº¦
    RESOURCE = "resource"         # è³‡æºä½¿ç”¨


@dataclass
class LatencyMetrics:
    """
    å»¶é²æŒ‡æ¨™

    â€¹1â€º è¿½è¹¤å„éšæ®µå»¶é²
    """
    total_ms: float              # ç¸½å»¶é²
    embedding_ms: float          # å‘é‡åŒ–å»¶é²
    search_ms: float             # æœå°‹å»¶é²
    postprocess_ms: float        # å¾Œè™•ç†å»¶é²
    network_ms: float            # ç¶²çµ¡å»¶é²

    def breakdown(self) -> Dict[str, float]:
        """å»¶é²åˆ†è§£"""
        return {
            "embedding": self.embedding_ms / self.total_ms * 100,
            "search": self.search_ms / self.total_ms * 100,
            "postprocess": self.postprocess_ms / self.total_ms * 100,
            "network": self.network_ms / self.total_ms * 100
        }


@dataclass
class ThroughputMetrics:
    """
    ååé‡æŒ‡æ¨™

    â€¹2â€º è¿½è¹¤ç³»çµ±è™•ç†èƒ½åŠ›
    """
    qps: float                   # æ¯ç§’æŸ¥è©¢æ•¸
    concurrent_queries: int      # ä¸¦ç™¼æŸ¥è©¢æ•¸
    batch_size: int              # æ‰¹æ¬¡å¤§å°
    vectors_per_second: int      # æ¯ç§’è™•ç†å‘é‡æ•¸


@dataclass
class AccuracyMetrics:
    """
    ç²¾ç¢ºåº¦æŒ‡æ¨™

    â€¹3â€º è¿½è¹¤æœå°‹å“è³ª
    """
    recall_at_k: float           # Recall@K
    precision_at_k: float        # Precision@K
    ndcg_at_k: float             # NDCG@K
    mrr: float                   # Mean Reciprocal Rank


class PerformanceProfiler:
    """
    æ•ˆèƒ½åˆ†æå™¨

    â€¹1â€º æ”¶é›†æ•ˆèƒ½æ•¸æ“š
    â€¹2â€º è­˜åˆ¥ç“¶é ¸
    â€¹3â€º ç”Ÿæˆå ±å‘Š
    """

    def __init__(self):
        self.latency_samples: List[LatencyMetrics] = []
        self.throughput_samples: List[ThroughputMetrics] = []
        self.accuracy_samples: List[AccuracyMetrics] = []

    def record_latency(self, metrics: LatencyMetrics):
        """è¨˜éŒ„å»¶é²æ•¸æ“š"""
        self.latency_samples.append(metrics)

    def get_latency_stats(self) -> Dict[str, float]:
        """
        â€¹1â€º ç²å–å»¶é²çµ±è¨ˆ
        """
        if not self.latency_samples:
            return {}

        totals = [s.total_ms for s in self.latency_samples]
        return {
            "min": min(totals),
            "max": max(totals),
            "mean": np.mean(totals),
            "p50": np.percentile(totals, 50),
            "p95": np.percentile(totals, 95),
            "p99": np.percentile(totals, 99),
            "std": np.std(totals)
        }

    def identify_bottleneck(self) -> str:
        """
        â€¹2â€º è­˜åˆ¥æ•ˆèƒ½ç“¶é ¸
        """
        if not self.latency_samples:
            return "insufficient_data"

        # è¨ˆç®—å„éšæ®µçš„å¹³å‡ä½”æ¯”
        breakdowns = [s.breakdown() for s in self.latency_samples[-100:]]
        avg_breakdown = {
            key: np.mean([b[key] for b in breakdowns])
            for key in breakdowns[0].keys()
        }

        # æ‰¾å‡ºä½”æ¯”æœ€é«˜çš„éšæ®µ
        bottleneck = max(avg_breakdown.items(), key=lambda x: x[1])
        return bottleneck[0]

    def generate_report(self) -> str:
        """
        â€¹3â€º ç”Ÿæˆæ•ˆèƒ½å ±å‘Š
        """
        stats = self.get_latency_stats()
        bottleneck = self.identify_bottleneck()

        report = f"""
æ•ˆèƒ½åˆ†æå ±å‘Š
============

å»¶é²çµ±è¨ˆ (ms):
  æœ€å°å€¼: {stats.get('min', 0):.2f}
  æœ€å¤§å€¼: {stats.get('max', 0):.2f}
  å¹³å‡å€¼: {stats.get('mean', 0):.2f}
  P50: {stats.get('p50', 0):.2f}
  P95: {stats.get('p95', 0):.2f}
  P99: {stats.get('p99', 0):.2f}

æ•ˆèƒ½ç“¶é ¸: {bottleneck}

å„ªåŒ–å»ºè­°:
"""
        if bottleneck == "embedding":
            report += "  - è€ƒæ…®ä½¿ç”¨æ›´å¿«çš„åµŒå…¥æ¨¡å‹\n"
            report += "  - å•Ÿç”¨åµŒå…¥å¿«å–\n"
            report += "  - ä½¿ç”¨ GPU åŠ é€ŸåµŒå…¥ç”Ÿæˆ\n"
        elif bottleneck == "search":
            report += "  - å„ªåŒ–ç´¢å¼•åƒæ•¸ï¼ˆnlist, nprobeï¼‰\n"
            report += "  - è€ƒæ…®ä½¿ç”¨æ›´å¿«çš„ç´¢å¼•é¡å‹ï¼ˆHNSWï¼‰\n"
            report += "  - å¢åŠ æœå°‹ç¯€é»æ•¸é‡\n"
        elif bottleneck == "postprocess":
            report += "  - æ¸›å°‘è¿”å›çµæœæ•¸é‡\n"
            report += "  - ç°¡åŒ–å¾Œè™•ç†é‚è¼¯\n"
            report += "  - ä½¿ç”¨æ›´é«˜æ•ˆçš„æ’åºç®—æ³•\n"
        elif bottleneck == "network":
            report += "  - æª¢æŸ¥ç¶²çµ¡å»¶é²\n"
            report += "  - è€ƒæ…®éƒ¨ç½²åœ¨æ›´è¿‘çš„å€åŸŸ\n"
            report += "  - ä½¿ç”¨é€£æ¥æ± \n"

        return report


def demonstrate_profiling():
    """
    â€¹1â€º æ•ˆèƒ½åˆ†æç¤ºç¯„
    """
    print("æ•ˆèƒ½åˆ†æç¤ºç¯„")
    print("=" * 60)

    profiler = PerformanceProfiler()

    # æ¨¡æ“¬æ”¶é›†æ•ˆèƒ½æ•¸æ“š
    np.random.seed(42)
    for _ in range(100):
        metrics = LatencyMetrics(
            total_ms=np.random.uniform(50, 200),
            embedding_ms=np.random.uniform(10, 50),
            search_ms=np.random.uniform(20, 100),
            postprocess_ms=np.random.uniform(5, 30),
            network_ms=np.random.uniform(5, 20)
        )
        profiler.record_latency(metrics)

    # ç”Ÿæˆå ±å‘Š
    print(profiler.generate_report())


if __name__ == "__main__":
    demonstrate_profiling()
```

---

## 11.2 ç´¢å¼•å„ªåŒ–

### 11.2.1 ç´¢å¼•åƒæ•¸èª¿å„ª

```python
import numpy as np
import time
from typing import Tuple, List, Dict

class IndexOptimizer:
    """
    ç´¢å¼•åƒæ•¸å„ªåŒ–å™¨

    â€¹1â€º è‡ªå‹•æœå°‹æœ€ä½³åƒæ•¸
    â€¹2â€º å¹³è¡¡ç²¾ç¢ºåº¦å’Œé€Ÿåº¦
    """

    def __init__(self, dimension: int, n_vectors: int):
        """
        â€¹1â€º åˆå§‹åŒ–å„ªåŒ–å™¨

        Args:
            dimension: å‘é‡ç¶­åº¦
            n_vectors: å‘é‡æ•¸é‡
        """
        self.dimension = dimension
        self.n_vectors = n_vectors

    def recommend_ivf_params(
        self,
        target_recall: float = 0.95,
        max_latency_ms: float = 50.0
    ) -> Dict[str, int]:
        """
        â€¹2â€º æ¨è–¦ IVF ç´¢å¼•åƒæ•¸

        Args:
            target_recall: ç›®æ¨™å¬å›ç‡
            max_latency_ms: æœ€å¤§å»¶é²

        Returns:
            æ¨è–¦çš„åƒæ•¸
        """
        # nlist è¨ˆç®—è¦å‰‡
        # å»ºè­°ï¼š4 * sqrt(n) åˆ° 16 * sqrt(n)
        sqrt_n = int(np.sqrt(self.n_vectors))
        nlist_candidates = [
            sqrt_n,
            2 * sqrt_n,
            4 * sqrt_n,
            8 * sqrt_n,
            16 * sqrt_n
        ]

        # æ ¹æ“šæ•¸æ“šé‡é¸æ“‡ nlist
        if self.n_vectors < 100000:
            nlist = min(nlist_candidates[1], 256)
        elif self.n_vectors < 1000000:
            nlist = min(nlist_candidates[2], 1024)
        else:
            nlist = min(nlist_candidates[3], 4096)

        # nprobe è¨ˆç®—è¦å‰‡
        # è¼ƒé«˜å¬å›ç‡éœ€è¦è¼ƒå¤§çš„ nprobe
        if target_recall >= 0.99:
            nprobe = max(nlist // 4, 64)
        elif target_recall >= 0.95:
            nprobe = max(nlist // 8, 32)
        elif target_recall >= 0.90:
            nprobe = max(nlist // 16, 16)
        else:
            nprobe = max(nlist // 32, 8)

        return {
            "nlist": nlist,
            "nprobe": nprobe,
            "estimated_recall": self._estimate_recall(nlist, nprobe)
        }

    def _estimate_recall(self, nlist: int, nprobe: int) -> float:
        """ä¼°ç®—å¬å›ç‡"""
        # ç°¡åŒ–çš„ä¼°ç®—å…¬å¼
        ratio = nprobe / nlist
        return min(0.99, 0.5 + 0.5 * np.sqrt(ratio))

    def recommend_hnsw_params(
        self,
        target_recall: float = 0.95,
        memory_limit_gb: float = 8.0
    ) -> Dict[str, int]:
        """
        â€¹3â€º æ¨è–¦ HNSW ç´¢å¼•åƒæ•¸

        Args:
            target_recall: ç›®æ¨™å¬å›ç‡
            memory_limit_gb: è¨˜æ†¶é«”é™åˆ¶

        Returns:
            æ¨è–¦çš„åƒæ•¸
        """
        # M åƒæ•¸ï¼ˆæ¯å€‹ç¯€é»çš„é€£æ¥æ•¸ï¼‰
        # è¼ƒå¤§çš„ M æé«˜ç²¾ç¢ºåº¦ä½†å¢åŠ è¨˜æ†¶é«”
        if target_recall >= 0.99:
            M = 64
        elif target_recall >= 0.95:
            M = 32
        else:
            M = 16

        # efConstructionï¼ˆå»ºæ§‹æ™‚çš„æœå°‹ç¯„åœï¼‰
        efConstruction = M * 8  # ç¶“é©—å€¼

        # efSearchï¼ˆæœå°‹æ™‚çš„å€™é¸é›†å¤§å°ï¼‰
        if target_recall >= 0.99:
            efSearch = 256
        elif target_recall >= 0.95:
            efSearch = 128
        else:
            efSearch = 64

        # æª¢æŸ¥è¨˜æ†¶é«”é™åˆ¶
        estimated_memory_gb = self._estimate_hnsw_memory(M) / (1024 ** 3)
        if estimated_memory_gb > memory_limit_gb:
            # æ¸›å° M
            M = max(8, int(M * memory_limit_gb / estimated_memory_gb))
            efConstruction = M * 8

        return {
            "M": M,
            "efConstruction": efConstruction,
            "efSearch": efSearch,
            "estimated_memory_gb": self._estimate_hnsw_memory(M) / (1024 ** 3)
        }

    def _estimate_hnsw_memory(self, M: int) -> int:
        """ä¼°ç®— HNSW è¨˜æ†¶é«”ä½¿ç”¨ï¼ˆbytesï¼‰"""
        # å‘é‡å­˜å„² + åœ–çµæ§‹
        vector_memory = self.n_vectors * self.dimension * 4  # float32
        graph_memory = self.n_vectors * M * 2 * 8  # æ¯å€‹é€£æ¥ 8 bytes
        return vector_memory + graph_memory

    def recommend_pq_params(
        self,
        target_compression: float = 8.0,
        min_recall: float = 0.85
    ) -> Dict[str, int]:
        """
        â€¹4â€º æ¨è–¦ PQ ç´¢å¼•åƒæ•¸

        Args:
            target_compression: ç›®æ¨™å£“ç¸®æ¯”
            min_recall: æœ€ä½å¬å›ç‡

        Returns:
            æ¨è–¦çš„åƒæ•¸
        """
        # mï¼ˆå­ç©ºé–“æ•¸ï¼‰
        # dimension å¿…é ˆèƒ½è¢« m æ•´é™¤
        possible_m = [d for d in range(4, min(65, self.dimension + 1))
                     if self.dimension % d == 0]

        # æ ¹æ“šå£“ç¸®æ¯”é¸æ“‡ m
        # å£“ç¸®æ¯” â‰ˆ dimension * 4 / m
        target_m = self.dimension * 4 / target_compression

        # æ‰¾æœ€æ¥è¿‘çš„ m
        m = min(possible_m, key=lambda x: abs(x - target_m))

        # nbitsï¼ˆæ¯å€‹å­ç©ºé–“çš„ä½å…ƒæ•¸ï¼‰
        # é€šå¸¸ä½¿ç”¨ 8ï¼ˆ256 å€‹èšé¡ä¸­å¿ƒï¼‰
        nbits = 8

        actual_compression = self.dimension * 4 / m

        return {
            "m": m,
            "nbits": nbits,
            "compression_ratio": actual_compression,
            "memory_per_vector_bytes": m * nbits // 8
        }


def demonstrate_index_optimization():
    """
    â€¹1â€º ç´¢å¼•å„ªåŒ–ç¤ºç¯„
    """
    print("ç´¢å¼•åƒæ•¸å„ªåŒ–")
    print("=" * 60)

    optimizer = IndexOptimizer(dimension=768, n_vectors=1000000)

    # IVF åƒæ•¸æ¨è–¦
    print("\nIVF ç´¢å¼•æ¨è–¦åƒæ•¸:")
    print("-" * 40)
    for target_recall in [0.90, 0.95, 0.99]:
        params = optimizer.recommend_ivf_params(target_recall=target_recall)
        print(f"  ç›®æ¨™å¬å›ç‡ {target_recall:.0%}:")
        print(f"    nlist: {params['nlist']}")
        print(f"    nprobe: {params['nprobe']}")
        print(f"    é ä¼°å¬å›ç‡: {params['estimated_recall']:.2%}")
        print()

    # HNSW åƒæ•¸æ¨è–¦
    print("HNSW ç´¢å¼•æ¨è–¦åƒæ•¸:")
    print("-" * 40)
    for target_recall in [0.90, 0.95, 0.99]:
        params = optimizer.recommend_hnsw_params(target_recall=target_recall)
        print(f"  ç›®æ¨™å¬å›ç‡ {target_recall:.0%}:")
        print(f"    M: {params['M']}")
        print(f"    efConstruction: {params['efConstruction']}")
        print(f"    efSearch: {params['efSearch']}")
        print(f"    é ä¼°è¨˜æ†¶é«”: {params['estimated_memory_gb']:.2f} GB")
        print()

    # PQ åƒæ•¸æ¨è–¦
    print("PQ ç´¢å¼•æ¨è–¦åƒæ•¸:")
    print("-" * 40)
    for target_compression in [4, 8, 16, 32]:
        params = optimizer.recommend_pq_params(target_compression=target_compression)
        print(f"  ç›®æ¨™å£“ç¸®æ¯” {target_compression}x:")
        print(f"    m: {params['m']}")
        print(f"    å¯¦éš›å£“ç¸®æ¯”: {params['compression_ratio']:.1f}x")
        print(f"    æ¯å‘é‡è¨˜æ†¶é«”: {params['memory_per_vector_bytes']} bytes")
        print()


if __name__ == "__main__":
    demonstrate_index_optimization()
```

### 11.2.2 ç´¢å¼•ç†±èº«èˆ‡é è¼‰å…¥

```python
import time
import numpy as np
from typing import List

class IndexWarmup:
    """
    ç´¢å¼•ç†±èº«

    â€¹1â€º é è¼‰å…¥ç´¢å¼•åˆ°è¨˜æ†¶é«”
    â€¹2â€º é ç†±å¿«å–
    â€¹3â€º JIT ç·¨è­¯å„ªåŒ–
    """

    def __init__(self, index):
        """
        â€¹1â€º åˆå§‹åŒ–

        Args:
            index: å‘é‡ç´¢å¼•å¯¦ä¾‹
        """
        self.index = index
        self.warmup_queries = []

    def generate_warmup_queries(
        self,
        n_queries: int = 100,
        dimension: int = 768
    ) -> np.ndarray:
        """
        â€¹2â€º ç”Ÿæˆç†±èº«æŸ¥è©¢

        ä½¿ç”¨éš¨æ©ŸæŸ¥è©¢æˆ–æ­·å²ç†±é–€æŸ¥è©¢
        """
        return np.random.randn(n_queries, dimension).astype(np.float32)

    def warmup(
        self,
        queries: np.ndarray = None,
        top_k: int = 10,
        rounds: int = 3
    ) -> Dict[str, float]:
        """
        â€¹3â€º åŸ·è¡Œç†±èº«

        Args:
            queries: ç†±èº«æŸ¥è©¢ï¼ˆå¯é¸ï¼‰
            top_k: æœå°‹ top-k
            rounds: ç†±èº«è¼ªæ•¸

        Returns:
            ç†±èº«çµ±è¨ˆ
        """
        if queries is None:
            queries = self.generate_warmup_queries()

        print(f"é–‹å§‹ç´¢å¼•ç†±èº«...")
        print(f"  æŸ¥è©¢æ•¸é‡: {len(queries)}")
        print(f"  ç†±èº«è¼ªæ•¸: {rounds}")

        latencies = []

        for round_idx in range(rounds):
            round_start = time.perf_counter()

            for query in queries:
                start = time.perf_counter()
                # åŸ·è¡Œæœå°‹ï¼ˆé€™è£¡æ˜¯æ¨¡æ“¬ï¼‰
                # self.index.search(query.reshape(1, -1), top_k)
                time.sleep(0.001)  # æ¨¡æ“¬æœå°‹
                latencies.append(time.perf_counter() - start)

            round_time = time.perf_counter() - round_start
            print(f"  ç¬¬ {round_idx + 1} è¼ªå®Œæˆ: {round_time:.2f}s")

        # çµ±è¨ˆ
        latencies = np.array(latencies)
        stats = {
            "total_queries": len(latencies),
            "avg_latency_ms": np.mean(latencies) * 1000,
            "p99_latency_ms": np.percentile(latencies, 99) * 1000,
            "improvement": (latencies[:len(queries)].mean() -
                          latencies[-len(queries):].mean()) / latencies[:len(queries)].mean()
        }

        print(f"\nç†±èº«å®Œæˆ:")
        print(f"  å¹³å‡å»¶é²: {stats['avg_latency_ms']:.2f}ms")
        print(f"  P99 å»¶é²: {stats['p99_latency_ms']:.2f}ms")
        print(f"  æ•ˆèƒ½æå‡: {stats['improvement']:.1%}")

        return stats


def demonstrate_warmup():
    """
    â€¹1â€º ç´¢å¼•ç†±èº«ç¤ºç¯„
    """
    print("ç´¢å¼•ç†±èº«ç¤ºç¯„")
    print("=" * 60)

    warmup = IndexWarmup(index=None)
    queries = warmup.generate_warmup_queries(n_queries=50, dimension=768)
    stats = warmup.warmup(queries, top_k=10, rounds=3)


if __name__ == "__main__":
    demonstrate_warmup()
```

---

## 11.3 æŸ¥è©¢å„ªåŒ–

### 11.3.1 æŸ¥è©¢å‘é‡åŒ–å„ªåŒ–

```python
import numpy as np
from typing import List, Dict, Any
import time

class EmbeddingOptimizer:
    """
    åµŒå…¥ç”Ÿæˆå„ªåŒ–

    â€¹1â€º æ‰¹é‡è™•ç†
    â€¹2â€º å¿«å–ç­–ç•¥
    â€¹3â€º æ¨¡å‹é¸æ“‡
    """

    def __init__(self, cache_size: int = 10000):
        """
        â€¹1â€º åˆå§‹åŒ–

        Args:
            cache_size: åµŒå…¥å¿«å–å¤§å°
        """
        self.cache: Dict[str, np.ndarray] = {}
        self.cache_size = cache_size
        self.cache_hits = 0
        self.cache_misses = 0

    def get_embedding(
        self,
        text: str,
        model: str = "default"
    ) -> np.ndarray:
        """
        â€¹2â€º ç²å–æ–‡å­—åµŒå…¥ï¼ˆå¸¶å¿«å–ï¼‰
        """
        cache_key = f"{model}:{hash(text)}"

        if cache_key in self.cache:
            self.cache_hits += 1
            return self.cache[cache_key]

        self.cache_misses += 1

        # ç”ŸæˆåµŒå…¥ï¼ˆæ¨¡æ“¬ï¼‰
        embedding = np.random.randn(768).astype(np.float32)

        # å¿«å–
        if len(self.cache) >= self.cache_size:
            # LRU æ·˜æ±°
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[cache_key] = embedding
        return embedding

    def batch_get_embeddings(
        self,
        texts: List[str],
        model: str = "default",
        batch_size: int = 32
    ) -> np.ndarray:
        """
        â€¹3â€º æ‰¹é‡ç²å–åµŒå…¥

        æ‰¹é‡è™•ç†å¯ä»¥é¡¯è‘—æå‡æ•ˆèƒ½
        """
        embeddings = []
        uncached_texts = []
        uncached_indices = []

        # å…ˆæª¢æŸ¥å¿«å–
        for i, text in enumerate(texts):
            cache_key = f"{model}:{hash(text)}"
            if cache_key in self.cache:
                embeddings.append((i, self.cache[cache_key]))
                self.cache_hits += 1
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
                self.cache_misses += 1

        # æ‰¹é‡ç”Ÿæˆæœªå¿«å–çš„åµŒå…¥
        if uncached_texts:
            for batch_start in range(0, len(uncached_texts), batch_size):
                batch_texts = uncached_texts[batch_start:batch_start + batch_size]
                # æ¨¡æ“¬æ‰¹é‡åµŒå…¥ç”Ÿæˆ
                batch_embeddings = np.random.randn(len(batch_texts), 768).astype(np.float32)

                for j, emb in enumerate(batch_embeddings):
                    idx = uncached_indices[batch_start + j]
                    text = uncached_texts[batch_start + j]
                    embeddings.append((idx, emb))

                    # å¿«å–
                    cache_key = f"{model}:{hash(text)}"
                    self.cache[cache_key] = emb

        # æŒ‰åŸå§‹é †åºæ’åº
        embeddings.sort(key=lambda x: x[0])
        return np.array([e[1] for e in embeddings])

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        â€¹4â€º ç²å–å¿«å–çµ±è¨ˆ
        """
        total = self.cache_hits + self.cache_misses
        return {
            "cache_size": len(self.cache),
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": self.cache_hits / total if total > 0 else 0
        }


def demonstrate_embedding_optimization():
    """
    â€¹1â€º åµŒå…¥å„ªåŒ–ç¤ºç¯„
    """
    print("åµŒå…¥ç”Ÿæˆå„ªåŒ–")
    print("=" * 60)

    optimizer = EmbeddingOptimizer(cache_size=1000)

    # æ¨¡æ“¬æŸ¥è©¢
    texts = [f"query_{i}" for i in range(100)]
    # é‡è¤‡ä¸€äº›æŸ¥è©¢ä»¥æ¸¬è©¦å¿«å–
    texts += [f"query_{i}" for i in range(50)]

    print(f"æŸ¥è©¢æ•¸é‡: {len(texts)}")
    print(f"å”¯ä¸€æŸ¥è©¢: 100")
    print(f"é‡è¤‡æŸ¥è©¢: 50")

    # æ‰¹é‡è™•ç†
    start = time.perf_counter()
    embeddings = optimizer.batch_get_embeddings(texts, batch_size=32)
    elapsed = time.perf_counter() - start

    print(f"\nè™•ç†æ™‚é–“: {elapsed*1000:.2f}ms")
    print(f"æ¯æŸ¥è©¢å¹³å‡: {elapsed/len(texts)*1000:.3f}ms")

    # å¿«å–çµ±è¨ˆ
    stats = optimizer.get_cache_stats()
    print(f"\nå¿«å–çµ±è¨ˆ:")
    print(f"  å‘½ä¸­: {stats['cache_hits']}")
    print(f"  æœªå‘½ä¸­: {stats['cache_misses']}")
    print(f"  å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")


if __name__ == "__main__":
    demonstrate_embedding_optimization()
```

### 11.3.2 çµæœé‡æ’åºå„ªåŒ–

```python
from typing import List, Tuple
import numpy as np

class RerankerOptimizer:
    """
    é‡æ’åºå„ªåŒ–

    â€¹1â€º å…©éšæ®µæª¢ç´¢
    â€¹2â€º è¼•é‡ç´šé‡æ’åº
    â€¹3â€º æ‰¹é‡é‡æ’åº
    """

    def __init__(self):
        self.stats = {
            "total_reranked": 0,
            "avg_improvement": 0
        }

    def two_stage_search(
        self,
        query_vector: np.ndarray,
        database: np.ndarray,
        first_stage_k: int = 100,
        final_k: int = 10
    ) -> List[Tuple[int, float]]:
        """
        â€¹1â€º å…©éšæ®µæª¢ç´¢

        ç¬¬ä¸€éšæ®µï¼šå¿«é€Ÿå¬å›
        ç¬¬äºŒéšæ®µï¼šç²¾ç¢ºé‡æ’åº
        """
        # ç¬¬ä¸€éšæ®µï¼šä½¿ç”¨è¿‘ä¼¼æœå°‹å¬å›å€™é¸
        # ï¼ˆé€™è£¡ä½¿ç”¨ç²¾ç¢ºæœå°‹æ¨¡æ“¬ï¼‰
        distances = np.linalg.norm(database - query_vector, axis=1)
        first_stage_indices = np.argpartition(distances, first_stage_k)[:first_stage_k]

        # ç¬¬äºŒéšæ®µï¼šå°å€™é¸é€²è¡Œç²¾ç¢ºæ’åº
        candidates = database[first_stage_indices]
        candidate_distances = distances[first_stage_indices]

        # æ’åº
        sorted_indices = np.argsort(candidate_distances)[:final_k]

        results = [
            (first_stage_indices[i], candidate_distances[i])
            for i in sorted_indices
        ]

        return results

    def lightweight_rerank(
        self,
        query: str,
        candidates: List[Tuple[int, float, str]],
        top_k: int = 10
    ) -> List[Tuple[int, float, str]]:
        """
        â€¹2â€º è¼•é‡ç´šé‡æ’åº

        ä½¿ç”¨ç°¡å–®çš„å•Ÿç™¼å¼è¦å‰‡é‡æ’åº
        é¿å…ä½¿ç”¨æ˜‚è²´çš„ cross-encoder
        """
        reranked = []

        for idx, score, content in candidates:
            # è¨ˆç®—é¡å¤–çš„ç›¸é—œæ€§ä¿¡è™Ÿ
            bonus = 0

            # é—œéµè©åŒ¹é…åŠ åˆ†
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())
            overlap = len(query_words & content_words)
            bonus += overlap * 0.05

            # å…§å®¹é•·åº¦é©ä¸­åŠ åˆ†
            word_count = len(content.split())
            if 50 <= word_count <= 500:
                bonus += 0.02

            # è¨ˆç®—æœ€çµ‚åˆ†æ•¸
            final_score = score + bonus
            reranked.append((idx, final_score, content))

        # æ’åº
        reranked.sort(key=lambda x: x[1], reverse=True)
        return reranked[:top_k]

    def batch_rerank(
        self,
        queries: List[str],
        all_candidates: List[List[Tuple[int, float, str]]],
        top_k: int = 10
    ) -> List[List[Tuple[int, float, str]]]:
        """
        â€¹3â€º æ‰¹é‡é‡æ’åº

        ä¸¦è¡Œè™•ç†å¤šå€‹æŸ¥è©¢çš„é‡æ’åº
        """
        results = []
        for query, candidates in zip(queries, all_candidates):
            reranked = self.lightweight_rerank(query, candidates, top_k)
            results.append(reranked)
            self.stats["total_reranked"] += len(candidates)

        return results


def demonstrate_reranking():
    """
    â€¹1â€º é‡æ’åºå„ªåŒ–ç¤ºç¯„
    """
    print("é‡æ’åºå„ªåŒ–")
    print("=" * 60)

    optimizer = RerankerOptimizer()

    # æ¨¡æ“¬å€™é¸çµæœ
    candidates = [
        (0, 0.85, "å‘é‡è³‡æ–™åº«æ˜¯ä¸€ç¨®å°ˆé–€ç”¨æ–¼å­˜å„²å’Œæœå°‹å‘é‡çš„è³‡æ–™åº«ç³»çµ±"),
        (1, 0.82, "FAISS æ˜¯ Facebook é–‹ç™¼çš„å‘é‡æœå°‹å‡½å¼åº«"),
        (2, 0.80, "Milvus æ˜¯ä¸€å€‹é–‹æºçš„å‘é‡è³‡æ–™åº«"),
        (3, 0.78, "å‘é‡æœå°‹åœ¨æ¨è–¦ç³»çµ±ä¸­æœ‰å»£æ³›æ‡‰ç”¨"),
        (4, 0.75, "åµŒå…¥æ¨¡å‹å¯ä»¥å°‡æ–‡å­—è½‰æ›ç‚ºå‘é‡"),
    ]

    query = "å‘é‡è³‡æ–™åº«"

    print(f"æŸ¥è©¢: {query}")
    print(f"\nåŸå§‹æ’åº:")
    for idx, score, content in candidates:
        print(f"  [{idx}] {score:.2f}: {content[:30]}...")

    # é‡æ’åº
    reranked = optimizer.lightweight_rerank(query, candidates, top_k=3)

    print(f"\né‡æ’åºå¾Œ:")
    for idx, score, content in reranked:
        print(f"  [{idx}] {score:.2f}: {content[:30]}...")


if __name__ == "__main__":
    demonstrate_reranking()
```

---

## 11.4 ç³»çµ±ç›£æ§

### 11.4.1 ç›£æ§å„€è¡¨æ¿è¨­è¨ˆ

```python
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime
import time

@dataclass
class DashboardMetric:
    """å„€è¡¨æ¿æŒ‡æ¨™"""
    name: str
    value: float
    unit: str
    trend: str  # up, down, stable
    health: str  # healthy, warning, critical


class MonitoringDashboard:
    """
    ç›£æ§å„€è¡¨æ¿

    â€¹1â€º å¯¦æ™‚æŒ‡æ¨™å±•ç¤º
    â€¹2â€º è¶¨å‹¢åˆ†æ
    â€¹3â€º å¥åº·ç‹€æ…‹
    """

    def __init__(self):
        self.metrics_history: Dict[str, List[float]] = {}
        self.thresholds = {
            "latency_p99_ms": {"warning": 100, "critical": 500},
            "error_rate": {"warning": 0.01, "critical": 0.05},
            "cpu_usage": {"warning": 70, "critical": 90},
            "memory_usage": {"warning": 80, "critical": 95},
            "qps": {"warning": 1000, "critical": 2000}  # é€™æ˜¯ä¸‹é™
        }

    def record_metric(self, name: str, value: float):
        """
        â€¹1â€º è¨˜éŒ„æŒ‡æ¨™
        """
        if name not in self.metrics_history:
            self.metrics_history[name] = []
        self.metrics_history[name].append(value)
        # åªä¿ç•™æœ€è¿‘ 1000 å€‹æ¨£æœ¬
        if len(self.metrics_history[name]) > 1000:
            self.metrics_history[name].pop(0)

    def get_current_metrics(self) -> List[DashboardMetric]:
        """
        â€¹2â€º ç²å–ç•¶å‰æŒ‡æ¨™
        """
        metrics = []

        for name, history in self.metrics_history.items():
            if not history:
                continue

            current = history[-1]
            trend = self._calculate_trend(history)
            health = self._evaluate_health(name, current)

            unit = self._get_unit(name)

            metrics.append(DashboardMetric(
                name=name,
                value=current,
                unit=unit,
                trend=trend,
                health=health
            ))

        return metrics

    def _calculate_trend(self, history: List[float]) -> str:
        """è¨ˆç®—è¶¨å‹¢"""
        if len(history) < 10:
            return "stable"

        recent = history[-10:]
        older = history[-20:-10] if len(history) >= 20 else history[:10]

        recent_avg = sum(recent) / len(recent)
        older_avg = sum(older) / len(older)

        change = (recent_avg - older_avg) / older_avg if older_avg != 0 else 0

        if change > 0.1:
            return "up"
        elif change < -0.1:
            return "down"
        else:
            return "stable"

    def _evaluate_health(self, name: str, value: float) -> str:
        """è©•ä¼°å¥åº·ç‹€æ…‹"""
        if name not in self.thresholds:
            return "healthy"

        thresholds = self.thresholds[name]

        # QPS æ˜¯è¶Šé«˜è¶Šå¥½
        if name == "qps":
            if value < thresholds["critical"]:
                return "critical"
            elif value < thresholds["warning"]:
                return "warning"
            return "healthy"

        # å…¶ä»–æŒ‡æ¨™è¶Šä½è¶Šå¥½
        if value >= thresholds["critical"]:
            return "critical"
        elif value >= thresholds["warning"]:
            return "warning"
        return "healthy"

    def _get_unit(self, name: str) -> str:
        """ç²å–å–®ä½"""
        units = {
            "latency_p99_ms": "ms",
            "error_rate": "%",
            "cpu_usage": "%",
            "memory_usage": "%",
            "qps": "req/s",
            "vector_count": "vectors"
        }
        return units.get(name, "")

    def render_dashboard(self) -> str:
        """
        â€¹3â€º æ¸²æŸ“å„€è¡¨æ¿
        """
        metrics = self.get_current_metrics()

        output = []
        output.append("=" * 70)
        output.append("å‘é‡æœå°‹ç³»çµ±ç›£æ§å„€è¡¨æ¿")
        output.append(f"æ›´æ–°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        output.append("=" * 70)
        output.append("")

        # å¥åº·æ¦‚è¦½
        health_counts = {"healthy": 0, "warning": 0, "critical": 0}
        for m in metrics:
            health_counts[m.health] += 1

        output.append("ç³»çµ±å¥åº·ç‹€æ…‹:")
        output.append(f"  [OK] æ­£å¸¸: {health_counts['healthy']}")
        output.append(f"  [!] è­¦å‘Š: {health_counts['warning']}")
        output.append(f"  [X] åš´é‡: {health_counts['critical']}")
        output.append("")

        # æŒ‡æ¨™è©³æƒ…
        output.append("æŒ‡æ¨™è©³æƒ…:")
        output.append("-" * 70)
        output.append(f"{'æŒ‡æ¨™åç¨±':<25} {'æ•¸å€¼':<15} {'è¶¨å‹¢':<10} {'ç‹€æ…‹':<10}")
        output.append("-" * 70)

        for m in metrics:
            trend_symbol = {"up": "â†‘", "down": "â†“", "stable": "â†’"}[m.trend]
            health_symbol = {"healthy": "[OK]", "warning": "[!]", "critical": "[X]"}[m.health]

            value_str = f"{m.value:.2f} {m.unit}"
            output.append(f"{m.name:<25} {value_str:<15} {trend_symbol:<10} {health_symbol:<10}")

        output.append("-" * 70)

        return "\n".join(output)


def demonstrate_dashboard():
    """
    â€¹1â€º å„€è¡¨æ¿ç¤ºç¯„
    """
    print("ç›£æ§å„€è¡¨æ¿ç¤ºç¯„")
    print()

    dashboard = MonitoringDashboard()

    # æ¨¡æ“¬è¨˜éŒ„æŒ‡æ¨™
    import random
    for _ in range(100):
        dashboard.record_metric("latency_p99_ms", random.uniform(30, 80))
        dashboard.record_metric("error_rate", random.uniform(0, 0.02))
        dashboard.record_metric("cpu_usage", random.uniform(40, 75))
        dashboard.record_metric("memory_usage", random.uniform(50, 85))
        dashboard.record_metric("qps", random.uniform(800, 1500))
        dashboard.record_metric("vector_count", 1000000 + random.randint(-1000, 1000))

    # æ¸²æŸ“å„€è¡¨æ¿
    print(dashboard.render_dashboard())


if __name__ == "__main__":
    demonstrate_dashboard()
```

### 11.4.2 å‘Šè­¦é…ç½®

```python
from typing import Dict, List, Callable, Any, Optional
from dataclasses import dataclass
from enum import Enum
import time

class AlertSeverity(Enum):
    """å‘Šè­¦åš´é‡ç¨‹åº¦"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


class AlertStatus(Enum):
    """å‘Šè­¦ç‹€æ…‹"""
    ACTIVE = "active"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"


@dataclass
class AlertRule:
    """å‘Šè­¦è¦å‰‡"""
    name: str
    metric: str
    condition: str  # gt, lt, eq
    threshold: float
    severity: AlertSeverity
    duration_seconds: int  # æŒçºŒå¤šä¹…æ‰å‘Šè­¦
    message_template: str


@dataclass
class Alert:
    """å‘Šè­¦å¯¦ä¾‹"""
    rule_name: str
    severity: AlertSeverity
    message: str
    value: float
    status: AlertStatus
    triggered_at: float
    acknowledged_at: Optional[float] = None
    resolved_at: Optional[float] = None


class AlertManager:
    """
    å‘Šè­¦ç®¡ç†å™¨

    â€¹1â€º è¦å‰‡è©•ä¼°
    â€¹2â€º å‘Šè­¦è§¸ç™¼èˆ‡æ¢å¾©
    â€¹3â€º é€šçŸ¥ç™¼é€
    """

    def __init__(self):
        self.rules: List[AlertRule] = []
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []
        self.pending_alerts: Dict[str, float] = {}  # rule_name -> first_triggered_time

        # é€šçŸ¥è™•ç†å™¨
        self.notifiers: List[Callable[[Alert], None]] = []

    def add_rule(self, rule: AlertRule):
        """
        â€¹1â€º æ·»åŠ å‘Šè­¦è¦å‰‡
        """
        self.rules.append(rule)
        print(f"æ·»åŠ è¦å‰‡: {rule.name}")

    def add_notifier(self, notifier: Callable[[Alert], None]):
        """
        â€¹2â€º æ·»åŠ é€šçŸ¥è™•ç†å™¨
        """
        self.notifiers.append(notifier)

    def evaluate(self, metrics: Dict[str, float]):
        """
        â€¹3â€º è©•ä¼°æ‰€æœ‰è¦å‰‡
        """
        current_time = time.time()

        for rule in self.rules:
            if rule.metric not in metrics:
                continue

            value = metrics[rule.metric]
            is_triggered = self._check_condition(value, rule.condition, rule.threshold)

            if is_triggered:
                self._handle_triggered(rule, value, current_time)
            else:
                self._handle_resolved(rule, current_time)

    def _check_condition(self, value: float, condition: str, threshold: float) -> bool:
        """æª¢æŸ¥æ¢ä»¶"""
        if condition == "gt":
            return value > threshold
        elif condition == "lt":
            return value < threshold
        elif condition == "eq":
            return abs(value - threshold) < 0.001
        return False

    def _handle_triggered(self, rule: AlertRule, value: float, current_time: float):
        """è™•ç†è§¸ç™¼"""
        if rule.name not in self.pending_alerts:
            self.pending_alerts[rule.name] = current_time

        pending_duration = current_time - self.pending_alerts[rule.name]

        # æª¢æŸ¥æ˜¯å¦è¶…éæŒçºŒæ™‚é–“é–¾å€¼
        if pending_duration >= rule.duration_seconds:
            if rule.name not in self.active_alerts:
                # å‰µå»ºæ–°å‘Šè­¦
                message = rule.message_template.format(
                    value=value,
                    threshold=rule.threshold
                )
                alert = Alert(
                    rule_name=rule.name,
                    severity=rule.severity,
                    message=message,
                    value=value,
                    status=AlertStatus.ACTIVE,
                    triggered_at=current_time
                )
                self.active_alerts[rule.name] = alert
                self._notify(alert)

    def _handle_resolved(self, rule: AlertRule, current_time: float):
        """è™•ç†æ¢å¾©"""
        # æ¸…é™¤ pending
        if rule.name in self.pending_alerts:
            del self.pending_alerts[rule.name]

        # æ¢å¾© active å‘Šè­¦
        if rule.name in self.active_alerts:
            alert = self.active_alerts[rule.name]
            alert.status = AlertStatus.RESOLVED
            alert.resolved_at = current_time
            self.alert_history.append(alert)
            del self.active_alerts[rule.name]
            print(f"[RESOLVED] {rule.name}")

    def _notify(self, alert: Alert):
        """ç™¼é€é€šçŸ¥"""
        severity_emoji = {
            AlertSeverity.INFO: "â„¹ï¸",
            AlertSeverity.WARNING: "âš ï¸",
            AlertSeverity.CRITICAL: "ğŸš¨"
        }

        print(f"\n{severity_emoji[alert.severity]} [{alert.severity.value.upper()}] {alert.rule_name}")
        print(f"   {alert.message}")
        print()

        for notifier in self.notifiers:
            try:
                notifier(alert)
            except Exception as e:
                print(f"é€šçŸ¥ç™¼é€å¤±æ•—: {e}")

    def get_active_alerts(self) -> List[Alert]:
        """ç²å–æ´»èºå‘Šè­¦"""
        return list(self.active_alerts.values())


def demonstrate_alerting():
    """
    â€¹1â€º å‘Šè­¦ç¤ºç¯„
    """
    print("å‘Šè­¦ç³»çµ±ç¤ºç¯„")
    print("=" * 60)

    manager = AlertManager()

    # æ·»åŠ å‘Šè­¦è¦å‰‡
    manager.add_rule(AlertRule(
        name="é«˜å»¶é²å‘Šè­¦",
        metric="latency_p99_ms",
        condition="gt",
        threshold=100,
        severity=AlertSeverity.WARNING,
        duration_seconds=2,
        message_template="P99 å»¶é² ({value:.2f}ms) è¶…éé–¾å€¼ ({threshold}ms)"
    ))

    manager.add_rule(AlertRule(
        name="éŒ¯èª¤ç‡å‘Šè­¦",
        metric="error_rate",
        condition="gt",
        threshold=0.05,
        severity=AlertSeverity.CRITICAL,
        duration_seconds=1,
        message_template="éŒ¯èª¤ç‡ ({value:.2%}) è¶…éé–¾å€¼ ({threshold:.2%})"
    ))

    manager.add_rule(AlertRule(
        name="CPU ä½¿ç”¨ç‡å‘Šè­¦",
        metric="cpu_usage",
        condition="gt",
        threshold=90,
        severity=AlertSeverity.WARNING,
        duration_seconds=3,
        message_template="CPU ä½¿ç”¨ç‡ ({value:.1f}%) è¶…éé–¾å€¼ ({threshold}%)"
    ))

    # æ¨¡æ“¬æŒ‡æ¨™
    print("\næ¨¡æ“¬æŒ‡æ¨™è©•ä¼°:")
    print("-" * 40)

    # æ­£å¸¸æŒ‡æ¨™
    manager.evaluate({
        "latency_p99_ms": 50,
        "error_rate": 0.01,
        "cpu_usage": 60
    })
    print("æ­£å¸¸æŒ‡æ¨™ - ç„¡å‘Šè­¦")

    # å»¶é²å‡é«˜
    time.sleep(1)
    manager.evaluate({
        "latency_p99_ms": 150,
        "error_rate": 0.01,
        "cpu_usage": 60
    })
    print("å»¶é²å‡é«˜ä¸­...")

    time.sleep(2)
    manager.evaluate({
        "latency_p99_ms": 150,
        "error_rate": 0.01,
        "cpu_usage": 60
    })

    # æ¢å¾©
    time.sleep(1)
    manager.evaluate({
        "latency_p99_ms": 50,
        "error_rate": 0.01,
        "cpu_usage": 60
    })


if __name__ == "__main__":
    demonstrate_alerting()
```

---

## 11.5 è² è¼‰æ¸¬è©¦èˆ‡å®¹é‡è¦åŠƒ

### 11.5.1 è² è¼‰æ¸¬è©¦æ¡†æ¶

```python
import numpy as np
import time
from typing import List, Dict, Any, Callable
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

@dataclass
class LoadTestConfig:
    """è² è¼‰æ¸¬è©¦é…ç½®"""
    duration_seconds: int
    target_qps: int
    ramp_up_seconds: int
    num_workers: int


@dataclass
class LoadTestResult:
    """è² è¼‰æ¸¬è©¦çµæœ"""
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_latency_ms: float
    p50_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    actual_qps: float
    error_rate: float


class LoadTester:
    """
    è² è¼‰æ¸¬è©¦å™¨

    â€¹1â€º æ¨¡æ“¬çœŸå¯¦è² è¼‰
    â€¹2â€º æ”¶é›†æ•ˆèƒ½æŒ‡æ¨™
    â€¹3â€º ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    """

    def __init__(
        self,
        target_function: Callable[[], bool],
        config: LoadTestConfig
    ):
        """
        â€¹1â€º åˆå§‹åŒ–

        Args:
            target_function: è¦æ¸¬è©¦çš„å‡½æ•¸ï¼Œè¿”å› True è¡¨ç¤ºæˆåŠŸ
            config: æ¸¬è©¦é…ç½®
        """
        self.target_function = target_function
        self.config = config

        self.results: List[Dict[str, Any]] = []
        self.results_lock = threading.Lock()
        self.stop_event = threading.Event()

    def run(self) -> LoadTestResult:
        """
        â€¹2â€º åŸ·è¡Œè² è¼‰æ¸¬è©¦
        """
        print(f"é–‹å§‹è² è¼‰æ¸¬è©¦")
        print(f"  ç›®æ¨™ QPS: {self.config.target_qps}")
        print(f"  æŒçºŒæ™‚é–“: {self.config.duration_seconds}s")
        print(f"  å·¥ä½œåŸ·è¡Œç·’: {self.config.num_workers}")
        print()

        start_time = time.time()

        with ThreadPoolExecutor(max_workers=self.config.num_workers) as executor:
            # æäº¤ä»»å‹™
            futures = []
            request_count = 0
            interval = 1.0 / self.config.target_qps

            while time.time() - start_time < self.config.duration_seconds:
                if self.stop_event.is_set():
                    break

                # Ramp up
                elapsed = time.time() - start_time
                if elapsed < self.config.ramp_up_seconds:
                    current_interval = interval * (self.config.ramp_up_seconds / max(elapsed, 0.1))
                else:
                    current_interval = interval

                futures.append(executor.submit(self._execute_request, request_count))
                request_count += 1

                time.sleep(current_interval)

                # å®šæœŸè¼¸å‡ºé€²åº¦
                if request_count % 100 == 0:
                    print(f"  å·²ç™¼é€ {request_count} è«‹æ±‚...")

            # ç­‰å¾…æ‰€æœ‰è«‹æ±‚å®Œæˆ
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"è«‹æ±‚åŸ·è¡ŒéŒ¯èª¤: {e}")

        # è¨ˆç®—çµæœ
        return self._calculate_results(start_time)

    def _execute_request(self, request_id: int):
        """åŸ·è¡Œå–®å€‹è«‹æ±‚"""
        start = time.perf_counter()
        try:
            success = self.target_function()
            latency = (time.perf_counter() - start) * 1000

            with self.results_lock:
                self.results.append({
                    "request_id": request_id,
                    "success": success,
                    "latency_ms": latency,
                    "timestamp": time.time()
                })
        except Exception as e:
            latency = (time.perf_counter() - start) * 1000
            with self.results_lock:
                self.results.append({
                    "request_id": request_id,
                    "success": False,
                    "latency_ms": latency,
                    "error": str(e),
                    "timestamp": time.time()
                })

    def _calculate_results(self, start_time: float) -> LoadTestResult:
        """è¨ˆç®—æ¸¬è©¦çµæœ"""
        if not self.results:
            return LoadTestResult(
                total_requests=0,
                successful_requests=0,
                failed_requests=0,
                avg_latency_ms=0,
                p50_latency_ms=0,
                p95_latency_ms=0,
                p99_latency_ms=0,
                actual_qps=0,
                error_rate=0
            )

        total = len(self.results)
        successful = sum(1 for r in self.results if r["success"])
        failed = total - successful
        latencies = [r["latency_ms"] for r in self.results]

        duration = time.time() - start_time

        return LoadTestResult(
            total_requests=total,
            successful_requests=successful,
            failed_requests=failed,
            avg_latency_ms=np.mean(latencies),
            p50_latency_ms=np.percentile(latencies, 50),
            p95_latency_ms=np.percentile(latencies, 95),
            p99_latency_ms=np.percentile(latencies, 99),
            actual_qps=total / duration,
            error_rate=failed / total if total > 0 else 0
        )


def demonstrate_load_testing():
    """
    â€¹1â€º è² è¼‰æ¸¬è©¦ç¤ºç¯„
    """
    print("è² è¼‰æ¸¬è©¦ç¤ºç¯„")
    print("=" * 60)

    # æ¨¡æ“¬ç›®æ¨™å‡½æ•¸
    def mock_search():
        time.sleep(np.random.uniform(0.01, 0.05))
        return np.random.random() > 0.02  # 2% éŒ¯èª¤ç‡

    config = LoadTestConfig(
        duration_seconds=5,
        target_qps=50,
        ramp_up_seconds=1,
        num_workers=10
    )

    tester = LoadTester(mock_search, config)
    result = tester.run()

    print("\nè² è¼‰æ¸¬è©¦çµæœ:")
    print("-" * 40)
    print(f"ç¸½è«‹æ±‚æ•¸: {result.total_requests}")
    print(f"æˆåŠŸè«‹æ±‚: {result.successful_requests}")
    print(f"å¤±æ•—è«‹æ±‚: {result.failed_requests}")
    print(f"éŒ¯èª¤ç‡: {result.error_rate:.2%}")
    print(f"å¯¦éš› QPS: {result.actual_qps:.2f}")
    print()
    print("å»¶é²çµ±è¨ˆ (ms):")
    print(f"  å¹³å‡: {result.avg_latency_ms:.2f}")
    print(f"  P50: {result.p50_latency_ms:.2f}")
    print(f"  P95: {result.p95_latency_ms:.2f}")
    print(f"  P99: {result.p99_latency_ms:.2f}")


if __name__ == "__main__":
    demonstrate_load_testing()
```

### 11.5.2 å®¹é‡è¦åŠƒ

```python
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class CapacityRequirements:
    """å®¹é‡éœ€æ±‚"""
    vector_count: int
    dimension: int
    qps_target: int
    latency_target_ms: float
    recall_target: float
    availability_target: float


class CapacityPlanner:
    """
    å®¹é‡è¦åŠƒå™¨

    â€¹1â€º ä¼°ç®—è³‡æºéœ€æ±‚
    â€¹2â€º æ¨è–¦æ¶æ§‹é…ç½®
    â€¹3â€º æˆæœ¬ä¼°ç®—
    """

    def __init__(self, requirements: CapacityRequirements):
        """
        â€¹1â€º åˆå§‹åŒ–

        Args:
            requirements: å®¹é‡éœ€æ±‚
        """
        self.requirements = requirements

    def estimate_memory(self) -> Dict[str, float]:
        """
        â€¹2â€º ä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚
        """
        r = self.requirements

        # åŸå§‹å‘é‡è¨˜æ†¶é«”
        raw_memory_gb = r.vector_count * r.dimension * 4 / (1024 ** 3)

        # ä¸åŒç´¢å¼•é¡å‹çš„è¨˜æ†¶é«”éœ€æ±‚
        estimates = {
            "FLAT": raw_memory_gb,
            "IVF_FLAT": raw_memory_gb * 1.05,  # 5% é¡å¤–é–‹éŠ·
            "IVF_PQ": raw_memory_gb * 0.15,    # ç´„ 85% å£“ç¸®
            "HNSW": raw_memory_gb * 1.5,       # 50% åœ–çµæ§‹é–‹éŠ·
        }

        return estimates

    def estimate_nodes(self) -> Dict[str, int]:
        """
        â€¹3â€º ä¼°ç®—ç¯€é»æ•¸é‡
        """
        r = self.requirements

        # åŸºæ–¼ QPS ä¼°ç®—
        # å‡è¨­å–®ç¯€é»å¯è™•ç† 1000 QPS
        qps_based = max(1, r.qps_target // 1000)

        # åŸºæ–¼è¨˜æ†¶é«”ä¼°ç®—ï¼ˆå‡è¨­æ¯ç¯€é» 32GB å¯ç”¨ï¼‰
        memory_gb = r.vector_count * r.dimension * 4 / (1024 ** 3)
        memory_based = max(1, int(memory_gb / 32) + 1)

        # åŸºæ–¼å¯ç”¨æ€§ï¼ˆè‡³å°‘éœ€è¦ 3 å€‹å‰¯æœ¬é”åˆ° 99.9%ï¼‰
        if r.availability_target >= 0.999:
            min_replicas = 3
        elif r.availability_target >= 0.99:
            min_replicas = 2
        else:
            min_replicas = 1

        return {
            "qps_based": qps_based,
            "memory_based": memory_based,
            "min_replicas": min_replicas,
            "recommended": max(qps_based, memory_based) * min_replicas
        }

    def recommend_index(self) -> str:
        """
        â€¹4â€º æ¨è–¦ç´¢å¼•é¡å‹
        """
        r = self.requirements

        if r.recall_target >= 0.99:
            if r.latency_target_ms >= 100:
                return "IVF_FLAT"
            else:
                return "HNSW"
        elif r.recall_target >= 0.95:
            if r.vector_count > 10_000_000:
                return "IVF_PQ"
            else:
                return "HNSW"
        else:
            return "IVF_PQ"

    def generate_plan(self) -> str:
        """
        â€¹5â€º ç”Ÿæˆå®¹é‡è¦åŠƒå ±å‘Š
        """
        r = self.requirements
        memory = self.estimate_memory()
        nodes = self.estimate_nodes()
        index = self.recommend_index()

        plan = f"""
å®¹é‡è¦åŠƒå ±å‘Š
============

éœ€æ±‚æ‘˜è¦:
- å‘é‡æ•¸é‡: {r.vector_count:,}
- å‘é‡ç¶­åº¦: {r.dimension}
- ç›®æ¨™ QPS: {r.qps_target:,}
- ç›®æ¨™å»¶é²: {r.latency_target_ms}ms
- ç›®æ¨™å¬å›ç‡: {r.recall_target:.2%}
- ç›®æ¨™å¯ç”¨æ€§: {r.availability_target:.2%}

è¨˜æ†¶é«”ä¼°ç®—:
- FLAT ç´¢å¼•: {memory['FLAT']:.2f} GB
- IVF_FLAT ç´¢å¼•: {memory['IVF_FLAT']:.2f} GB
- IVF_PQ ç´¢å¼•: {memory['IVF_PQ']:.2f} GB
- HNSW ç´¢å¼•: {memory['HNSW']:.2f} GB

ç¯€é»ä¼°ç®—:
- åŸºæ–¼ QPS: {nodes['qps_based']} ç¯€é»
- åŸºæ–¼è¨˜æ†¶é«”: {nodes['memory_based']} ç¯€é»
- æœ€å°å‰¯æœ¬æ•¸: {nodes['min_replicas']}
- æ¨è–¦ç¸½ç¯€é»: {nodes['recommended']} ç¯€é»

æ¨è–¦é…ç½®:
- ç´¢å¼•é¡å‹: {index}
- ç¯€é»è¦æ ¼: 32GB è¨˜æ†¶é«”, 8 vCPU
- ç¸½ç¯€é»æ•¸: {nodes['recommended']}
- é ä¼°æœˆæˆæœ¬: ${nodes['recommended'] * 500:,}ï¼ˆåŸºæ–¼é›²ç«¯å®šåƒ¹ï¼‰

æ³¨æ„äº‹é …:
- å»ºè­°é ç•™ 20% çš„è³‡æºé¤˜é‡
- é«˜å³°æœŸå¯èƒ½éœ€è¦è‡ªå‹•æ“´ç¸®å®¹
- å®šæœŸç›£æ§è³‡æºä½¿ç”¨ç‡
"""
        return plan


def demonstrate_capacity_planning():
    """
    â€¹1â€º å®¹é‡è¦åŠƒç¤ºç¯„
    """
    print("å®¹é‡è¦åŠƒç¤ºç¯„")
    print("=" * 60)

    requirements = CapacityRequirements(
        vector_count=10_000_000,
        dimension=768,
        qps_target=5000,
        latency_target_ms=50,
        recall_target=0.95,
        availability_target=0.999
    )

    planner = CapacityPlanner(requirements)
    print(planner.generate_plan())


if __name__ == "__main__":
    demonstrate_capacity_planning()
```

---

## 11.6 æœ¬ç« å›é¡§

### æ ¸å¿ƒè¦é»

1. **æ•ˆèƒ½æŒ‡æ¨™**
   - å»¶é²ï¼šP50ã€P95ã€P99
   - ååé‡ï¼šQPS
   - ç²¾ç¢ºåº¦ï¼šRecallã€Precision

2. **ç´¢å¼•å„ªåŒ–**
   - IVF åƒæ•¸ï¼šnlistã€nprobe
   - HNSW åƒæ•¸ï¼šMã€efConstructionã€efSearch
   - PQ åƒæ•¸ï¼šmã€nbits

3. **æŸ¥è©¢å„ªåŒ–**
   - åµŒå…¥å¿«å–
   - æ‰¹é‡è™•ç†
   - å…©éšæ®µæª¢ç´¢

4. **ç›£æ§å‘Šè­¦**
   - é—œéµæŒ‡æ¨™ç›£æ§
   - é–¾å€¼å‘Šè­¦
   - è¶¨å‹¢åˆ†æ

5. **å®¹é‡è¦åŠƒ**
   - è³‡æºä¼°ç®—
   - ç¯€é»è¦åŠƒ
   - æˆæœ¬é ä¼°

### æœ€ä½³å¯¦è¸

- å®šæœŸé€²è¡Œè² è¼‰æ¸¬è©¦
- ç›£æ§ P99 å»¶é²è€Œéå¹³å‡å»¶é²
- é ç•™ 20% çš„è³‡æºé¤˜é‡
- å»ºç«‹å®Œæ•´çš„ç›£æ§å‘Šè­¦é«”ç³»

---

## æ€è€ƒé¡Œ

1. å¦‚ä½•åœ¨ä¸å½±éŸ¿ç·šä¸Šæœå‹™çš„æƒ…æ³ä¸‹é€²è¡Œç´¢å¼•åƒæ•¸èª¿å„ªï¼Ÿ

2. ç•¶ç³»çµ±å»¶é²çªç„¶å‡é«˜æ™‚ï¼Œæ‡‰è©²å¦‚ä½•å¿«é€Ÿå®šä½å•é¡Œï¼Ÿ

3. å¦‚ä½•è¨­è¨ˆä¸€å€‹è‡ªé©æ‡‰çš„å¿«å–ç­–ç•¥ï¼Œæ ¹æ“šæŸ¥è©¢ç†±åº¦å‹•æ…‹èª¿æ•´ï¼Ÿ

4. åœ¨å¤šç§Ÿæˆ¶å ´æ™¯ä¸‹ï¼Œå¦‚ä½•ç¢ºä¿å„ç§Ÿæˆ¶çš„æ•ˆèƒ½éš”é›¢ï¼Ÿ

5. å¦‚ä½•å¹³è¡¡ç›£æ§çš„ç²’åº¦å’Œç›£æ§æœ¬èº«çš„é–‹éŠ·ï¼Ÿ

---

ä¸‹ä¸€ç« ï¼Œæˆ‘å€‘å°‡é€šéä¸€å€‹å®Œæ•´çš„è‡ªå‹•é§•é§›æ„ŸçŸ¥ç³»çµ±æ¡ˆä¾‹ï¼Œå±•ç¤ºå‘é‡æœå°‹æŠ€è¡“çš„å¯¦éš›æ‡‰ç”¨ã€‚
