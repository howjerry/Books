# Chapter 13: QA èˆ‡ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²

> "The best time to catch a bug is before it reaches production. The second best time is immediately after." â€” DevOps Wisdom

ç•¶æˆ‘å€‘å®Œæˆäº† dbt æ¨¡å‹çš„é–‹ç™¼å’Œæ¸¬è©¦å¾Œï¼Œä¸‹ä¸€å€‹é—œéµæŒ‘æˆ°æ˜¯å¦‚ä½•å®‰å…¨ã€å¯é åœ°å°‡é€™äº›è®Šæ›´éƒ¨ç½²åˆ° QA å’Œç”Ÿç”¢ç’°å¢ƒã€‚M3 åœ˜éšŠåœ¨é€™å€‹éšæ®µç¶“æ­·äº†å¾æ‰‹å‹•éƒ¨ç½²åˆ°è‡ªå‹•åŒ–æµç¨‹çš„æ¼”é€²ï¼Œç´¯ç©äº†å¯¶è²´çš„å¯¦æˆ°ç¶“é©—ã€‚

æœ¬ç« å°‡æ·±å…¥æ¢è¨å¤šç’°å¢ƒéƒ¨ç½²ç­–ç•¥ï¼Œå¾ç’°å¢ƒé…ç½®è¨­è¨ˆåˆ°è—ç¶ éƒ¨ç½²å¯¦ä½œï¼Œå¾ rollback æ©Ÿåˆ¶åˆ°ç”Ÿç”¢ç›£æ§ï¼Œæä¾›ä¸€å¥—å®Œæ•´çš„éƒ¨ç½²æœ€ä½³å¯¦è¸ã€‚

## 13.1 å¤šç’°å¢ƒç­–ç•¥è¨­è¨ˆ

### 13.1.1 ç’°å¢ƒæ¶æ§‹è¦åŠƒ

M3 åœ˜éšŠæ¡ç”¨äº†ä¸‰å±¤ç’°å¢ƒæ¶æ§‹ï¼š

```
Development (dev)  â†’ é–‹ç™¼ç’°å¢ƒï¼Œå€‹äººæ²™ç®±
    â†“
Quality Assurance (qa) â†’ æ¸¬è©¦ç’°å¢ƒï¼Œåœ˜éšŠå…±äº«
    â†“
Production (prod) â†’ ç”Ÿç”¢ç’°å¢ƒï¼Œå¯¦éš›æ¥­å‹™
```

**ç’°å¢ƒéš”é›¢åŸå‰‡**

æ¯å€‹ç’°å¢ƒéƒ½æœ‰ç¨ç«‹çš„ï¼š
- BigQuery datasetï¼ˆä¾‹å¦‚ï¼š`dbt_dev`ã€`dbt_qa`ã€`dbt_prod`ï¼‰
- æœå‹™å¸³è™Ÿå’Œæ¬Šé™é…ç½®
- è³‡æ–™ä¾†æºï¼ˆdev/qa ä½¿ç”¨å–æ¨£æ•¸æ“šï¼Œprod ä½¿ç”¨å®Œæ•´æ•¸æ“šï¼‰

è®“æˆ‘å€‘çœ‹çœ‹å¯¦éš›çš„ BigQuery å°ˆæ¡ˆçµæ§‹ï¼š

```sql
-- é–‹ç™¼ç’°å¢ƒ
project_id: m3-analytics-dev
dataset: dbt_dev_jerry, dbt_dev_alice  -- æ¯å€‹é–‹ç™¼è€…æœ‰è‡ªå·±çš„ schema
source data: raw_data_sample  -- ä½¿ç”¨å–æ¨£æ•¸æ“šï¼ˆæœ€è¿‘ 7 å¤©ï¼‰

-- QA ç’°å¢ƒ
project_id: m3-analytics-qa
dataset: dbt_qa  -- åœ˜éšŠå…±äº«
source data: raw_data_sample  -- ä½¿ç”¨å–æ¨£æ•¸æ“šï¼ˆæœ€è¿‘ 30 å¤©ï¼‰

-- ç”Ÿç”¢ç’°å¢ƒ
project_id: m3-analytics-prod
dataset: dbt_prod  -- æ­£å¼ç’°å¢ƒ
source data: raw_data  -- å®Œæ•´æ­·å²æ•¸æ“š
```

é€™ç¨®éš”é›¢ç­–ç•¥çš„å„ªå‹¢ï¼š
1. **å®‰å…¨æ€§**ï¼šé–‹ç™¼ç’°å¢ƒçš„éŒ¯èª¤ä¸æœƒå½±éŸ¿ç”Ÿç”¢æ•¸æ“š
2. **æˆæœ¬æ§åˆ¶**ï¼šdev/qa ä½¿ç”¨å–æ¨£æ•¸æ“šï¼Œé™ä½æŸ¥è©¢æˆæœ¬
3. **ç¨ç«‹æ€§**ï¼šæ¯å€‹é–‹ç™¼è€…å¯ä»¥ç¨ç«‹æ¸¬è©¦ï¼Œä¸äº’ç›¸å¹²æ“¾

### 13.1.2 profiles.yml é…ç½®

dbt ä½¿ç”¨ `profiles.yml` ä¾†ç®¡ç†ä¸åŒç’°å¢ƒçš„é€£æ¥é…ç½®ã€‚M3 åœ˜éšŠçš„é…ç½®ç¤ºä¾‹ï¼š

```yaml
# ~/.dbt/profiles.yml
m3_migration:
  outputs:
    dev:
      type: bigquery
      method: oauth
      project: m3-analytics-dev
      dataset: "dbt_dev_{{ env_var('DBT_USER', 'default') }}"
      threads: 4
      timeout_seconds: 300
      location: asia-east1
      priority: interactive

    qa:
      type: bigquery
      method: service-account
      project: m3-analytics-qa
      dataset: dbt_qa
      threads: 8
      timeout_seconds: 600
      location: asia-east1
      priority: interactive
      keyfile: "{{ env_var('DBT_SERVICE_ACCOUNT_KEY') }}"

    prod:
      type: bigquery
      method: service-account
      project: m3-analytics-prod
      dataset: dbt_prod
      threads: 16
      timeout_seconds: 900
      location: asia-east1
      priority: batch
      keyfile: "{{ env_var('DBT_PROD_SERVICE_ACCOUNT_KEY') }}"

  target: dev  # é è¨­ç›®æ¨™ç’°å¢ƒ
```

**é…ç½®é‡é»èªªæ˜**

1. **èªè­‰æ–¹å¼**
   - devï¼šä½¿ç”¨ OAuthï¼ˆé–‹ç™¼è€…å€‹äººå¸³è™Ÿï¼‰
   - qa/prodï¼šä½¿ç”¨ Service Accountï¼ˆæœå‹™å¸³è™Ÿï¼‰

2. **åŸ·è¡Œç·’æ•¸èª¿æ•´**
   - dev: 4 threadsï¼ˆé¿å…éåº¦æ¶ˆè€—è³‡æºï¼‰
   - qa: 8 threadsï¼ˆåŠ å¿«æ¸¬è©¦é€Ÿåº¦ï¼‰
   - prod: 16 threadsï¼ˆå……åˆ†åˆ©ç”¨è³‡æºï¼ŒåŠ å¿«éƒ¨ç½²ï¼‰

3. **æŸ¥è©¢å„ªå…ˆç´š**
   - dev/qa: `interactive`ï¼ˆå³æ™‚äº’å‹•ï¼‰
   - prod: `batch`ï¼ˆæ‰¹æ¬¡è™•ç†ï¼Œé™ä½æˆæœ¬ï¼‰

4. **å‹•æ…‹ schema å‘½å**
   ```yaml
   dataset: "dbt_dev_{{ env_var('DBT_USER', 'default') }}"
   ```
   ä½¿ç”¨ç’°å¢ƒè®Šæ•¸å‹•æ…‹ç”Ÿæˆé–‹ç™¼è€…å°ˆå±¬çš„ schemaã€‚

**ä½¿ç”¨æ–¹å¼**

```bash
# é–‹ç™¼ç’°å¢ƒï¼ˆé è¨­ï¼‰
dbt run

# æ˜ç¢ºæŒ‡å®š QA ç’°å¢ƒ
dbt run --target qa

# ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²
dbt run --target prod --models state:modified+

# æª¢æŸ¥ç•¶å‰ä½¿ç”¨çš„ profile
dbt debug --target prod
```

### 13.1.3 ç’°å¢ƒè®Šæ•¸ç®¡ç†

M3 åœ˜éšŠä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¾†ç®¡ç†æ•æ„Ÿè³‡è¨Šå’Œç’°å¢ƒç‰¹å®šé…ç½®ã€‚

**é–‹ç™¼ç’°å¢ƒè¨­ç½®**

```bash
# ~/.bashrc æˆ– ~/.zshrc
export DBT_USER="jerry"
export DBT_PROJECT="m3-analytics-dev"

# Service account keysï¼ˆä¸è¦æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ï¼ï¼‰
export DBT_SERVICE_ACCOUNT_KEY="/path/to/qa-service-account.json"
export DBT_PROD_SERVICE_ACCOUNT_KEY="/path/to/prod-service-account.json"
```

**CI/CD ç’°å¢ƒè®Šæ•¸ï¼ˆGitHub Actionsï¼‰**

```yaml
# .github/workflows/dbt-deploy.yml
name: dbt Deploy

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      DBT_PROJECT: m3-analytics-prod

    steps:
      - uses: actions/checkout@v2

      - name: Set up service account
        run: |
          echo "${{ secrets.PROD_SERVICE_ACCOUNT_KEY }}" > sa-key.json
          export DBT_PROD_SERVICE_ACCOUNT_KEY="$(pwd)/sa-key.json"

      - name: Install dbt
        run: pip install dbt-bigquery

      - name: Deploy to production
        run: |
          dbt deps
          dbt run --target prod --select state:modified+ --defer --state ./prod-manifest/
          dbt test --target prod --select state:modified+
```

**dbt_project.yml ä¸­çš„ç’°å¢ƒè®Šæ•¸**

```yaml
# dbt_project.yml
name: 'm3_migration'
version: '1.0.0'

vars:
  # ç’°å¢ƒç‰¹å®šè®Šæ•¸
  source_dataset: "{{ env_var('SOURCE_DATASET', 'raw_data_sample') }}"
  date_range_days: "{{ env_var('DATE_RANGE_DAYS', '7') | int }}"

  # æ ¹æ“šç’°å¢ƒèª¿æ•´åƒæ•¸
  partition_expiration_days: >
    {{ 90 if target.name == 'prod' else 30 }}

models:
  m3_migration:
    +materialized: table
    +partition_by:
      field: event_date
      data_type: date
      granularity: day
```

**åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ç’°å¢ƒè®Šæ•¸**

```sql
-- models/staging/stg_order_events.sql
{{ config(
    materialized='incremental',
    unique_key='order_id',
    partition_by={
        "field": "order_date",
        "data_type": "date"
    }
) }}

with source as (
    select * from {{ source('raw_data', 'order_events') }}
    where order_date >= date_sub(current_date(),
                                  interval {{ var('date_range_days') }} day)
    {% if target.name == 'prod' %}
    -- ç”Ÿç”¢ç’°å¢ƒé¡å¤–éæ¿¾æ¢ä»¶
    and is_deleted = false
    {% endif %}
),

-- ... å…¶é¤˜è½‰æ›é‚è¼¯
```

### 13.1.4 ç’°å¢ƒç‰¹å®šé…ç½®ç®¡ç†

æœ‰äº›é…ç½®éœ€è¦æ ¹æ“šç’°å¢ƒå‹•æ…‹èª¿æ•´ã€‚M3 åœ˜éšŠä½¿ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š

**æ–¹æ³• 1ï¼šä½¿ç”¨ dbt è®Šæ•¸ (vars)**

```yaml
# dbt_project.yml
vars:
  # é–‹ç™¼ç’°å¢ƒä½¿ç”¨å°æ‰¹æ¬¡æ¸¬è©¦
  dev:
    batch_size: 1000
    enable_full_refresh: true

  # QA ç’°å¢ƒä½¿ç”¨ä¸­ç­‰æ‰¹æ¬¡
  qa:
    batch_size: 10000
    enable_full_refresh: true

  # ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨å¤§æ‰¹æ¬¡
  prod:
    batch_size: 100000
    enable_full_refresh: false  # é˜²æ­¢æ„å¤–å…¨é‡åˆ·æ–°
```

åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ï¼š

```sql
-- models/marts/daily_order_summary.sql
{% set batch_size = var('batch_size', 10000) %}

with orders as (
    select * from {{ ref('stg_orders') }}
    limit {{ batch_size if target.name != 'prod' else none }}
),
-- ...
```

**æ–¹æ³• 2ï¼šç’°å¢ƒç‰¹å®šçš„ seeds æ–‡ä»¶**

```
seeds/
â”œâ”€â”€ dev/
â”‚   â””â”€â”€ currency_rates.csv  -- æ¸¬è©¦ç”¨å‡æ•¸æ“š
â”œâ”€â”€ qa/
â”‚   â””â”€â”€ currency_rates.csv  -- QA ç’°å¢ƒæ•¸æ“š
â””â”€â”€ prod/
    â””â”€â”€ currency_rates.csv  -- ç”Ÿç”¢ç’°å¢ƒå¯¦éš›åŒ¯ç‡
```

```yaml
# dbt_project.yml
seeds:
  m3_migration:
    +enabled: true
    +schema: config
    +database: "{{ target.project }}"

  # æ ¹æ“šç’°å¢ƒè¼‰å…¥ä¸åŒçš„ seed
  m3_migration:
    dev:
      +enabled: "{{ target.name == 'dev' }}"
    qa:
      +enabled: "{{ target.name == 'qa' }}"
    prod:
      +enabled: "{{ target.name == 'prod' }}"
```

**æ–¹æ³• 3ï¼šä½¿ç”¨ dbt Packages çš„ç’°å¢ƒç®¡ç†**

```yaml
# packages.yml
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
  - package: calogica/dbt_expectations
    version: 0.9.0
```

```sql
-- macros/get_env_config.sql
{% macro get_refresh_strategy() %}
    {% if target.name == 'dev' %}
        {{ return('table') }}  -- é–‹ç™¼ç’°å¢ƒå¿«é€Ÿè¿­ä»£
    {% elif target.name == 'qa' %}
        {{ return('incremental') }}  -- QA æ¸¬è©¦å¢é‡é‚è¼¯
    {% else %}
        {{ return('incremental') }}  -- ç”Ÿç”¢ç’°å¢ƒå¢é‡æ›´æ–°
    {% endif %}
{% endmacro %}
```

### 13.1.5 ç’°å¢ƒåˆ‡æ›æª¢æŸ¥æ¸…å–®

åœ¨åˆ‡æ›ç’°å¢ƒæ™‚ï¼ŒM3 åœ˜éšŠä½¿ç”¨ä»¥ä¸‹æª¢æŸ¥æ¸…å–®ç¢ºä¿é…ç½®æ­£ç¢ºï¼š

```bash
#!/bin/bash
# scripts/check_env.sh - ç’°å¢ƒé…ç½®æª¢æŸ¥è…³æœ¬

ENV=$1  # dev, qa, or prod

echo "ğŸ” æª¢æŸ¥ç’°å¢ƒé…ç½®: $ENV"
echo "================================"

# 1. æª¢æŸ¥ dbt profile
echo "1. æª¢æŸ¥ dbt profile..."
dbt debug --target $ENV | grep "Connection test: OK" || {
    echo "âŒ dbt é€£æ¥å¤±æ•—"
    exit 1
}

# 2. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
echo "2. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸..."
if [ "$ENV" == "prod" ]; then
    if [ -z "$DBT_PROD_SERVICE_ACCOUNT_KEY" ]; then
        echo "âŒ ç¼ºå°‘ DBT_PROD_SERVICE_ACCOUNT_KEY"
        exit 1
    fi
fi

# 3. æª¢æŸ¥ BigQuery æ¬Šé™
echo "3. æª¢æŸ¥ BigQuery æ¬Šé™..."
bq ls --project_id=$(dbt debug --target $ENV 2>/dev/null | grep "project" | awk '{print $2}') > /dev/null || {
    echo "âŒ BigQuery æ¬Šé™ä¸è¶³"
    exit 1
}

# 4. æª¢æŸ¥ git åˆ†æ”¯ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
if [ "$ENV" == "prod" ]; then
    BRANCH=$(git branch --show-current)
    if [ "$BRANCH" != "main" ]; then
        echo "âš ï¸  è­¦å‘Š: ç•¶å‰ä¸åœ¨ main åˆ†æ”¯ (ç•¶å‰: $BRANCH)"
        read -p "æ˜¯å¦ç¹¼çºŒ? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi
fi

echo "================================"
echo "âœ… ç’°å¢ƒé…ç½®æª¢æŸ¥é€šé"
```

ä½¿ç”¨æ–¹å¼ï¼š

```bash
# éƒ¨ç½²å‰æª¢æŸ¥
./scripts/check_env.sh prod

# è¼¸å‡º:
# ğŸ” æª¢æŸ¥ç’°å¢ƒé…ç½®: prod
# ================================
# 1. æª¢æŸ¥ dbt profile...
# Connection test: OK
# 2. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸...
# 3. æª¢æŸ¥ BigQuery æ¬Šé™...
# 4. æª¢æŸ¥ git åˆ†æ”¯...
# ================================
# âœ… ç’°å¢ƒé…ç½®æª¢æŸ¥é€šé
```

## 13.2 éƒ¨ç½²å‰æª¢æŸ¥æ¸…å–®

åœ¨æ­£å¼éƒ¨ç½²åˆ° QA æˆ–ç”Ÿç”¢ç’°å¢ƒä¹‹å‰ï¼ŒM3 åœ˜éšŠå»ºç«‹äº†ä¸€å¥—åš´æ ¼çš„æª¢æŸ¥æµç¨‹ï¼Œç¢ºä¿ä»£ç¢¼å“è³ªå’Œéƒ¨ç½²å®‰å…¨ã€‚

### 13.2.1 ä»£ç¢¼å¯©æŸ¥è¦é»

**1. SQL é‚è¼¯å¯©æŸ¥**

å¯©æŸ¥è€…éœ€è¦æª¢æŸ¥ä»¥ä¸‹æ–¹é¢ï¼š

```sql
-- âŒ ä¸å¥½çš„å¯¦è¸ï¼šæ²’æœ‰åˆ†å€éæ¿¾
select * from {{ source('raw_data', 'events') }}
where user_id = 12345

-- âœ… å¥½çš„å¯¦è¸ï¼šä½¿ç”¨åˆ†å€éæ¿¾
select * from {{ source('raw_data', 'events') }}
where event_date = current_date()
  and user_id = 12345
```

**æª¢æŸ¥è¦é»æ¸…å–®**

```markdown
### SQL å“è³ªæª¢æŸ¥

- [ ] æ˜¯å¦ä½¿ç”¨äº†åˆ†å€éæ¿¾ï¼Ÿï¼ˆé¿å…å…¨è¡¨æƒæï¼‰
- [ ] JOIN æ¢ä»¶æ˜¯å¦æ­£ç¢ºï¼Ÿï¼ˆé¿å…ç¬›å¡çˆ¾ç©ï¼‰
- [ ] æ˜¯å¦æœ‰é‡è¤‡è¨ˆç®—ï¼Ÿï¼ˆå¯ä»¥ç”¨ CTE æˆ–è‡¨æ™‚è¡¨å„ªåŒ–ï¼‰
- [ ] èšåˆå‡½æ•¸æ˜¯å¦æ­£ç¢ºï¼Ÿï¼ˆCOUNT, SUM, AVG ç­‰ï¼‰
- [ ] æ—¥æœŸè™•ç†æ˜¯å¦æ­£ç¢ºï¼Ÿï¼ˆæ™‚å€ã€æ ¼å¼è½‰æ›ï¼‰
- [ ] NULL å€¼è™•ç†æ˜¯å¦å®Œå–„ï¼Ÿï¼ˆCOALESCE, IFNULLï¼‰

### dbt é…ç½®æª¢æŸ¥

- [ ] materialized ç­–ç•¥æ˜¯å¦åˆç†ï¼Ÿï¼ˆtable vs incremental vs viewï¼‰
- [ ] incremental æ¨¡å‹æ˜¯å¦æœ‰æ­£ç¢ºçš„ unique_keyï¼Ÿ
- [ ] partition_by é…ç½®æ˜¯å¦æ­£ç¢ºï¼Ÿ
- [ ] cluster_by æ˜¯å¦æœ‰åŠ©æ–¼æŸ¥è©¢æ•ˆèƒ½ï¼Ÿ
- [ ] schema å’Œ alias æ˜¯å¦ç¬¦åˆå‘½åè¦ç¯„ï¼Ÿ

### æ¸¬è©¦è¦†è“‹ç‡

- [ ] æ˜¯å¦æœ‰ not_null æ¸¬è©¦ï¼Ÿ
- [ ] æ˜¯å¦æœ‰ unique æ¸¬è©¦ï¼Ÿ
- [ ] æ˜¯å¦æœ‰ relationships æ¸¬è©¦ï¼Ÿ
- [ ] æ˜¯å¦æœ‰è‡ªå®šç¾© data testï¼Ÿ
- [ ] æ¸¬è©¦è¦†è“‹é—œéµæ¬„ä½å—ï¼Ÿï¼ˆè‡³å°‘ 70% è¦†è“‹ç‡ï¼‰

### æ–‡æª”å®Œæ•´æ€§

- [ ] æ¨¡å‹æ˜¯å¦æœ‰ descriptionï¼Ÿ
- [ ] é—œéµæ¬„ä½æ˜¯å¦æœ‰èªªæ˜ï¼Ÿ
- [ ] æ˜¯å¦æœ‰ä½¿ç”¨ç¯„ä¾‹ï¼Ÿ
- [ ] æ˜¯å¦æ›´æ–°äº† CHANGELOGï¼Ÿ
```

**2. Pull Request æª¢æŸ¥è…³æœ¬**

M3 åœ˜éšŠä½¿ç”¨è‡ªå‹•åŒ–è…³æœ¬è¼”åŠ©ä»£ç¢¼å¯©æŸ¥ï¼š

```bash
#!/bin/bash
# scripts/pr_check.sh - Pull Request è‡ªå‹•æª¢æŸ¥

echo "ğŸ” é–‹å§‹ Pull Request æª¢æŸ¥..."

# 1. æª¢æŸ¥ä¿®æ”¹çš„æ¨¡å‹
MODIFIED_MODELS=$(git diff --name-only origin/main | grep "models/.*\.sql$")

if [ -z "$MODIFIED_MODELS" ]; then
    echo "âœ… æ²’æœ‰æ¨¡å‹è®Šæ›´"
    exit 0
fi

echo "ğŸ“ æª¢æ¸¬åˆ°ä»¥ä¸‹æ¨¡å‹è®Šæ›´:"
echo "$MODIFIED_MODELS"
echo ""

# 2. ç·¨è­¯æª¢æŸ¥
echo "ğŸ”¨ Step 1: ç·¨è­¯æª¢æŸ¥..."
dbt compile --select state:modified --state ./prod-manifest/ || {
    echo "âŒ ç·¨è­¯å¤±æ•—"
    exit 1
}
echo "âœ… ç·¨è­¯é€šé"

# 3. æ¸¬è©¦æª¢æŸ¥
echo "ğŸ§ª Step 2: æ¸¬è©¦æª¢æŸ¥..."
dbt test --select state:modified+ --state ./prod-manifest/ || {
    echo "âŒ æ¸¬è©¦å¤±æ•—"
    exit 1
}
echo "âœ… æ¸¬è©¦é€šé"

# 4. æ–‡æª”æª¢æŸ¥
echo "ğŸ“– Step 3: æ–‡æª”æª¢æŸ¥..."
for model in $MODIFIED_MODELS; do
    if ! grep -q "description:" "$model"; then
        echo "âš ï¸  è­¦å‘Š: $model ç¼ºå°‘ description"
    fi
done

# 5. SQL é¢¨æ ¼æª¢æŸ¥ï¼ˆä½¿ç”¨ sqlfluffï¼‰
if command -v sqlfluff &> /dev/null; then
    echo "ğŸ¨ Step 4: SQL é¢¨æ ¼æª¢æŸ¥..."
    sqlfluff lint $MODIFIED_MODELS --dialect bigquery || {
        echo "âš ï¸  SQL é¢¨æ ¼æª¢æŸ¥æœ‰è­¦å‘Šï¼ˆä¸é˜»æ–·éƒ¨ç½²ï¼‰"
    }
fi

# 6. ç”Ÿæˆå½±éŸ¿åˆ†æå ±å‘Š
echo "ğŸ“Š Step 5: å½±éŸ¿åˆ†æ..."
dbt ls --select state:modified+ --state ./prod-manifest/ --output json > /tmp/impact_analysis.json

echo ""
echo "================================"
echo "âœ… Pull Request æª¢æŸ¥å®Œæˆ"
echo "================================"
echo "è®Šæ›´å½±éŸ¿ç¯„åœ:"
dbt ls --select state:modified+ --state ./prod-manifest/ | wc -l | xargs echo "å—å½±éŸ¿æ¨¡å‹æ•¸:"
```

**3. äººå·¥å¯©æŸ¥é‡é»**

é™¤äº†è‡ªå‹•åŒ–æª¢æŸ¥ï¼Œé‚„éœ€è¦äººå·¥å¯©æŸ¥ä»¥ä¸‹é …ç›®ï¼š

```python
# scripts/review_checklist.py
# ç”Ÿæˆäººå·¥å¯©æŸ¥æ¸…å–®

import json
import sys

def generate_review_checklist(impact_analysis_file):
    """æ ¹æ“šå½±éŸ¿åˆ†æç”Ÿæˆå¯©æŸ¥æ¸…å–®"""

    with open(impact_analysis_file) as f:
        models = [json.loads(line) for line in f]

    print("# äººå·¥å¯©æŸ¥æ¸…å–®")
    print("")

    # 1. é«˜é¢¨éšªè®Šæ›´
    print("## ğŸ”´ é«˜é¢¨éšªè®Šæ›´ï¼ˆéœ€è¦ç‰¹åˆ¥æ³¨æ„ï¼‰")
    high_risk = [m for m in models if 'mart' in m['path'] or 'prod_' in m['name']]
    if high_risk:
        for model in high_risk:
            print(f"- [ ] {model['name']} (è·¯å¾‘: {model['path']})")
            print(f"  - åŸå› : å½±éŸ¿ä¸‹æ¸¸æ¥­å‹™å ±è¡¨")
            print(f"  - å»ºè­°: æ¸¬è©¦æ•¸æ“šä¸€è‡´æ€§ï¼Œé€šçŸ¥ä¸‹æ¸¸ç”¨æˆ¶")
    else:
        print("- ç„¡é«˜é¢¨éšªè®Šæ›´")
    print("")

    # 2. å¢é‡æ¨¡å‹è®Šæ›´
    print("## ğŸŸ¡ å¢é‡æ¨¡å‹è®Šæ›´ï¼ˆéœ€è¦æ¸¬è©¦å¢é‡é‚è¼¯ï¼‰")
    incremental = [m for m in models if m.get('config', {}).get('materialized') == 'incremental']
    if incremental:
        for model in incremental:
            print(f"- [ ] {model['name']}")
            print(f"  - æª¢æŸ¥ unique_key æ˜¯å¦æ­£ç¢º")
            print(f"  - æ¸¬è©¦å¢é‡é‹è¡Œæ˜¯å¦æ­£å¸¸")
            print(f"  - ç¢ºèª merge é‚è¼¯ç„¡èª¤")
    else:
        print("- ç„¡å¢é‡æ¨¡å‹è®Šæ›´")
    print("")

    # 3. Schema è®Šæ›´
    print("## ğŸŸ¢ Schema è®Šæ›´ï¼ˆéœ€è¦é€šçŸ¥ä¸‹æ¸¸ï¼‰")
    print("- [ ] æª¢æŸ¥æ˜¯å¦æœ‰æ¬„ä½æ–°å¢/åˆªé™¤")
    print("- [ ] æª¢æŸ¥è³‡æ–™é¡å‹æ˜¯å¦è®Šæ›´")
    print("- [ ] æ›´æ–°ä¸‹æ¸¸ä¾è³´æ–‡æª”")
    print("")

    # 4. æ•ˆèƒ½å½±éŸ¿
    print("## âš¡ æ•ˆèƒ½å½±éŸ¿è©•ä¼°")
    print("- [ ] æŸ¥è©¢æˆæœ¬æ˜¯å¦åœ¨å¯æ¥å—ç¯„åœï¼Ÿï¼ˆ< $10 per runï¼‰")
    print("- [ ] åŸ·è¡Œæ™‚é–“æ˜¯å¦åˆç†ï¼Ÿï¼ˆ< 10 minutesï¼‰")
    print("- [ ] æ˜¯å¦éœ€è¦å¢åŠ  cluster_byï¼Ÿ")
    print("")

if __name__ == "__main__":
    generate_review_checklist("/tmp/impact_analysis.json")
```

è¼¸å‡ºç¤ºä¾‹ï¼š

```markdown
# äººå·¥å¯©æŸ¥æ¸…å–®

## ğŸ”´ é«˜é¢¨éšªè®Šæ›´ï¼ˆéœ€è¦ç‰¹åˆ¥æ³¨æ„ï¼‰
- [ ] daily_revenue_summary (è·¯å¾‘: models/marts/finance/)
  - åŸå› : å½±éŸ¿ä¸‹æ¸¸æ¥­å‹™å ±è¡¨
  - å»ºè­°: æ¸¬è©¦æ•¸æ“šä¸€è‡´æ€§ï¼Œé€šçŸ¥ä¸‹æ¸¸ç”¨æˆ¶

## ğŸŸ¡ å¢é‡æ¨¡å‹è®Šæ›´ï¼ˆéœ€è¦æ¸¬è©¦å¢é‡é‚è¼¯ï¼‰
- [ ] stg_order_events
  - æª¢æŸ¥ unique_key æ˜¯å¦æ­£ç¢º
  - æ¸¬è©¦å¢é‡é‹è¡Œæ˜¯å¦æ­£å¸¸
  - ç¢ºèª merge é‚è¼¯ç„¡èª¤

## ğŸŸ¢ Schema è®Šæ›´ï¼ˆéœ€è¦é€šçŸ¥ä¸‹æ¸¸ï¼‰
- [ ] æª¢æŸ¥æ˜¯å¦æœ‰æ¬„ä½æ–°å¢/åˆªé™¤
- [ ] æª¢æŸ¥è³‡æ–™é¡å‹æ˜¯å¦è®Šæ›´
- [ ] æ›´æ–°ä¸‹æ¸¸ä¾è³´æ–‡æª”

## âš¡ æ•ˆèƒ½å½±éŸ¿è©•ä¼°
- [ ] æŸ¥è©¢æˆæœ¬æ˜¯å¦åœ¨å¯æ¥å—ç¯„åœï¼Ÿï¼ˆ< $10 per runï¼‰
- [ ] åŸ·è¡Œæ™‚é–“æ˜¯å¦åˆç†ï¼Ÿï¼ˆ< 10 minutesï¼‰
- [ ] æ˜¯å¦éœ€è¦å¢åŠ  cluster_byï¼Ÿ
```

### 13.2.2 æ¸¬è©¦é©—è­‰æµç¨‹

**1. æœ¬åœ°æ¸¬è©¦ï¼ˆé–‹ç™¼ç’°å¢ƒï¼‰**

é–‹ç™¼è€…åœ¨æäº¤ PR ä¹‹å‰éœ€è¦å®Œæˆï¼š

```bash
# Step 1: å®Œæ•´ç·¨è­¯
dbt clean
dbt deps
dbt compile

# Step 2: é‹è¡Œä¿®æ”¹çš„æ¨¡å‹ï¼ˆåŒ…å«ä¸Šæ¸¸ä¾è³´ï¼‰
dbt run --select +my_new_model

# Step 3: é‹è¡Œæ‰€æœ‰ç›¸é—œæ¸¬è©¦
dbt test --select my_new_model+

# Step 4: ç”Ÿæˆä¸¦æª¢æŸ¥æ–‡æª”
dbt docs generate
dbt docs serve  # åœ¨ç€è¦½å™¨ä¸­æª¢æŸ¥æ–‡æª”

# Step 5: æª¢æŸ¥æŸ¥è©¢æˆæœ¬ï¼ˆå¯é¸ï¼‰
dbt run-operation query_cost --args '{model: my_new_model}'
```

**2. QA ç’°å¢ƒæ¸¬è©¦**

PR åˆä½µåˆ° develop åˆ†æ”¯å¾Œï¼Œè‡ªå‹•è§¸ç™¼ QA ç’°å¢ƒéƒ¨ç½²ï¼š

```yaml
# .github/workflows/deploy-qa.yml
name: Deploy to QA

on:
  push:
    branches: [develop]

jobs:
  deploy-qa:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install dbt-bigquery sqlfluff
          dbt deps

      - name: Deploy to QA
        env:
          DBT_SERVICE_ACCOUNT_KEY: ${{ secrets.QA_SERVICE_ACCOUNT_KEY }}
        run: |
          # è¨­ç½®æœå‹™å¸³è™Ÿ
          echo "$DBT_SERVICE_ACCOUNT_KEY" > sa-key.json
          export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/sa-key.json"

          # é‹è¡Œè®Šæ›´çš„æ¨¡å‹
          dbt run --target qa --select state:modified+ --defer --state ./manifests/prod/

          # é‹è¡Œæ¸¬è©¦
          dbt test --target qa --select state:modified+

      - name: Data Quality Check
        run: |
          # åŸ·è¡Œè‡ªå®šç¾©æ•¸æ“šå“è³ªæª¢æŸ¥
          python scripts/qa_validation.py

      - name: Notify Team
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'QA éƒ¨ç½²å¤±æ•—ï¼Œè«‹æª¢æŸ¥'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

**3. æ•¸æ“šä¸€è‡´æ€§é©—è­‰**

```python
# scripts/qa_validation.py
# QA ç’°å¢ƒæ•¸æ“šé©—è­‰

from google.cloud import bigquery
import sys

def validate_data_consistency():
    """é©—è­‰ QA ç’°å¢ƒæ•¸æ“šä¸€è‡´æ€§"""

    client = bigquery.Client(project='m3-analytics-qa')

    checks = [
        {
            'name': 'è¨˜éŒ„æ•¸æª¢æŸ¥',
            'query': '''
                select
                    count(*) as qa_count,
                    (select count(*) from `m3-analytics-dev.dbt_dev_jerry.stg_orders`) as dev_count
                from `m3-analytics-qa.dbt_qa.stg_orders`
            ''',
            'validation': lambda row: abs(row.qa_count - row.dev_count) / row.dev_count < 0.01  # å…è¨± 1% èª¤å·®
        },
        {
            'name': 'æ•¸æ“šæ–°é®®åº¦æª¢æŸ¥',
            'query': '''
                select max(order_date) as latest_date
                from `m3-analytics-qa.dbt_qa.stg_orders`
            ''',
            'validation': lambda row: (datetime.now().date() - row.latest_date).days <= 1
        },
        {
            'name': 'NULL å€¼æ¯”ä¾‹æª¢æŸ¥',
            'query': '''
                select
                    countif(customer_id is null) / count(*) as null_ratio
                from `m3-analytics-qa.dbt_qa.stg_orders`
            ''',
            'validation': lambda row: row.null_ratio < 0.05  # NULL å€¼ä¸è¶…é 5%
        }
    ]

    print("ğŸ” é–‹å§‹æ•¸æ“šä¸€è‡´æ€§é©—è­‰...")
    all_passed = True

    for check in checks:
        print(f"\næª¢æŸ¥: {check['name']}")
        result = client.query(check['query']).result()
        row = next(result)

        if check['validation'](row):
            print(f"âœ… {check['name']} é€šé")
        else:
            print(f"âŒ {check['name']} å¤±æ•—")
            print(f"   çµæœ: {dict(row)}")
            all_passed = False

    if not all_passed:
        sys.exit(1)

    print("\nâœ… æ‰€æœ‰æ•¸æ“šé©—è­‰é€šé")

if __name__ == "__main__":
    validate_data_consistency()
```

### 13.2.3 æ–‡æª”æ›´æ–°è¦æ±‚

æ¯æ¬¡éƒ¨ç½²å‰éƒ½éœ€è¦ç¢ºä¿æ–‡æª”æ˜¯æœ€æ–°çš„ï¼š

**1. æ¨¡å‹æ–‡æª”ï¼ˆschema.ymlï¼‰**

```yaml
# models/staging/schema.yml
version: 2

models:
  - name: stg_orders
    description: |
      è¨‚å–®äº‹ä»¶ staging å±¤æ¨¡å‹

      **æ•¸æ“šä¾†æº**: raw_data.order_events
      **æ›´æ–°é »ç‡**: æ¯å°æ™‚
      **æ•¸æ“šä¿ç•™**: 90 å¤©

      **è®Šæ›´æ­·å²**:
      - 2024-01-15: æ–°å¢ customer_segment æ¬„ä½
      - 2024-01-10: ä¿®æ”¹å¢é‡é‚è¼¯ï¼Œæ”¹ç”¨ order_updated_at

    columns:
      - name: order_id
        description: è¨‚å–®å”¯ä¸€è­˜åˆ¥ç¢¼
        tests:
          - unique
          - not_null

      - name: customer_id
        description: |
          å®¢æˆ¶ IDï¼Œé—œè¯åˆ° dim_customers
          **æ³¨æ„**: 2024-01-01 ä¹‹å‰çš„è¨‚å–®å¯èƒ½ç‚º NULL
        tests:
          - not_null
          - relationships:
              to: ref('dim_customers')
              field: customer_id

      - name: customer_segment
        description: |
          å®¢æˆ¶åˆ†ç¾¤æ¨™ç±¤ï¼ˆ2024-01-15 æ–°å¢ï¼‰
          - 'VIP': å¹´æ¶ˆè²» > $10,000
          - 'Regular': å¹´æ¶ˆè²» $1,000 - $10,000
          - 'New': é¦–æ¬¡è³¼è²·
```

**2. CHANGELOG ç¶­è­·**

```markdown
# CHANGELOG.md

## [Unreleased]

### Added
- æ–°å¢ `customer_segment` æ¬„ä½åˆ° `stg_orders` æ¨¡å‹
- æ–°å¢ `daily_customer_retention` mart æ¨¡å‹

### Changed
- å„ªåŒ– `stg_orders` å¢é‡é‚è¼¯ï¼Œæ”¹ç”¨ `order_updated_at` å–ä»£ `order_created_at`
- èª¿æ•´ `dim_customers` çš„ cluster_by é…ç½®ï¼Œæå‡æŸ¥è©¢æ•ˆèƒ½

### Fixed
- ä¿®å¾© `stg_payments` ä¸­é‡è¤‡è¨˜éŒ„çš„å•é¡Œï¼ˆ#234ï¼‰
- ä¿®æ­£ `daily_revenue` æ™‚å€è½‰æ›éŒ¯èª¤ï¼ˆ#245ï¼‰

## [1.2.0] - 2024-01-10

### Added
- å¯¦ä½œè—ç¶ éƒ¨ç½²æ©Ÿåˆ¶
- æ–°å¢è‡ªå‹•åŒ–æ•¸æ“šé©—è­‰æµç¨‹

...
```

**3. éƒ¨ç½²èªªæ˜æ–‡æª”**

```markdown
# docs/deployment/deploy-to-prod.md

# ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æŒ‡å—

## å‰ç½®æª¢æŸ¥

- [ ] æ‰€æœ‰æ¸¬è©¦é€šé
- [ ] Code review å®Œæˆï¼ˆè‡³å°‘ 2 ä½ approverï¼‰
- [ ] QA ç’°å¢ƒé©—è­‰é€šé
- [ ] CHANGELOG å·²æ›´æ–°
- [ ] å½±éŸ¿åˆ†ææ–‡æª”å·²å®Œæˆ
- [ ] ä¸‹æ¸¸ç”¨æˆ¶å·²é€šçŸ¥ï¼ˆå¦‚æœ‰ schema è®Šæ›´ï¼‰

## éƒ¨ç½²æ­¥é©Ÿ

1. **ç¢ºèªåˆ†æ”¯**
   ```bash
   git checkout main
   git pull origin main
   ```

2. **åŸ·è¡Œéƒ¨ç½²å‰æª¢æŸ¥**
   ```bash
   ./scripts/check_env.sh prod
   ./scripts/pr_check.sh
   ```

3. **éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ**
   ```bash
   dbt run --target prod --select state:modified+ --defer --state ./manifests/prod/
   ```

4. **é‹è¡Œæ¸¬è©¦**
   ```bash
   dbt test --target prod --select state:modified+
   ```

5. **æ›´æ–° manifest**
   ```bash
   dbt docs generate --target prod
   cp target/manifest.json manifests/prod/
   git add manifests/prod/manifest.json
   git commit -m "Update prod manifest after deployment"
   git push
   ```

## Rollback æµç¨‹

å¦‚æœéƒ¨ç½²å¾Œç™¼ç¾å•é¡Œï¼Œç«‹å³åŸ·è¡Œï¼š

```bash
./scripts/rollback.sh --version previous
```

è©³è¦‹ [Rollback æ©Ÿåˆ¶](#rollback-mechanism)ã€‚

## ç›£æ§

éƒ¨ç½²å¾ŒæŒçºŒç›£æ§ 30 åˆ†é˜ï¼š
- BigQuery æŸ¥è©¢æˆæœ¬
- dbt é‹è¡Œæ™‚é–“
- æ•¸æ“šæ–°é®®åº¦
- ä¸‹æ¸¸å ±è¡¨ç•°å¸¸å‘Šè­¦

ç›£æ§é¢æ¿: https://datastudio.google.com/monitoring-dashboard
```

---

**å°çµ**

ç¬¬ 1-2 ç¯€å»ºç«‹äº†å®Œæ•´çš„å¤šç’°å¢ƒç®¡ç†å’Œéƒ¨ç½²å‰æª¢æŸ¥é«”ç³»ï¼š

1. **ç’°å¢ƒæ¶æ§‹**: dev/qa/prod ä¸‰å±¤éš”é›¢ï¼Œç¢ºä¿å®‰å…¨å’Œæˆæœ¬æ§åˆ¶
2. **é…ç½®ç®¡ç†**: ä½¿ç”¨ profiles.yml å’Œç’°å¢ƒè®Šæ•¸éˆæ´»ç®¡ç†ä¸åŒç’°å¢ƒ
3. **ä»£ç¢¼å¯©æŸ¥**: è‡ªå‹•åŒ– + äººå·¥å¯©æŸ¥ï¼Œç¢ºä¿ä»£ç¢¼å“è³ª
4. **æ¸¬è©¦é©—è­‰**: æœ¬åœ° â†’ QA â†’ ç”Ÿç”¢ï¼Œé€å±¤é©—è­‰
5. **æ–‡æª”ç¶­è­·**: æ¨¡å‹æ–‡æª”ã€CHANGELOGã€éƒ¨ç½²æŒ‡å—ä¿æŒåŒæ­¥

é€™äº›æº–å‚™å·¥ä½œç‚ºå®‰å…¨å¯é çš„ç”Ÿç”¢éƒ¨ç½²å¥ å®šäº†åŸºç¤ã€‚æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°‡æ·±å…¥æ¢è¨è—ç¶ éƒ¨ç½²å’Œ rollback æ©Ÿåˆ¶çš„å¯¦ä½œç´°ç¯€ã€‚

## 13.3 è—ç¶ éƒ¨ç½²å¯¦ä½œ

è—ç¶ éƒ¨ç½²ï¼ˆBlue-Green Deploymentï¼‰æ˜¯ä¸€ç¨®é›¶åœæ©Ÿéƒ¨ç½²ç­–ç•¥ï¼Œé€šéç¶­è­·å…©å¥—å®Œå…¨ç›¸åŒçš„ç”Ÿç”¢ç’°å¢ƒä¾†å¯¦ç¾ç„¡ç¸«åˆ‡æ›ã€‚M3 åœ˜éšŠåœ¨ dbt é·ç§»ä¸­æˆåŠŸæ‡‰ç”¨äº†é€™ç¨®æ¨¡å¼ã€‚

### 13.3.1 è—ç¶ éƒ¨ç½²åŸç†

**åŸºæœ¬æ¦‚å¿µ**

```
ç”Ÿç”¢ç’°å¢ƒåˆ†ç‚ºå…©å¥—ï¼š
- è—ç’°å¢ƒï¼ˆBlueï¼‰: ç•¶å‰æ­£åœ¨æœå‹™çš„ç‰ˆæœ¬
- ç¶ ç’°å¢ƒï¼ˆGreenï¼‰: æ–°ç‰ˆæœ¬éƒ¨ç½²æ¸¬è©¦ç’°å¢ƒ

éƒ¨ç½²æµç¨‹ï¼š
1. ç¶ ç’°å¢ƒéƒ¨ç½²æ–°ç‰ˆæœ¬
2. é©—è­‰ç¶ ç’°å¢ƒæ•¸æ“šæ­£ç¢ºæ€§
3. åˆ‡æ›æµé‡åˆ°ç¶ ç’°å¢ƒ
4. è—ç’°å¢ƒè®Šæˆä¸‹æ¬¡éƒ¨ç½²çš„å€™é¸ç’°å¢ƒ
```

**åœ¨ dbt ä¸­çš„å¯¦ç¾**

M3 åœ˜éšŠä½¿ç”¨ BigQuery schema ä¾†å¯¦ç¾è—ç¶ éƒ¨ç½²ï¼š

```sql
-- ç•¶å‰ç”Ÿç”¢ç’°å¢ƒ
dbt_prod_blue   (è—ç’°å¢ƒï¼Œæ­£åœ¨æœå‹™ä¸­)
dbt_prod_green  (ç¶ ç’°å¢ƒï¼Œæº–å‚™éƒ¨ç½²)

-- å¤–éƒ¨è¦–åœ–æŒ‡å‘ç•¶å‰æ´»èºç’°å¢ƒ
dbt_prod        â†’ æŒ‡å‘ dbt_prod_blueï¼ˆç•¶å‰ï¼‰
```

### 13.3.2 å¯¦ä½œç´°ç¯€

**1. ç’°å¢ƒé…ç½®**

```yaml
# profiles.yml - è—ç¶ ç’°å¢ƒé…ç½®
m3_migration:
  outputs:
    prod_blue:
      type: bigquery
      project: m3-analytics-prod
      dataset: dbt_prod_blue
      threads: 16
      keyfile: "{{ env_var('DBT_PROD_SERVICE_ACCOUNT_KEY') }}"

    prod_green:
      type: bigquery
      project: m3-analytics-prod
      dataset: dbt_prod_green
      threads: 16
      keyfile: "{{ env_var('DBT_PROD_SERVICE_ACCOUNT_KEY') }}"

    prod:  # æŒ‡å‘ç•¶å‰æ´»èºç’°å¢ƒ
      type: bigquery
      project: m3-analytics-prod
      dataset: dbt_prod
      threads: 16
      keyfile: "{{ env_var('DBT_PROD_SERVICE_ACCOUNT_KEY') }}"
```

**2. éƒ¨ç½²è…³æœ¬**

```bash
#!/bin/bash
# scripts/blue_green_deploy.sh - è—ç¶ éƒ¨ç½²ä¸»è…³æœ¬

set -e  # é‡åˆ°éŒ¯èª¤ç«‹å³é€€å‡º

# æª¢æ¸¬ç•¶å‰æ´»èºç’°å¢ƒ
CURRENT_ENV=$(bq query --project_id=m3-analytics-prod --use_legacy_sql=false \
  "SELECT schema_name FROM \`m3-analytics-prod.INFORMATION_SCHEMA.SCHEMATA\`
   WHERE schema_name = 'dbt_prod'" | grep dbt_prod | awk '{print $1}')

# ç¢ºå®šç›®æ¨™éƒ¨ç½²ç’°å¢ƒ
if [ "$CURRENT_ENV" == "dbt_prod_blue" ]; then
    TARGET_ENV="green"
    TARGET_SCHEMA="dbt_prod_green"
    echo "ğŸ”µ ç•¶å‰ç’°å¢ƒ: Blue, éƒ¨ç½²åˆ°: Green"
else
    TARGET_ENV="blue"
    TARGET_SCHEMA="dbt_prod_blue"
    echo "ğŸŸ¢ ç•¶å‰ç’°å¢ƒ: Green, éƒ¨ç½²åˆ°: Blue"
fi

echo "================================"
echo "é–‹å§‹è—ç¶ éƒ¨ç½²æµç¨‹"
echo "ç›®æ¨™ç’°å¢ƒ: $TARGET_ENV ($TARGET_SCHEMA)"
echo "================================"

# Step 1: éƒ¨ç½²åˆ°ç›®æ¨™ç’°å¢ƒ
echo ""
echo "ğŸ“¦ Step 1: éƒ¨ç½²æ–°ç‰ˆæœ¬åˆ° $TARGET_ENV ç’°å¢ƒ..."
dbt run --target prod_$TARGET_ENV --full-refresh

# Step 2: é‹è¡Œæ¸¬è©¦
echo ""
echo "ğŸ§ª Step 2: é‹è¡Œæ¸¬è©¦é©—è­‰..."
dbt test --target prod_$TARGET_ENV

# Step 3: æ•¸æ“šä¸€è‡´æ€§é©—è­‰
echo ""
echo "ğŸ” Step 3: æ•¸æ“šä¸€è‡´æ€§é©—è­‰..."
python scripts/validate_blue_green.py --target $TARGET_ENV || {
    echo "âŒ æ•¸æ“šé©—è­‰å¤±æ•—ï¼Œå–æ¶ˆéƒ¨ç½²"
    exit 1
}

# Step 4: åˆ‡æ›æµé‡
echo ""
echo "ğŸ”„ Step 4: æº–å‚™åˆ‡æ›æµé‡..."
read -p "æ•¸æ“šé©—è­‰é€šéï¼Œæ˜¯å¦åˆ‡æ›åˆ° $TARGET_ENV ç’°å¢ƒï¼Ÿ (yes/no): " confirm

if [ "$confirm" != "yes" ]; then
    echo "âŒ éƒ¨ç½²å·²å–æ¶ˆ"
    exit 0
fi

# åŸ·è¡Œæµé‡åˆ‡æ›
./scripts/switch_environment.sh $TARGET_ENV

echo ""
echo "================================"
echo "âœ… è—ç¶ éƒ¨ç½²å®Œæˆï¼"
echo "ç•¶å‰æ´»èºç’°å¢ƒ: $TARGET_ENV"
echo "================================"
```

**3. æ•¸æ“šé©—è­‰è…³æœ¬**

```python
# scripts/validate_blue_green.py
# è—ç¶ ç’°å¢ƒæ•¸æ“šä¸€è‡´æ€§é©—è­‰

import argparse
from google.cloud import bigquery
import sys

def compare_environments(target_env):
    """æ¯”è¼ƒè—ç¶ ç’°å¢ƒçš„æ•¸æ“šä¸€è‡´æ€§"""

    client = bigquery.Client(project='m3-analytics-prod')

    # ç¢ºå®šæ¯”è¼ƒå°è±¡
    source_env = 'blue' if target_env == 'green' else 'green'

    # é—œéµæ¨¡å‹åˆ—è¡¨
    critical_models = [
        'daily_revenue_summary',
        'customer_retention_cohort',
        'product_performance_metrics'
    ]

    print(f"ğŸ” æ¯”è¼ƒç’°å¢ƒ: {source_env} vs {target_env}")
    print("=" * 50)

    all_passed = True

    for model in critical_models:
        print(f"\næª¢æŸ¥æ¨¡å‹: {model}")

        # 1. è¨˜éŒ„æ•¸æ¯”è¼ƒ
        query = f"""
        SELECT
            (SELECT COUNT(*) FROM `m3-analytics-prod.dbt_prod_{source_env}.{model}`) as source_count,
            (SELECT COUNT(*) FROM `m3-analytics-prod.dbt_prod_{target_env}.{model}`) as target_count
        """

        result = client.query(query).result()
        row = next(result)

        diff_pct = abs(row.source_count - row.target_count) / row.source_count * 100 if row.source_count > 0 else 0

        if diff_pct > 1:  # å…è¨± 1% èª¤å·®
            print(f"  âŒ è¨˜éŒ„æ•¸å·®ç•°éå¤§: {source_env}={row.source_count}, {target_env}={row.target_count} (å·®ç•°: {diff_pct:.2f}%)")
            all_passed = False
        else:
            print(f"  âœ… è¨˜éŒ„æ•¸ä¸€è‡´: {row.target_count} ç­† (å·®ç•°: {diff_pct:.2f}%)")

        # 2. é—œéµæŒ‡æ¨™æ¯”è¼ƒï¼ˆä»¥ daily_revenue_summary ç‚ºä¾‹ï¼‰
        if model == 'daily_revenue_summary':
            query = f"""
            SELECT
                ABS(
                    (SELECT SUM(total_revenue) FROM `m3-analytics-prod.dbt_prod_{source_env}.{model}` WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)) -
                    (SELECT SUM(total_revenue) FROM `m3-analytics-prod.dbt_prod_{target_env}.{model}` WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY))
                ) / (SELECT SUM(total_revenue) FROM `m3-analytics-prod.dbt_prod_{source_env}.{model}` WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)) * 100 as revenue_diff_pct
            """

            result = client.query(query).result()
            row = next(result)

            if row.revenue_diff_pct > 0.1:  # ç‡Ÿæ”¶å·®ç•°ä¸è¶…é 0.1%
                print(f"  âŒ ç‡Ÿæ”¶æ•¸æ“šå·®ç•°: {row.revenue_diff_pct:.4f}%")
                all_passed = False
            else:
                print(f"  âœ… ç‡Ÿæ”¶æ•¸æ“šä¸€è‡´ (å·®ç•°: {row.revenue_diff_pct:.4f}%)")

    print("\n" + "=" * 50)

    if all_passed:
        print("âœ… æ‰€æœ‰é©—è­‰é€šéï¼Œå¯ä»¥å®‰å…¨åˆ‡æ›")
        return 0
    else:
        print("âŒ é©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ•¸æ“šå·®ç•°")
        return 1

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--target', required=True, choices=['blue', 'green'])
    args = parser.parse_args()

    sys.exit(compare_environments(args.target))
```

**4. æµé‡åˆ‡æ›è…³æœ¬**

```bash
#!/bin/bash
# scripts/switch_environment.sh - åˆ‡æ›æ´»èºç’°å¢ƒ

TARGET_ENV=$1  # blue or green

echo "ğŸ”„ é–‹å§‹åˆ‡æ›æµé‡åˆ° $TARGET_ENV ç’°å¢ƒ..."

# 1. æ›´æ–°è¦–åœ–æŒ‡å‘æ–°ç’°å¢ƒ
bq mk --force --view "
SELECT * FROM \`m3-analytics-prod.dbt_prod_${TARGET_ENV}.*\`
" m3-analytics-prod:dbt_prod

# 2. ä½¿ç”¨ authorized views ç¢ºä¿ä¸‹æ¸¸è¨ªå•æ¬Šé™
for table in daily_revenue_summary customer_retention_cohort product_performance_metrics; do
    bq mk --force --use_legacy_sql=false --view \
        "SELECT * FROM \`m3-analytics-prod.dbt_prod_${TARGET_ENV}.${table}\`" \
        m3-analytics-prod:dbt_prod.${table}

    echo "  âœ… å·²åˆ‡æ›: $table â†’ dbt_prod_${TARGET_ENV}.${table}"
done

# 3. è¨˜éŒ„åˆ‡æ›äº‹ä»¶
bq query --use_legacy_sql=false --project_id=m3-analytics-prod \
  "INSERT INTO \`m3-analytics-prod.dbt_metadata.deployment_log\`
   VALUES (CURRENT_TIMESTAMP(), '$TARGET_ENV', 'switch', 'success')"

echo "âœ… æµé‡åˆ‡æ›å®Œæˆï¼"
```

### 13.3.3 åˆ‡æ›æµç¨‹èˆ‡é©—è­‰

**å®Œæ•´éƒ¨ç½²æ™‚é–“è»¸**

```
T0  : é–‹å§‹éƒ¨ç½²åˆ°ç¶ ç’°å¢ƒ
T+10: éƒ¨ç½²å®Œæˆï¼Œé–‹å§‹æ¸¬è©¦
T+15: æ¸¬è©¦é€šéï¼Œé–‹å§‹æ•¸æ“šé©—è­‰
T+20: é©—è­‰é€šéï¼Œæº–å‚™åˆ‡æ›
T+21: åŸ·è¡Œæµé‡åˆ‡æ›ï¼ˆ< 1 åˆ†é˜ï¼‰
T+22: é–‹å§‹ç›£æ§æ–°ç’°å¢ƒ
T+52: ç›£æ§ 30 åˆ†é˜ç„¡ç•°å¸¸ï¼Œéƒ¨ç½²æˆåŠŸ
```

**éƒ¨ç½²æª¢æŸ¥æ¸…å–®**

```markdown
## è—ç¶ éƒ¨ç½²æª¢æŸ¥æ¸…å–®

### éƒ¨ç½²å‰
- [ ] ç¢ºèªç›®æ¨™ç’°å¢ƒï¼ˆè—æˆ–ç¶ ï¼‰
- [ ] å‚™ä»½ç•¶å‰ manifest.json
- [ ] é€šçŸ¥ä¸‹æ¸¸ç”¨æˆ¶è¨ˆåŠƒåˆ‡æ›æ™‚é–“
- [ ] æº–å‚™å›æ»¾è…³æœ¬

### éƒ¨ç½²ä¸­
- [ ] dbt run åŸ·è¡ŒæˆåŠŸ
- [ ] dbt test å…¨éƒ¨é€šé
- [ ] æ•¸æ“šä¸€è‡´æ€§é©—è­‰é€šéï¼ˆ< 1% å·®ç•°ï¼‰
- [ ] é—œéµæŒ‡æ¨™é©—è­‰é€šéï¼ˆç‡Ÿæ”¶ã€ç”¨æˆ¶æ•¸ç­‰ï¼‰

### åˆ‡æ›å¾Œ
- [ ] é©—è­‰è¦–åœ–æŒ‡å‘æ­£ç¢ºç’°å¢ƒ
- [ ] æª¢æŸ¥ä¸‹æ¸¸å ±è¡¨æ˜¯å¦æ­£å¸¸
- [ ] ç›£æ§æŸ¥è©¢æ€§èƒ½ï¼ˆå»¶é²ã€æˆæœ¬ï¼‰
- [ ] æŒçºŒç›£æ§ 30 åˆ†é˜
- [ ] è¨˜éŒ„éƒ¨ç½²æ—¥èªŒ
```

**ç›£æ§æŒ‡æ¨™**

```python
# scripts/monitor_deployment.py
# éƒ¨ç½²å¾Œç›£æ§

from google.cloud import bigquery, monitoring_v3
import time

def monitor_post_deployment(duration_minutes=30):
    """éƒ¨ç½²å¾ŒæŒçºŒç›£æ§"""

    client = bigquery.Client(project='m3-analytics-prod')
    monitoring_client = monitoring_v3.MetricServiceClient()

    print(f"ğŸ” é–‹å§‹ç›£æ§ {duration_minutes} åˆ†é˜...")

    start_time = time.time()
    check_interval = 60  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡

    while time.time() - start_time < duration_minutes * 60:
        print(f"\nâ° {int((time.time() - start_time) / 60)} åˆ†é˜...")

        # 1. æŸ¥è©¢æˆæœ¬ç›£æ§
        query = """
        SELECT
            SUM(total_bytes_processed) / POW(10, 12) as tb_processed,
            COUNT(*) as query_count
        FROM `m3-analytics-prod.region-asia-east1.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
        WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 MINUTE)
          AND statement_type = 'SELECT'
        """
        result = client.query(query).result()
        row = next(result)

        cost = row.tb_processed * 5  # BigQuery $5 per TB
        print(f"  ğŸ’° æŸ¥è©¢æˆæœ¬: ${cost:.2f} ({row.query_count} queries, {row.tb_processed:.2f} TB)")

        # 2. éŒ¯èª¤ç‡ç›£æ§
        query = """
        SELECT
            COUNTIF(error_result IS NOT NULL) / COUNT(*) * 100 as error_rate
        FROM `m3-analytics-prod.region-asia-east1.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
        WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 MINUTE)
        """
        result = client.query(query).result()
        row = next(result)

        if row.error_rate > 5:  # éŒ¯èª¤ç‡è¶…é 5%
            print(f"  âŒ éŒ¯èª¤ç‡éé«˜: {row.error_rate:.2f}%")
            return False
        else:
            print(f"  âœ… éŒ¯èª¤ç‡æ­£å¸¸: {row.error_rate:.2f}%")

        time.sleep(check_interval)

    print("\nâœ… ç›£æ§å®Œæˆï¼Œç„¡ç•°å¸¸")
    return True

if __name__ == "__main__":
    if not monitor_post_deployment(30):
        print("âš ï¸  ç›£æ§ç™¼ç¾ç•°å¸¸ï¼Œå»ºè­°å›æ»¾")
        sys.exit(1)
```

## 13.4 Rollback æ©Ÿåˆ¶

å³ä½¿æœ‰å®Œå–„çš„æ¸¬è©¦å’Œé©—è­‰ï¼Œç”Ÿç”¢ç’°å¢ƒä»å¯èƒ½å‡ºç¾æ„å¤–æƒ…æ³ã€‚å®Œå–„çš„ rollback æ©Ÿåˆ¶æ˜¯éƒ¨ç½²å®‰å…¨çš„æœ€å¾Œä¸€é“é˜²ç·šã€‚

### 13.4.1 å›æ»¾ç­–ç•¥

**å›æ»¾è§¸ç™¼æ¢ä»¶**

```markdown
## ä½•æ™‚éœ€è¦å›æ»¾ï¼Ÿ

ğŸ”´ **ç«‹å³å›æ»¾**ï¼ˆCriticalï¼‰
- æ•¸æ“šæº–ç¢ºæ€§å•é¡Œï¼ˆç‡Ÿæ”¶ã€ç”¨æˆ¶æ•¸ç­‰é—œéµæŒ‡æ¨™ç•°å¸¸ï¼‰
- ä¸‹æ¸¸å ±è¡¨å®Œå…¨ç„¡æ³•è¨ªå•
- æŸ¥è©¢éŒ¯èª¤ç‡ > 10%
- é‡å¤§å®‰å…¨æ¼æ´

ğŸŸ¡ **è©•ä¼°å¾Œå›æ»¾**ï¼ˆHighï¼‰
- æŸ¥è©¢æ€§èƒ½ä¸‹é™ > 50%
- éƒ¨åˆ†å ±è¡¨æ•¸æ“šç•°å¸¸
- æŸ¥è©¢æˆæœ¬å¢åŠ  > 3å€

ğŸŸ¢ **ç›£æ§è§€å¯Ÿ**ï¼ˆMediumï¼‰
- è¼•å¾®æ€§èƒ½ä¸‹é™ (< 20%)
- å€‹åˆ¥éé—œéµå ±è¡¨ç•°å¸¸
- æŸ¥è©¢æˆæœ¬å¢åŠ  < 2å€
```

**M3 åœ˜éšŠçš„å›æ»¾æ±ºç­–æµç¨‹**

```python
# scripts/rollback_decision.py
# è‡ªå‹•å›æ»¾æ±ºç­–

def should_rollback(metrics):
    """æ ¹æ“šæŒ‡æ¨™è‡ªå‹•åˆ¤æ–·æ˜¯å¦éœ€è¦å›æ»¾"""

    critical_issues = []
    high_issues = []

    # 1. æ•¸æ“šæº–ç¢ºæ€§æª¢æŸ¥
    if metrics.get('revenue_diff_pct', 0) > 5:
        critical_issues.append(f"ç‡Ÿæ”¶æ•¸æ“šç•°å¸¸: å·®ç•° {metrics['revenue_diff_pct']}%")

    # 2. éŒ¯èª¤ç‡æª¢æŸ¥
    if metrics.get('error_rate', 0) > 10:
        critical_issues.append(f"éŒ¯èª¤ç‡éé«˜: {metrics['error_rate']}%")

    # 3. æ€§èƒ½æª¢æŸ¥
    if metrics.get('latency_increase_pct', 0) > 50:
        high_issues.append(f"æŸ¥è©¢å»¶é²å¢åŠ : {metrics['latency_increase_pct']}%")

    # 4. æˆæœ¬æª¢æŸ¥
    if metrics.get('cost_increase_pct', 0) > 300:
        high_issues.append(f"æŸ¥è©¢æˆæœ¬å¢åŠ : {metrics['cost_increase_pct']}%")

    # æ±ºç­–é‚è¼¯
    if critical_issues:
        print("ğŸ”´ ç™¼ç¾åš´é‡å•é¡Œï¼Œå»ºè­°ç«‹å³å›æ»¾:")
        for issue in critical_issues:
            print(f"  - {issue}")
        return True, "CRITICAL"

    if len(high_issues) >= 2:
        print("ğŸŸ¡ ç™¼ç¾å¤šå€‹é«˜å„ªå…ˆç´šå•é¡Œï¼Œå»ºè­°å›æ»¾:")
        for issue in high_issues:
            print(f"  - {issue}")
        return True, "HIGH"

    print("ğŸŸ¢ æœªç™¼ç¾éœ€è¦å›æ»¾çš„å•é¡Œ")
    return False, "OK"
```

### 13.4.2 å¿«é€Ÿå›æ»¾å¯¦ä½œ

**1. è—ç¶ ç’°å¢ƒå¿«é€Ÿåˆ‡æ›**

```bash
#!/bin/bash
# scripts/rollback.sh - å¿«é€Ÿå›æ»¾åˆ°ä¸Šä¸€ç‰ˆæœ¬

set -e

echo "ğŸ”„ é–‹å§‹å›æ»¾æµç¨‹..."

# æª¢æ¸¬ç•¶å‰ç’°å¢ƒ
CURRENT_ENV=$(bq query --project_id=m3-analytics-prod --use_legacy_sql=false \
  "SELECT table_schema FROM \`m3-analytics-prod.dbt_prod.INFORMATION_SCHEMA.TABLES\` LIMIT 1" \
  | grep dbt_prod | awk -F'.' '{print $2}')

# åˆ‡æ›åˆ°å¦ä¸€å€‹ç’°å¢ƒ
if [[ "$CURRENT_ENV" == *"blue"* ]]; then
    ROLLBACK_TO="green"
    echo "ğŸ”µ ç•¶å‰: Blue â†’ å›æ»¾åˆ°: Green"
else
    ROLLBACK_TO="blue"
    echo "ğŸŸ¢ ç•¶å‰: Green â†’ å›æ»¾åˆ°: Blue"
fi

# åŸ·è¡Œåˆ‡æ›
./scripts/switch_environment.sh $ROLLBACK_TO

# è¨˜éŒ„å›æ»¾äº‹ä»¶
bq query --use_legacy_sql=false --project_id=m3-analytics-prod \
  "INSERT INTO \`m3-analytics-prod.dbt_metadata.deployment_log\`
   VALUES (CURRENT_TIMESTAMP(), '$ROLLBACK_TO', 'rollback', 'success')"

echo "âœ… å›æ»¾å®Œæˆï¼ç•¶å‰ç’°å¢ƒ: $ROLLBACK_TO"
echo "âš ï¸  è«‹ç«‹å³æª¢æŸ¥æ•¸æ“šå’Œå ±è¡¨"
```

**åŸ·è¡Œæ™‚é–“**ï¼š< 2 åˆ†é˜

**2. Manifest ç‰ˆæœ¬å›æ»¾**

å¦‚æœæ²’æœ‰ä½¿ç”¨è—ç¶ éƒ¨ç½²ï¼Œå¯ä»¥ä½¿ç”¨ dbt manifest é€²è¡Œç‰ˆæœ¬å›æ»¾ï¼š

```bash
#!/bin/bash
# scripts/rollback_manifest.sh - åŸºæ–¼ manifest çš„å›æ»¾

VERSION=${1:-"previous"}  # é»˜èªå›æ»¾åˆ°ä¸Šä¸€ç‰ˆæœ¬

echo "ğŸ“¦ å›æ»¾åˆ°ç‰ˆæœ¬: $VERSION"

# 1. æ¢å¾©èˆŠç‰ˆ manifest
if [ "$VERSION" == "previous" ]; then
    cp manifests/prod/manifest.json.backup manifests/prod/manifest.json
else
    cp manifests/prod/manifest.json.$VERSION manifests/prod/manifest.json
fi

# 2. é‡æ–°éƒ¨ç½²èˆŠç‰ˆæœ¬æ¨¡å‹
dbt run --target prod --state manifests/prod/

# 3. é©—è­‰
dbt test --target prod

echo "âœ… Manifest å›æ»¾å®Œæˆ"
```

### 13.4.3 æ•¸æ“šæ¢å¾©ç­–ç•¥

**1. æ™‚é–“æ—…è¡Œï¼ˆTime Travelï¼‰**

BigQuery æ”¯æ´ 7 å¤©å…§çš„æ­·å²æ•¸æ“šæŸ¥è©¢ï¼š

```sql
-- æ¢å¾© 2 å°æ™‚å‰çš„æ•¸æ“š
CREATE OR REPLACE TABLE `m3-analytics-prod.dbt_prod.daily_revenue_summary` AS
SELECT * FROM `m3-analytics-prod.dbt_prod.daily_revenue_summary`
FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 2 HOUR);

-- é©—è­‰æ¢å¾©çš„æ•¸æ“š
SELECT
    COUNT(*) as recovered_rows,
    MAX(updated_at) as last_update
FROM `m3-analytics-prod.dbt_prod.daily_revenue_summary`;
```

**2. å¿«ç…§æ¢å¾©**

M3 åœ˜éšŠå®šæœŸå‰µå»ºé—œéµè¡¨çš„å¿«ç…§ï¼š

```bash
#!/bin/bash
# scripts/create_snapshot.sh - å‰µå»ºå¿«ç…§

TABLES=(
    "daily_revenue_summary"
    "customer_retention_cohort"
    "product_performance_metrics"
)

TIMESTAMP=$(date +%Y%m%d_%H%M%S)

for table in "${TABLES[@]}"; do
    echo "ğŸ“¸ å‰µå»ºå¿«ç…§: $table"

    bq cp -f \
        m3-analytics-prod:dbt_prod.$table \
        m3-analytics-prod:dbt_snapshots.${table}_${TIMESTAMP}

    echo "  âœ… å¿«ç…§å·²å‰µå»º: ${table}_${TIMESTAMP}"
done

# æ¸…ç† 30 å¤©å‰çš„å¿«ç…§
bq ls --max_results=1000 m3-analytics-prod:dbt_snapshots | \
    grep "_20" | \
    awk -v date=$(date -d '30 days ago' +%Y%m%d) '$1 < date {print $1}' | \
    xargs -I {} bq rm -f m3-analytics-prod:dbt_snapshots.{}
```

**ä½¿ç”¨å¿«ç…§æ¢å¾©**ï¼š

```bash
# æ¢å¾©åˆ°æŒ‡å®šå¿«ç…§
bq cp -f \
    m3-analytics-prod:dbt_snapshots.daily_revenue_summary_20240115_093000 \
    m3-analytics-prod:dbt_prod.daily_revenue_summary
```

**3. å¢é‡æ¨¡å‹å›æ»¾**

å°æ–¼å¢é‡æ¨¡å‹ï¼Œéœ€è¦ç‰¹åˆ¥è™•ç†ï¼š

```bash
#!/bin/bash
# scripts/rollback_incremental.sh

MODEL=$1
ROLLBACK_DATE=$2  # YYYY-MM-DD

echo "ğŸ”„ å›æ»¾å¢é‡æ¨¡å‹: $MODEL åˆ°æ—¥æœŸ: $ROLLBACK_DATE"

# 1. åˆªé™¤å›æ»¾æ—¥æœŸä¹‹å¾Œçš„æ•¸æ“š
bq query --use_legacy_sql=false --project_id=m3-analytics-prod "
DELETE FROM \`m3-analytics-prod.dbt_prod.$MODEL\`
WHERE _dbt_updated_at >= TIMESTAMP('$ROLLBACK_DATE')
"

# 2. é‡æ–°é‹è¡Œè©²æ—¥æœŸä¹‹å¾Œçš„å¢é‡é‚è¼¯
dbt run --target prod --select $MODEL --vars "{'start_date': '$ROLLBACK_DATE'}"

echo "âœ… å¢é‡æ¨¡å‹å›æ»¾å®Œæˆ"
```

## 13.5 ç›£æ§èˆ‡å‘Šè­¦

å®Œå–„çš„ç›£æ§ç³»çµ±æ˜¯åŠæ™‚ç™¼ç¾å•é¡Œã€æ¸›å°‘æ•…éšœå½±éŸ¿çš„é—œéµã€‚

### 13.5.1 ç›£æ§æŒ‡æ¨™é«”ç³»

M3 åœ˜éšŠå»ºç«‹äº†å››å±¤ç›£æ§æŒ‡æ¨™ï¼š

**1. åŸºç¤è¨­æ–½å±¤**

```python
# scripts/monitor_infrastructure.py
# åŸºç¤è¨­æ–½ç›£æ§

from google.cloud import monitoring_v3
import time

def monitor_bigquery_metrics():
    """ç›£æ§ BigQuery åŸºç¤æŒ‡æ¨™"""

    client = monitoring_v3.MetricServiceClient()
    project_name = f"projects/m3-analytics-prod"

    # æŸ¥è©¢ slot ä½¿ç”¨ç‡
    now = time.time()
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(now)},
        "start_time": {"seconds": int(now - 300)},  # æœ€è¿‘ 5 åˆ†é˜
    })

    results = client.list_time_series(
        request={
            "name": project_name,
            "filter": 'metric.type = "bigquery.googleapis.com/slots/total_allocated"',
            "interval": interval,
            "view": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,
        }
    )

    for result in results:
        print(f"Slot ä½¿ç”¨ç‡: {result.points[0].value.int64_value} slots")

        if result.points[0].value.int64_value > 1000:
            send_alert("BigQuery slot ä½¿ç”¨ç‡éé«˜", "HIGH")
```

**ç›£æ§æŒ‡æ¨™**ï¼š
- BigQuery slot ä½¿ç”¨ç‡
- æŸ¥è©¢éšŠåˆ—é•·åº¦
- å„²å­˜ç©ºé–“ä½¿ç”¨é‡
- API è«‹æ±‚å»¶é²

**2. dbt åŸ·è¡Œå±¤**

```yaml
# dbt-cloud/monitor_config.yml
# æˆ–ä½¿ç”¨ dbt Core + è‡ªå®šç¾©ç›£æ§

monitors:
  - name: dbt_run_duration
    metric: run_duration_seconds
    threshold: 600  # 10 åˆ†é˜
    severity: HIGH

  - name: dbt_run_failure_rate
    metric: failed_runs / total_runs
    threshold: 0.05  # 5%
    severity: CRITICAL

  - name: dbt_test_failure_rate
    metric: failed_tests / total_tests
    threshold: 0.02  # 2%
    severity: HIGH
```

**è‡ªå®šç¾© dbt ç›£æ§**ï¼š

```python
# scripts/monitor_dbt_runs.py

import json
from datetime import datetime, timedelta
from google.cloud import bigquery

def monitor_dbt_runs():
    """ç›£æ§ dbt é‹è¡Œç‹€æ…‹"""

    client = bigquery.Client(project='m3-analytics-prod')

    # åˆ†ææœ€è¿‘ 1 å°æ™‚çš„ dbt é‹è¡Œæ—¥èªŒ
    query = """
    WITH recent_runs AS (
        SELECT
            run_id,
            status,
            TIMESTAMP_DIFF(completed_at, started_at, SECOND) as duration_seconds,
            model_count,
            failed_models
        FROM `m3-analytics-prod.dbt_metadata.run_results`
        WHERE started_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
    )
    SELECT
        COUNT(*) as total_runs,
        COUNTIF(status = 'success') as successful_runs,
        COUNTIF(status = 'failed') as failed_runs,
        AVG(duration_seconds) as avg_duration,
        MAX(duration_seconds) as max_duration
    FROM recent_runs
    """

    result = client.query(query).result()
    row = next(result)

    # æª¢æŸ¥å¤±æ•—ç‡
    failure_rate = row.failed_runs / row.total_runs if row.total_runs > 0 else 0

    if failure_rate > 0.1:
        send_alert(
            title=f"dbt é‹è¡Œå¤±æ•—ç‡éé«˜: {failure_rate:.1%}",
            message=f"æœ€è¿‘ 1 å°æ™‚å…§ {row.failed_runs}/{row.total_runs} æ¬¡é‹è¡Œå¤±æ•—",
            severity="CRITICAL"
        )

    # æª¢æŸ¥åŸ·è¡Œæ™‚é–“
    if row.max_duration > 1800:  # 30 åˆ†é˜
        send_alert(
            title=f"dbt é‹è¡Œæ™‚é–“éé•·",
            message=f"æœ€é•·åŸ·è¡Œæ™‚é–“: {row.max_duration/60:.1f} åˆ†é˜",
            severity="HIGH"
        )

    print(f"âœ… dbt é‹è¡Œç›£æ§: {row.successful_runs}/{row.total_runs} æˆåŠŸ")
```

**3. æ•¸æ“šå“è³ªå±¤**

```sql
-- å‰µå»ºæ•¸æ“šå“è³ªç›£æ§è¦–åœ–
CREATE OR REPLACE VIEW `m3-analytics-prod.dbt_monitoring.data_quality_metrics` AS

WITH freshness_check AS (
    SELECT
        'daily_revenue_summary' as table_name,
        MAX(date) as latest_date,
        DATE_DIFF(CURRENT_DATE(), MAX(date), DAY) as days_stale
    FROM `m3-analytics-prod.dbt_prod.daily_revenue_summary`
),

completeness_check AS (
    SELECT
        'stg_orders' as table_name,
        COUNTIF(customer_id IS NULL) / COUNT(*) * 100 as null_pct
    FROM `m3-analytics-prod.dbt_prod.stg_orders`
    WHERE order_date = CURRENT_DATE()
)

SELECT * FROM freshness_check
UNION ALL
SELECT
    table_name,
    null_pct,
    CASE WHEN null_pct > 5 THEN 1 ELSE 0 END as alert_flag
FROM completeness_check;
```

**ç›£æ§è…³æœ¬**ï¼š

```python
# scripts/monitor_data_quality.py

def check_data_freshness():
    """æª¢æŸ¥æ•¸æ“šæ–°é®®åº¦"""

    client = bigquery.Client(project='m3-analytics-prod')

    query = """
    SELECT
        table_name,
        latest_date,
        days_stale
    FROM `m3-analytics-prod.dbt_monitoring.data_quality_metrics`
    WHERE days_stale > 1
    """

    stale_tables = list(client.query(query).result())

    if stale_tables:
        message = "ä»¥ä¸‹è¡¨æ ¼æ•¸æ“šä¸æ–°é®®:\n"
        for table in stale_tables:
            message += f"- {table.table_name}: æœ€å¾Œæ›´æ–° {table.days_stale} å¤©å‰\n"

        send_alert("æ•¸æ“šæ–°é®®åº¦å‘Šè­¦", message, "HIGH")
```

**4. æ¥­å‹™æŒ‡æ¨™å±¤**

```python
# scripts/monitor_business_metrics.py
# æ¥­å‹™æŒ‡æ¨™ç•°å¸¸æª¢æ¸¬

def detect_anomalies():
    """æª¢æ¸¬æ¥­å‹™æŒ‡æ¨™ç•°å¸¸"""

    client = bigquery.Client(project='m3-analytics-prod')

    # ä½¿ç”¨ç§»å‹•å¹³å‡å’Œæ¨™æº–å·®æª¢æ¸¬ç•°å¸¸
    query = """
    WITH daily_metrics AS (
        SELECT
            date,
            total_revenue,
            AVG(total_revenue) OVER (
                ORDER BY date
                ROWS BETWEEN 6 PRECEDING AND 1 PRECEDING
            ) as avg_revenue_7d,
            STDDEV(total_revenue) OVER (
                ORDER BY date
                ROWS BETWEEN 6 PRECEDING AND 1 PRECEDING
            ) as stddev_revenue_7d
        FROM `m3-analytics-prod.dbt_prod.daily_revenue_summary`
        WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
    )
    SELECT
        date,
        total_revenue,
        avg_revenue_7d,
        ABS(total_revenue - avg_revenue_7d) / NULLIF(stddev_revenue_7d, 0) as z_score
    FROM daily_metrics
    WHERE date = CURRENT_DATE()
    """

    result = client.query(query).result()
    row = next(result)

    # Z-score > 3 è¡¨ç¤ºç•°å¸¸ï¼ˆè¶…é 3 å€‹æ¨™æº–å·®ï¼‰
    if row.z_score > 3:
        change_pct = (row.total_revenue - row.avg_revenue_7d) / row.avg_revenue_7d * 100

        send_alert(
            title="ç‡Ÿæ”¶ç•°å¸¸æ³¢å‹•",
            message=f"""
            ä»Šæ—¥ç‡Ÿæ”¶: ${row.total_revenue:,.0f}
            7æ—¥å¹³å‡: ${row.avg_revenue_7d:,.0f}
            è®ŠåŒ–: {change_pct:+.1f}%
            ç•°å¸¸åˆ†æ•¸: {row.z_score:.2f}
            """,
            severity="CRITICAL"
        )
```

### 13.5.2 å‘Šè­¦é…ç½®

**å‘Šè­¦å„ªå…ˆç´šå®šç¾©**

```python
# config/alert_levels.py

ALERT_LEVELS = {
    'CRITICAL': {
        'channels': ['pagerduty', 'slack', 'email'],
        'response_time': '15 minutes',
        'escalation': True
    },
    'HIGH': {
        'channels': ['slack', 'email'],
        'response_time': '1 hour',
        'escalation': False
    },
    'MEDIUM': {
        'channels': ['slack'],
        'response_time': '4 hours',
        'escalation': False
    },
    'LOW': {
        'channels': ['email'],
        'response_time': '24 hours',
        'escalation': False
    }
}
```

**Slack å‘Šè­¦æ•´åˆ**

```python
# scripts/alerting.py

import requests
import json

def send_alert(title, message, severity='HIGH'):
    """ç™¼é€å‘Šè­¦åˆ° Slack"""

    webhook_url = os.getenv('SLACK_WEBHOOK_URL')

    color_map = {
        'CRITICAL': '#FF0000',
        'HIGH': '#FFA500',
        'MEDIUM': '#FFFF00',
        'LOW': '#00FF00'
    }

    payload = {
        "attachments": [{
            "color": color_map.get(severity, '#808080'),
            "title": f"[{severity}] {title}",
            "text": message,
            "footer": "M3 dbt Monitoring",
            "ts": int(time.time())
        }]
    }

    response = requests.post(
        webhook_url,
        data=json.dumps(payload),
        headers={'Content-Type': 'application/json'}
    )

    if response.status_code != 200:
        print(f"âŒ å‘Šè­¦ç™¼é€å¤±æ•—: {response.text}")
    else:
        print(f"âœ… å‘Šè­¦å·²ç™¼é€: {title}")
```

### 13.5.3 ç›£æ§å„€è¡¨æ¿

M3 åœ˜éšŠä½¿ç”¨ Looker Studio (Data Studio) å»ºç«‹ç›£æ§å„€è¡¨æ¿ï¼š

**å„€è¡¨æ¿çµæ§‹**

```markdown
## dbt ç”Ÿç”¢ç›£æ§å„€è¡¨æ¿

### ç¬¬ä¸€å€å¡Šï¼šåŸ·è¡Œæ¦‚æ³
- ä»Šæ—¥ dbt é‹è¡Œæ¬¡æ•¸ï¼š12 æ¬¡
- æˆåŠŸç‡ï¼š100%
- å¹³å‡åŸ·è¡Œæ™‚é–“ï¼š8.5 åˆ†é˜
- è™•ç†æ•¸æ“šé‡ï¼š125 GB

### ç¬¬äºŒå€å¡Šï¼šæ•¸æ“šæ–°é®®åº¦
- âœ… daily_revenue_summary: 1 å°æ™‚å‰
- âœ… customer_retention_cohort: 2 å°æ™‚å‰
- âš ï¸  product_performance_metrics: 25 å°æ™‚å‰

### ç¬¬ä¸‰å€å¡Šï¼šæ¸¬è©¦çµæœ
- ç¸½æ¸¬è©¦æ•¸ï¼š284
- é€šéï¼š284 (100%)
- å¤±æ•—ï¼š0
- è­¦å‘Šï¼š2

### ç¬¬å››å€å¡Šï¼šæˆæœ¬åˆ†æ
- ä»Šæ—¥æŸ¥è©¢æˆæœ¬ï¼š$45.20
- æœ¬æœˆç´¯è¨ˆï¼š$1,234.50
- é ç®—ä½¿ç”¨ç‡ï¼š61.7%

### ç¬¬äº”å€å¡Šï¼šæ¥­å‹™æŒ‡æ¨™
- ä»Šæ—¥ç‡Ÿæ”¶ï¼š$125,450 (+2.3%)
- æ–°ç”¨æˆ¶ï¼š1,234 (+5.1%)
- è¨‚å–®æ•¸ï¼š5,678 (-1.2%)
```

**å„€è¡¨æ¿ SQL**

```sql
-- dbt é‹è¡Œçµ±è¨ˆ
CREATE OR REPLACE VIEW `m3-analytics-prod.dbt_monitoring.run_summary` AS
SELECT
    DATE(started_at) as run_date,
    COUNT(*) as total_runs,
    COUNTIF(status = 'success') as successful_runs,
    AVG(TIMESTAMP_DIFF(completed_at, started_at, SECOND)) as avg_duration_seconds,
    SUM(bytes_processed) / POW(10, 9) as total_gb_processed
FROM `m3-analytics-prod.dbt_metadata.run_results`
WHERE started_at >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY 1
ORDER BY 1 DESC;
```

---

**æœ¬ç« å°çµ**

ç¬¬ 13 ç« ç³»çµ±æ€§åœ°ä»‹ç´¹äº† dbt çš„ QA èˆ‡ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²å¯¦è¸ï¼š

1. **å¤šç’°å¢ƒç­–ç•¥**ï¼šdev/qa/prod ä¸‰å±¤æ¶æ§‹ï¼Œä½¿ç”¨ profiles.yml å’Œç’°å¢ƒè®Šæ•¸éˆæ´»ç®¡ç†
2. **éƒ¨ç½²æª¢æŸ¥**ï¼šè‡ªå‹•åŒ– + äººå·¥å¯©æŸ¥çš„é›™é‡ä¿éšœæ©Ÿåˆ¶
3. **è—ç¶ éƒ¨ç½²**ï¼šé›¶åœæ©Ÿéƒ¨ç½²ï¼Œå¿«é€Ÿåˆ‡æ›å’Œé©—è­‰
4. **Rollback æ©Ÿåˆ¶**ï¼šå¤šç¨®å›æ»¾ç­–ç•¥ï¼Œç¢ºä¿éƒ¨ç½²å®‰å…¨
5. **ç›£æ§å‘Šè­¦**ï¼šå››å±¤ç›£æ§é«”ç³»ï¼Œå¾åŸºç¤è¨­æ–½åˆ°æ¥­å‹™æŒ‡æ¨™å…¨é¢è¦†è“‹

é€™äº›å¯¦è¸ç¢ºä¿äº† M3 åœ˜éšŠèƒ½å¤ å®‰å…¨ã€å¯é åœ°å°‡ dbt è®Šæ›´éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒï¼ŒåŒæ™‚æœ€å°åŒ–æ¥­å‹™é¢¨éšªã€‚ä¸‹ä¸€ç« å°‡æ¢è¨åœ˜éšŠå”ä½œèˆ‡çŸ¥è­˜è³‡ç”¢åŒ–ï¼Œå¹«åŠ©çµ„ç¹”æ›´å¥½åœ°è¦æ¨¡åŒ– dbt å¯¦è¸ã€‚
