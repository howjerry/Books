"""
Main Application - å®Œæ•´æ‡‰ç”¨ç¨‹å¼é‡å¯«ç³»çµ±

æ•´åˆ Meta Agentï¼ˆè¦åŠƒå±¤ï¼‰ã€Task Coordinatorï¼ˆå”èª¿å±¤ï¼‰ã€Subagent Executorï¼ˆåŸ·è¡Œå±¤ï¼‰
"""

import asyncio
import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any
from dotenv import load_dotenv

from meta_agent import MetaAgent, ExecutionPlan
from task_coordinator import TaskCoordinator

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('application_rewrite.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ApplicationRewriteSystem:
    """
    å®Œæ•´æ‡‰ç”¨ç¨‹å¼é‡å¯«ç³»çµ±

    æ•´åˆä¸‰å±¤æ¶æ§‹ï¼š
    - Meta Agentï¼ˆè¦åŠƒå±¤ï¼‰
    - Task Coordinatorï¼ˆå”èª¿å±¤ï¼‰
    - Subagent Executorï¼ˆåŸ·è¡Œå±¤ï¼‰
    """

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.meta_agent = MetaAgent(api_key=api_key)
        self.execution_history = []

    async def rewrite_application(
        self,
        project_description: str,
        codebase_path: str,
        output_path: str,
        max_parallel: int = 3
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„æ‡‰ç”¨ç¨‹å¼é‡å¯«æµç¨‹

        Args:
            project_description: å°ˆæ¡ˆæè¿°
            codebase_path: åŸå§‹ç¨‹å¼ç¢¼è·¯å¾‘
            output_path: è¼¸å‡ºè·¯å¾‘
            max_parallel: æœ€å¤§ä¸¦è¡Œä»»å‹™æ•¸

        Returns:
            å®Œæ•´çš„åŸ·è¡Œå ±å‘Š
        """
        logger.info("=" * 80)
        logger.info("é–‹å§‹æ‡‰ç”¨ç¨‹å¼é‡å¯«å°ˆæ¡ˆ")
        logger.info("=" * 80)

        start_time = datetime.now()

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(output_path, exist_ok=True)

        # éšæ®µ 1ï¼šæƒæç¨‹å¼ç¢¼åº«
        logger.info("\n[éšæ®µ 1/4] æƒæç¨‹å¼ç¢¼åº«...")
        codebase_info = self._scan_codebase(codebase_path)
        logger.info(
            f"ç™¼ç¾ {codebase_info['total_files']} å€‹æª”æ¡ˆï¼Œ"
            f"{codebase_info['total_lines']:,} è¡Œç¨‹å¼ç¢¼"
        )

        # å„²å­˜ç¨‹å¼ç¢¼åº«è³‡è¨Š
        with open(os.path.join(output_path, "codebase_analysis.json"), 'w', encoding='utf-8') as f:
            json.dump(codebase_info, f, indent=2, ensure_ascii=False)

        # éšæ®µ 2ï¼šç”ŸæˆåŸ·è¡Œè¨ˆç•«
        logger.info("\n[éšæ®µ 2/4] ç”ŸæˆåŸ·è¡Œè¨ˆç•«...")
        plan = self.meta_agent.analyze_project(
            project_description,
            codebase_info
        )

        logger.info(f"è¨ˆç•«ç”Ÿæˆå®Œæˆï¼š")
        logger.info(f"  - ç¸½ä»»å‹™æ•¸ï¼š{len(plan.tasks)}")
        logger.info(f"  - é ä¼°æ™‚é–“ï¼š{plan.estimated_total_time // 60} åˆ†é˜")
        logger.info(f"  - é—œéµè·¯å¾‘ï¼š{len(plan.critical_path)} å€‹ä»»å‹™")
        logger.info(f"  - å¯ä¸¦è¡Œçµ„ï¼š{len(plan.parallel_groups)} çµ„")

        # å„²å­˜è¨ˆç•«
        self._save_plan(plan, output_path)

        # éšæ®µ 3ï¼šåŸ·è¡Œè¨ˆç•«
        logger.info("\n[éšæ®µ 3/4] åŸ·è¡Œé‡å¯«ä»»å‹™...")
        logger.info(f"æœ€å¤§ä¸¦è¡Œæ•¸ï¼š{max_parallel}")

        coordinator = TaskCoordinator(
            plan=plan,
            max_parallel=max_parallel,
            api_key=self.api_key
        )

        execution_result = await coordinator.execute_plan()

        # éšæ®µ 4ï¼šç”Ÿæˆå ±å‘Š
        logger.info("\n[éšæ®µ 4/4] ç”Ÿæˆæœ€çµ‚å ±å‘Š...")

        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()

        final_report = {
            "project": {
                "name": plan.project_name,
                "objective": plan.objective,
                "codebase_path": codebase_path,
                "output_path": output_path
            },
            "codebase_info": codebase_info,
            "execution": execution_result,
            "timing": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "total_duration": total_duration,
                "estimated_duration": plan.estimated_total_time,
                "efficiency": (
                    plan.estimated_total_time / total_duration
                    if total_duration > 0 else 0
                )
            },
            "quality_metrics": self._calculate_quality_metrics(execution_result)
        }

        # å„²å­˜å ±å‘Š
        self._save_report(final_report, output_path)

        # åˆ—å°æ‘˜è¦
        self._print_summary(final_report)

        return final_report

    def _scan_codebase(self, path: str) -> Dict[str, Any]:
        """
        æƒæç¨‹å¼ç¢¼åº«

        Returns:
            ç¨‹å¼ç¢¼åº«çµ±è¨ˆè³‡è¨Š
        """
        if not os.path.exists(path):
            logger.warning(f"è·¯å¾‘ä¸å­˜åœ¨ï¼š{path}ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            return {
                "total_files": 0,
                "total_lines": 0,
                "file_types": {},
                "path": path,
                "note": "è·¯å¾‘ä¸å­˜åœ¨ï¼Œé€™æ˜¯æ¨¡æ“¬æ•¸æ“š"
            }

        total_files = 0
        total_lines = 0
        file_types = {}
        largest_files = []

        for root, dirs, files in os.walk(path):
            # è·³ééš±è—ç›®éŒ„å’Œå¸¸è¦‹çš„å¿½ç•¥ç›®éŒ„
            dirs[:] = [
                d for d in dirs
                if not d.startswith('.') and d not in ['node_modules', 'venv', '__pycache__', 'dist', 'build']
            ]

            for file in files:
                if file.startswith('.'):
                    continue

                file_path = Path(root) / file
                suffix = file_path.suffix.lower() or 'no_extension'

                total_files += 1
                file_types[suffix] = file_types.get(suffix, 0) + 1

                # è¨ˆç®—è¡Œæ•¸
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        lines = len(f.readlines())
                        total_lines += lines

                        # è¨˜éŒ„å¤§æª”æ¡ˆ
                        largest_files.append({
                            "path": str(file_path),
                            "lines": lines,
                            "size_kb": file_path.stat().st_size / 1024
                        })
                except Exception as e:
                    logger.debug(f"ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}: {e}")

        # åªä¿ç•™å‰ 10 å¤§æª”æ¡ˆ
        largest_files.sort(key=lambda x: x["lines"], reverse=True)
        largest_files = largest_files[:10]

        return {
            "total_files": total_files,
            "total_lines": total_lines,
            "file_types": file_types,
            "largest_files": largest_files,
            "path": path
        }

    def _save_plan(self, plan: ExecutionPlan, output_path: str):
        """å„²å­˜åŸ·è¡Œè¨ˆç•«"""
        plan_file = os.path.join(output_path, "execution_plan.json")
        with open(plan_file, 'w', encoding='utf-8') as f:
            json.dump(plan.to_dict(), f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… åŸ·è¡Œè¨ˆç•«å·²å„²å­˜ï¼š{plan_file}")

    def _save_report(self, report: Dict[str, Any], output_path: str):
        """å„²å­˜æœ€çµ‚å ±å‘Š"""
        report_file = os.path.join(output_path, "final_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… æœ€çµ‚å ±å‘Šå·²å„²å­˜ï¼š{report_file}")

    def _calculate_quality_metrics(self, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—å“è³ªæŒ‡æ¨™"""
        summary = execution_result["summary"]

        total_cost = sum(
            task.get("result", {}).get("metrics", {}).get("total_cost", 0)
            for task in execution_result.get("completed_tasks", [])
        )

        return {
            "success_rate": summary["success_rate"],
            "time_efficiency": summary.get("time_efficiency", 0),
            "tasks_completed": summary["completed"],
            "tasks_failed": summary["failed"],
            "total_cost_usd": total_cost,
            "average_cost_per_task": (
                total_cost / summary["completed"]
                if summary["completed"] > 0 else 0
            )
        }

    def _print_summary(self, report: Dict[str, Any]):
        """åˆ—å°åŸ·è¡Œæ‘˜è¦"""
        logger.info("\n" + "=" * 80)
        logger.info("åŸ·è¡Œæ‘˜è¦")
        logger.info("=" * 80)

        project = report["project"]
        execution = report["execution"]["summary"]
        timing = report["timing"]
        quality = report["quality_metrics"]

        logger.info(f"\nå°ˆæ¡ˆï¼š{project['name']}")
        logger.info(f"ç›®æ¨™ï¼š{project['objective']}")

        logger.info(f"\nåŸ·è¡Œçµæœï¼š")
        logger.info(f"  âœ… å®Œæˆä»»å‹™ï¼š{execution['completed']}/{execution['total_tasks']}")
        logger.info(f"  âŒ å¤±æ•—ä»»å‹™ï¼š{execution['failed']}")
        logger.info(f"  ğŸ“Š æˆåŠŸç‡ï¼š{execution['success_rate']:.1%}")

        logger.info(f"\næ™‚é–“çµ±è¨ˆï¼š")
        logger.info(f"  â±ï¸  å¯¦éš›è€—æ™‚ï¼š{timing['total_duration'] / 60:.1f} åˆ†é˜")
        logger.info(f"  ğŸ“… é ä¼°è€—æ™‚ï¼š{timing['estimated_duration'] / 60:.1f} åˆ†é˜")
        logger.info(f"  âš¡ æ•ˆç‡æ¯”ï¼š{timing['efficiency']:.2f}x")

        logger.info(f"\nå“è³ªæŒ‡æ¨™ï¼š")
        logger.info(f"  ğŸ’° ç¸½æˆæœ¬ï¼š${quality['total_cost_usd']:.4f}")
        logger.info(f"  ğŸ’µ å¹³å‡æˆæœ¬ï¼š${quality['average_cost_per_task']:.4f}/ä»»å‹™")
        logger.info(f"  ğŸ¯ æ™‚é–“æ•ˆç‡ï¼š{quality['time_efficiency']:.2f}x")

        logger.info("\n" + "=" * 80)


async def main():
    """ä¸»ç¨‹å¼å…¥å£"""

    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    api_key = os.getenv("ANTHROPIC_API_KEY")

    if not api_key:
        logger.error("âŒ è«‹è¨­å®š ANTHROPIC_API_KEY ç’°å¢ƒè®Šæ•¸")
        return

    # å‰µå»ºç³»çµ±
    system = ApplicationRewriteSystem(api_key=api_key)

    # å®šç¾©é‡å¯«å°ˆæ¡ˆ
    project_description = """
å°‡ä¸€å€‹ 8 å¹´æ­·å²çš„ PHP å–®é«” ERP ç³»çµ±é‡å¯«ç‚º Python å¾®æœå‹™æ¶æ§‹ã€‚

## åŸç³»çµ±
- PHP 5.6 + MySQL
- ç´„ 30,000 è¡Œç¨‹å¼ç¢¼
- 4 å€‹æ ¸å¿ƒæ¨¡çµ„ï¼šå®¢æˆ¶ç®¡ç†ã€è¨‚å–®è™•ç†ã€åº«å­˜ç®¡ç†ã€å¸³å–®ç³»çµ±
- å–®é«”æ¶æ§‹ï¼Œæ‰€æœ‰åŠŸèƒ½åœ¨ä¸€å€‹æ‡‰ç”¨ä¸­

## ç›®æ¨™ç³»çµ±
- Python 3.11 + FastAPI
- PostgreSQL + Redis
- å¾®æœå‹™æ¶æ§‹ï¼ˆæ¯å€‹æ¨¡çµ„ç¨ç«‹æœå‹™ï¼‰
- RESTful API è¨­è¨ˆ
- Docker å®¹å™¨åŒ–éƒ¨ç½²
- å®Œæ•´çš„å–®å…ƒæ¸¬è©¦èˆ‡æ•´åˆæ¸¬è©¦ï¼ˆè¦†è“‹ç‡ > 90%ï¼‰
- API æ–‡ä»¶ï¼ˆOpenAPI/Swaggerï¼‰
- éƒ¨ç½²æŒ‡å—

## æŠ€è¡“è¦æ±‚
- ä½¿ç”¨ Pydantic é€²è¡Œè³‡æ–™é©—è­‰
- ä½¿ç”¨ SQLAlchemy é€²è¡Œ ORM
- å¯¦ä½œ JWT èªè­‰
- Redis å¿«å–ç†±è³‡æ–™
- å®Œæ•´çš„éŒ¯èª¤è™•ç†èˆ‡æ—¥èªŒè¨˜éŒ„
"""

    # åŸ·è¡Œé‡å¯«
    report = await system.rewrite_application(
        project_description=project_description,
        codebase_path="./legacy_erp",  # åŸå§‹ç¨‹å¼ç¢¼è·¯å¾‘
        output_path="./output/rewritten_system",  # è¼¸å‡ºè·¯å¾‘
        max_parallel=3  # æœ€å¤§ä¸¦è¡Œä»»å‹™æ•¸
    )

    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 80)
    print("âœ… é‡å¯«å®Œæˆï¼")
    print("=" * 80)
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„ï¼š{report['project']['output_path']}")
    print(f"ğŸ“Š åŸ·è¡Œè¨ˆç•«ï¼š{report['project']['output_path']}/execution_plan.json")
    print(f"ğŸ“„ è©³ç´°å ±å‘Šï¼š{report['project']['output_path']}/final_report.json")
    print(f"ğŸ’° ç¸½æˆæœ¬ï¼š${report['quality_metrics']['total_cost_usd']:.4f}")
    print(f"â±ï¸  ç¸½è€—æ™‚ï¼š{report['timing']['total_duration'] / 60:.1f} åˆ†é˜")
    print("=" * 80)


if __name__ == "__main__":
    # é‹è¡Œä¸»ç¨‹å¼
    asyncio.run(main())
