#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 9 ç« ï¼šå»ºæ§‹ä½ çš„ç¬¬ä¸€å€‹ç ”ç©¶ä»£ç†äºº
é«˜ç´šé©—è­‰æ¨¡çµ„

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†äº‹å¯¦æŸ¥è­‰åŠŸèƒ½ï¼š
1. å¤šä¾†æºäº¤å‰é©—è­‰
2. çŸ›ç›¾è­˜åˆ¥
3. å¯ä¿¡åº¦è©•ä¼°

ä½¿ç”¨æ–¹å¼ï¼š
    python verification.py --demo
"""

import asyncio
import argparse
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv

load_dotenv()


# =============================================================================
# è³‡æ–™çµæ§‹
# =============================================================================

@dataclass
class VerificationResult:
    """é©—è­‰çµæœ"""
    claim: str
    verified: bool
    confidence: float
    supporting_sources: int
    contradicting_sources: int
    status: str  # "verified", "likely", "conflict", "unverified"
    notes: str = ""


@dataclass
class ReportVerification:
    """å ±å‘Šé©—è­‰çµæœ"""
    overall_confidence: float
    verified_count: int
    total_claims: int
    conflicts: List[Dict[str, Any]]
    details: List[VerificationResult]


# =============================================================================
# æ¨¡æ“¬è³‡æ–™
# =============================================================================

@dataclass
class MockFinding:
    """æ¨¡æ“¬çš„ç ”ç©¶ç™¼ç¾"""
    content: str
    source_url: str
    relevance_score: float = 0.5


# =============================================================================
# é«˜ç´šé©—è­‰æ¨¡çµ„
# =============================================================================

class AdvancedVerificationModule:
    """
    é«˜ç´šé©—è­‰æ¨¡çµ„

    â€¹1â€º å¤šä¾†æºäº¤å‰é©—è­‰
    â€¹2â€º æ™‚æ•ˆæ€§æª¢æŸ¥
    â€¹3â€º å¯ä¿¡åº¦è©•åˆ†
    """

    def __init__(self, llm_client=None, search_module=None):
        self.llm = llm_client
        self.search = search_module

    async def verify_claim(
        self,
        claim: str,
        existing_sources: List[Any]
    ) -> VerificationResult:
        """
        é©—è­‰å–®ä¸€é™³è¿°

        â€¹1â€º æª¢æŸ¥ç¾æœ‰ä¾†æº
        â€¹2â€º æœå°‹é¡å¤–è­‰æ“š
        â€¹3â€º è©•ä¼°å¯ä¿¡åº¦
        """
        supporting_sources = []
        contradicting_sources = []

        # â€¹1â€º æª¢æŸ¥ç¾æœ‰ä¾†æºä¸­çš„æ”¯æŒè­‰æ“š
        for source in existing_sources:
            content = getattr(source, 'content', str(source))
            relevance = await self._check_relevance(claim, content)
            if relevance > 0.7:
                supporting_sources.append(source)
            elif relevance < 0.3:
                contradicting_sources.append(source)

        # â€¹2â€º è¨ˆç®—å¯ä¿¡åº¦
        confidence = self._calculate_confidence(
            supporting_count=len(supporting_sources),
            contradicting_count=len(contradicting_sources),
            total_sources=len(existing_sources)
        )

        # â€¹3â€º ç¢ºå®šç‹€æ…‹
        status = self._determine_status(confidence, len(contradicting_sources))

        return VerificationResult(
            claim=claim,
            verified=confidence > 0.7,
            confidence=confidence,
            supporting_sources=len(supporting_sources),
            contradicting_sources=len(contradicting_sources),
            status=status,
            notes=f"åŸºæ–¼ {len(existing_sources)} å€‹ä¾†æºé©—è­‰"
        )

    async def _check_relevance(self, claim: str, content: str) -> float:
        """æª¢æŸ¥å…§å®¹èˆ‡é™³è¿°çš„ç›¸é—œæ€§"""
        # ç°¡åŒ–å¯¦ç¾ï¼šåŸºæ–¼é—œéµè©é‡ç–Šåº¦
        claim_words = set(claim.lower().split())
        content_words = set(content.lower().split())
        overlap = len(claim_words & content_words)
        return min(1.0, overlap / max(len(claim_words), 1) * 1.5)

    def _calculate_confidence(
        self,
        supporting_count: int,
        contradicting_count: int,
        total_sources: int
    ) -> float:
        """è¨ˆç®—å¯ä¿¡åº¦åˆ†æ•¸"""
        if total_sources == 0:
            return 0.5

        base_score = supporting_count / max(total_sources, 1)
        penalty = contradicting_count * 0.2

        return max(0.0, min(1.0, base_score - penalty + 0.3))

    def _determine_status(
        self,
        confidence: float,
        contradicting_count: int
    ) -> str:
        """ç¢ºå®šé©—è­‰ç‹€æ…‹"""
        if contradicting_count > 0:
            return "conflict"
        elif confidence > 0.7:
            return "verified"
        elif confidence > 0.4:
            return "likely"
        else:
            return "unverified"

    async def verify_claims(
        self,
        claims: List[str],
        existing_sources: List[Any]
    ) -> List[VerificationResult]:
        """æ‰¹é‡é©—è­‰å¤šå€‹é™³è¿°"""
        results = []
        for claim in claims:
            result = await self.verify_claim(claim, existing_sources)
            results.append(result)
        return results

    async def verify_report(
        self,
        summary: str,
        key_findings: List[str],
        existing_sources: List[Any]
    ) -> ReportVerification:
        """é©—è­‰æ•´ä»½å ±å‘Š"""
        claims = key_findings + [summary]
        verification_results = await self.verify_claims(claims, existing_sources)

        overall_confidence = sum(
            r.confidence for r in verification_results
        ) / max(len(verification_results), 1)

        verified_count = sum(1 for r in verification_results if r.verified)

        conflicts = [
            {"claim": r.claim, "confidence": r.confidence, "notes": r.notes}
            for r in verification_results if r.status == "conflict"
        ]

        return ReportVerification(
            overall_confidence=overall_confidence,
            verified_count=verified_count,
            total_claims=len(claims),
            conflicts=conflicts,
            details=verification_results
        )


# =============================================================================
# å¯ä¿¡åº¦è¨ˆç®—å™¨
# =============================================================================

class CredibilityCalculator:
    """
    å¯ä¿¡åº¦è¨ˆç®—å™¨

    â€¹1â€º ä¾†æºè©•ä¼°
    â€¹2â€º æ™‚æ•ˆæ€§è©•ä¼°
    â€¹3â€º ä¸€è‡´æ€§è©•ä¼°
    """

    # å·²çŸ¥å¯ä¿¡ä¾†æºåŸŸå
    TRUSTED_DOMAINS = [
        "reuters.com", "bloomberg.com", "wsj.com", "ft.com",
        "nature.com", "science.org", "ieee.org",
        "arxiv.org", "github.com"
    ]

    def evaluate_source(self, url: str) -> float:
        """è©•ä¼°ä¾†æºå¯ä¿¡åº¦"""
        from urllib.parse import urlparse

        try:
            domain = urlparse(url).netloc.lower()
        except:
            return 0.3

        # æª¢æŸ¥æ˜¯å¦ç‚ºå·²çŸ¥å¯ä¿¡ä¾†æº
        for trusted in self.TRUSTED_DOMAINS:
            if trusted in domain:
                return 0.9

        # æª¢æŸ¥æ˜¯å¦ç‚ºæ”¿åºœæˆ–æ•™è‚²æ©Ÿæ§‹
        if domain.endswith(".gov") or domain.endswith(".edu"):
            return 0.85

        # é»˜èªå¯ä¿¡åº¦
        return 0.5

    def evaluate_consistency(
        self,
        claim: str,
        sources: List[str]
    ) -> float:
        """è©•ä¼°å¤šä¾†æºä¸€è‡´æ€§"""
        if len(sources) < 2:
            return 0.5

        # ç°¡åŒ–å¯¦ç¾ï¼šå‡è¨­ä¾†æºè¶Šå¤šï¼Œä¸€è‡´æ€§è¶Šé«˜
        return min(1.0, 0.5 + len(sources) * 0.1)

    def calculate_overall(
        self,
        source_score: float,
        consistency_score: float,
        recency_score: float = 0.7
    ) -> float:
        """è¨ˆç®—æ•´é«”å¯ä¿¡åº¦"""
        weights = {
            "source": 0.4,
            "consistency": 0.4,
            "recency": 0.2
        }

        return (
            source_score * weights["source"] +
            consistency_score * weights["consistency"] +
            recency_score * weights["recency"]
        )


# =============================================================================
# çŸ›ç›¾æª¢æ¸¬å™¨
# =============================================================================

class ContradictionDetector:
    """
    çŸ›ç›¾æª¢æ¸¬å™¨

    è­˜åˆ¥ä¾†æºé–“çš„çŸ›ç›¾è³‡è¨Š
    """

    def __init__(self, llm_client=None):
        self.llm = llm_client

    async def detect(
        self,
        statements: List[str]
    ) -> List[Dict[str, Any]]:
        """
        æª¢æ¸¬é™³è¿°é–“çš„çŸ›ç›¾

        è¿”å›çŸ›ç›¾å°åˆ—è¡¨
        """
        contradictions = []

        # ç°¡åŒ–å¯¦ç¾ï¼šæª¢æ¸¬æ˜é¡¯çš„å¦å®šé—œä¿‚
        negative_words = ["ä¸", "æ²’æœ‰", "å¦", "ä¸¦é", "ç„¡æ³•"]

        for i, s1 in enumerate(statements):
            for j, s2 in enumerate(statements[i+1:], i+1):
                # æª¢æŸ¥æ˜¯å¦å­˜åœ¨å¦å®šé—œä¿‚
                has_negation = any(
                    w in s1 and w not in s2 or w in s2 and w not in s1
                    for w in negative_words
                )

                if has_negation:
                    # æª¢æŸ¥ä¸»é¡Œæ˜¯å¦ç›¸é—œ
                    s1_words = set(s1.split())
                    s2_words = set(s2.split())
                    overlap = len(s1_words & s2_words)

                    if overlap > 2:  # æœ‰è¶³å¤ çš„å…±åŒè©å½™
                        contradictions.append({
                            "statement1": s1,
                            "statement2": s2,
                            "type": "potential_negation",
                            "confidence": 0.6
                        })

        return contradictions


# =============================================================================
# ç¤ºç¯„
# =============================================================================

async def demo():
    """ç¤ºç¯„é©—è­‰åŠŸèƒ½"""
    print("=" * 60)
    print("âœ“ é«˜ç´šé©—è­‰æ¨¡çµ„ç¤ºç¯„")
    print("=" * 60)

    # å‰µå»ºé©—è­‰æ¨¡çµ„
    verifier = AdvancedVerificationModule()

    # æ¨¡æ“¬ä¾†æº
    sources = [
        MockFinding(
            content="NVIDIA åœ¨ AI æ™¶ç‰‡å¸‚å ´ä½”æ“šä¸»å°åœ°ä½ï¼Œå¸‚å ´ä»½é¡ç´„ 80%",
            source_url="https://reuters.com/nvidia-market"
        ),
        MockFinding(
            content="NVIDIA GPU æ˜¯è¨“ç·´å¤§å‹èªè¨€æ¨¡å‹çš„é¦–é¸ï¼ŒCUDA ç”Ÿæ…‹ç³»çµ±æˆç†Ÿ",
            source_url="https://techcrunch.com/nvidia-cuda"
        ),
        MockFinding(
            content="AMD MI300 ç³»åˆ—é–‹å§‹ç²å¾—å¸‚å ´èªå¯ï¼Œæ­£åœ¨è¿½è¶• NVIDIA",
            source_url="https://amd.com/mi300"
        ),
    ]

    # é©—è­‰é™³è¿°
    claims = [
        "NVIDIA åœ¨ AI æ™¶ç‰‡å¸‚å ´ä½”æ“šé ˜å…ˆåœ°ä½",
        "AMD æ­£åœ¨è¿½è¶• NVIDIA çš„å¸‚å ´ä»½é¡",
        "Intel æ”¾æ£„äº† AI æ™¶ç‰‡æ¥­å‹™"
    ]

    print("\nğŸ“‹ å¾…é©—è­‰é™³è¿°:")
    for i, claim in enumerate(claims, 1):
        print(f"  {i}. {claim}")

    print("\nğŸ” é©—è­‰çµæœ:")
    print("-" * 40)

    results = await verifier.verify_claims(claims, sources)

    for result in results:
        status_icon = {
            "verified": "âœ…",
            "likely": "ğŸ”¶",
            "conflict": "âš ï¸",
            "unverified": "â“"
        }.get(result.status, "â“")

        print(f"\n{status_icon} {result.claim}")
        print(f"   ç‹€æ…‹: {result.status}")
        print(f"   å¯ä¿¡åº¦: {result.confidence:.0%}")
        print(f"   æ”¯æŒä¾†æº: {result.supporting_sources}")
        print(f"   çŸ›ç›¾ä¾†æº: {result.contradicting_sources}")

    # å¯ä¿¡åº¦è¨ˆç®—ç¤ºç¯„
    print("\n" + "-" * 40)
    print("ğŸ“Š ä¾†æºå¯ä¿¡åº¦è©•ä¼°:")
    print("-" * 40)

    calculator = CredibilityCalculator()

    test_urls = [
        "https://reuters.com/article",
        "https://nature.com/paper",
        "https://random-blog.com/post",
        "https://edu.tw/research"
    ]

    for url in test_urls:
        score = calculator.evaluate_source(url)
        print(f"  {url}: {score:.0%}")


def main():
    parser = argparse.ArgumentParser(description="é©—è­‰æ¨¡çµ„")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")

    args = parser.parse_args()
    asyncio.run(demo())


if __name__ == "__main__":
    main()
