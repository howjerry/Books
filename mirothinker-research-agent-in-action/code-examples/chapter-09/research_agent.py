#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 9 ç« ï¼šå»ºæ§‹ä½ çš„ç¬¬ä¸€å€‹ç ”ç©¶ä»£ç†äºº
æ ¸å¿ƒç ”ç©¶ä»£ç†äººå¯¦ç¾

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†å®Œæ•´çš„æ·±åº¦ç ”ç©¶ä»£ç†äººï¼š
1. å•é¡Œç†è§£èˆ‡è¦åŠƒ
2. è³‡è¨Šæœå°‹èˆ‡æ”¶é›†
3. åˆ†æèˆ‡é©—è­‰
4. å ±å‘Šç”Ÿæˆ

ä½¿ç”¨æ–¹å¼ï¼š
    python research_agent.py --demo
    python research_agent.py -q "AI æ™¶ç‰‡å¸‚å ´åˆ†æ"
"""

import asyncio
import argparse
import hashlib
import json
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

from dotenv import load_dotenv

load_dotenv()


# =============================================================================
# è³‡æ–™çµæ§‹
# =============================================================================

class ResearchPhase(Enum):
    """ç ”ç©¶éšæ®µ"""
    UNDERSTANDING = "understanding"
    PLANNING = "planning"
    SEARCHING = "searching"
    ANALYZING = "analyzing"
    VERIFYING = "verifying"
    REPORTING = "reporting"
    COMPLETED = "completed"


@dataclass
class ResearchQuery:
    """ç ”ç©¶æŸ¥è©¢"""
    question: str
    context: str = ""
    constraints: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)

    @property
    def id(self) -> str:
        return hashlib.md5(
            f"{self.question}:{self.created_at.isoformat()}".encode()
        ).hexdigest()[:12]


@dataclass
class ResearchFinding:
    """ç ”ç©¶ç™¼ç¾"""
    content: str
    source_url: str
    relevance_score: float = 0.0
    verified: bool = False
    verification_notes: str = ""
    extracted_at: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict:
        return {
            "content": self.content,
            "source": self.source_url,
            "relevance": self.relevance_score,
            "verified": self.verified
        }


@dataclass
class ResearchReport:
    """ç ”ç©¶å ±å‘Š"""
    query: ResearchQuery
    summary: str
    key_findings: List[str]
    detailed_analysis: str
    sources: List[Dict[str, str]]
    confidence_score: float
    generated_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_markdown(self) -> str:
        """è½‰æ›ç‚º Markdown æ ¼å¼"""
        lines = [
            f"# ç ”ç©¶å ±å‘Š",
            f"",
            f"**ç ”ç©¶å•é¡Œ**: {self.query.question}",
            f"**ç”Ÿæˆæ™‚é–“**: {self.generated_at.strftime('%Y-%m-%d %H:%M')}",
            f"**ä¿¡å¿ƒåˆ†æ•¸**: {self.confidence_score:.0%}",
            f"",
            f"---",
            f"",
            f"## æ‘˜è¦",
            f"",
            self.summary,
            f"",
            f"## é—œéµç™¼ç¾",
            f""
        ]

        for i, finding in enumerate(self.key_findings, 1):
            lines.append(f"{i}. {finding}")

        lines.extend([
            f"",
            f"## è©³ç´°åˆ†æ",
            f"",
            self.detailed_analysis,
            f"",
            f"## åƒè€ƒä¾†æº",
            f""
        ])

        for source in self.sources:
            lines.append(f"- [{source.get('title', 'ä¾†æº')}]({source.get('url', '')})")

        return "\n".join(lines)

    def to_dict(self) -> dict:
        return {
            "query": self.query.question,
            "summary": self.summary,
            "key_findings": self.key_findings,
            "confidence_score": self.confidence_score,
            "sources_count": len(self.sources),
            "generated_at": self.generated_at.isoformat()
        }


@dataclass
class ResearchState:
    """ç ”ç©¶ç‹€æ…‹"""
    query: ResearchQuery
    phase: ResearchPhase = ResearchPhase.UNDERSTANDING
    findings: List[ResearchFinding] = field(default_factory=list)
    search_queries: List[str] = field(default_factory=list)
    verified_facts: List[Dict[str, Any]] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    tool_calls: int = 0
    tokens_used: int = 0


# =============================================================================
# æ¨¡æ“¬ LLM å®¢æˆ¶ç«¯
# =============================================================================

class MockLLMClient:
    """æ¨¡æ“¬ LLM å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼ç¤ºç¯„ï¼‰"""

    def __init__(self):
        self._total_tokens = 0

    async def generate(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 4096,
        json_mode: bool = False
    ) -> str:
        """æ¨¡æ“¬ç”Ÿæˆ"""
        await asyncio.sleep(0.1)

        last_message = messages[-1]["content"] if messages else ""

        if "åˆ†æ" in last_message or "ç†è§£" in last_message:
            if json_mode:
                return json.dumps({
                    "intent": "research",
                    "keywords": ["AI", "æ™¶ç‰‡", "å¸‚å ´"],
                    "sub_questions": [
                        "AI æ™¶ç‰‡çš„ä¸»è¦å» å•†æœ‰å“ªäº›ï¼Ÿ",
                        "å„å» å•†çš„å¸‚å ´ä»½é¡å¦‚ä½•ï¼Ÿ",
                        "æœªä¾†ç™¼å±•è¶¨å‹¢æ˜¯ä»€éº¼ï¼Ÿ"
                    ],
                    "domain": "ç§‘æŠ€",
                    "complexity": "medium"
                }, ensure_ascii=False)

        if "è¦åŠƒ" in last_message or "æœå°‹" in last_message:
            if json_mode:
                return json.dumps({
                    "search_queries": [
                        "AI æ™¶ç‰‡å¸‚å ´ä»½é¡ 2024",
                        "NVIDIA GPU å¸‚å ´åˆ†æ",
                        "AMD Intel AI æ™¶ç‰‡ç«¶çˆ­"
                    ],
                    "search_strategy": "å¤šè§’åº¦æœå°‹",
                    "expected_sources": 5
                }, ensure_ascii=False)

        if "é©—è­‰" in last_message:
            if json_mode:
                return json.dumps({
                    "verified_claims": ["NVIDIA å¸‚å ´é ˜å…ˆ", "AMD è¿½è¶•ä¸­"],
                    "contradictions": [],
                    "confidence_score": 85,
                    "needs_verification": []
                }, ensure_ascii=False)

        if "å ±å‘Š" in last_message or "ç¸½çµ" in last_message:
            if json_mode:
                return json.dumps({
                    "summary": "å…¨çƒ AI æ™¶ç‰‡å¸‚å ´ç”± NVIDIA ä¸»å°ï¼Œå¸‚å ´ä»½é¡ç´„ 80%ã€‚AMD å’Œ Intel æ­£åœ¨ç©æ¥µè¿½è¶•ï¼Œä½†çŸ­æœŸå…§é›£ä»¥æ’¼å‹• NVIDIA çš„åœ°ä½ã€‚",
                    "key_findings": [
                        "NVIDIA æ†‘è—‰ CUDA ç”Ÿæ…‹ç³»çµ±å»ºç«‹è­·åŸæ²³",
                        "AMD MI300 ç³»åˆ—é–‹å§‹ç²å¾—å¸‚å ´èªå¯",
                        "è‡ªç ”æ™¶ç‰‡è¶¨å‹¢æ˜é¡¯ï¼ˆGoogle TPUã€Amazon Trainiumï¼‰"
                    ],
                    "detailed_analysis": "AI æ™¶ç‰‡å¸‚å ´æ­£åœ¨å¿«é€Ÿæˆé•·ï¼Œé è¨ˆåˆ° 2028 å¹´å°‡é”åˆ° 1000 å„„ç¾å…ƒè¦æ¨¡ã€‚NVIDIA çš„å„ªå‹¢ä¸»è¦ä¾†è‡ªå…¶å®Œæ•´çš„è»Ÿé«”ç”Ÿæ…‹ç³»çµ±å’Œå…ˆç™¼å„ªå‹¢ã€‚ç„¶è€Œï¼Œéš¨è‘—ç«¶çˆ­åŠ åŠ‡ï¼Œå¸‚å ´æ ¼å±€å¯èƒ½æœƒé€æ¼¸æ”¹è®Šã€‚",
                    "confidence_score": 0.85
                }, ensure_ascii=False)

        self._total_tokens += 100
        return "é€™æ˜¯ä¸€å€‹æ¨¡æ“¬å›æ‡‰ã€‚"

    @property
    def total_tokens(self) -> int:
        return self._total_tokens


# =============================================================================
# ç ”ç©¶æ¨¡çµ„
# =============================================================================

class UnderstandingModule:
    """å•é¡Œç†è§£æ¨¡çµ„"""

    PROMPT = """åˆ†æä»¥ä¸‹ç ”ç©¶å•é¡Œï¼Œæå–é—œéµè³‡è¨Šã€‚

å•é¡Œ: {question}
{context}

è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
    "intent": "ç ”ç©¶æ„åœ–",
    "keywords": ["é—œéµè©1", "é—œéµè©2"],
    "sub_questions": ["å­å•é¡Œ1", "å­å•é¡Œ2"],
    "domain": "æ‰€å±¬é ˜åŸŸ",
    "complexity": "low/medium/high"
}}"""

    def __init__(self, llm_client):
        self.llm = llm_client

    async def process(self, state: ResearchState) -> Dict[str, Any]:
        """è™•ç†å•é¡Œç†è§£"""
        context = f"\nèƒŒæ™¯: {state.query.context}" if state.query.context else ""

        response = await self.llm.generate(
            messages=[{
                "role": "user",
                "content": self.PROMPT.format(
                    question=state.query.question,
                    context=context
                )
            }],
            json_mode=True,
            temperature=0.3
        )

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {
                "intent": "ä¸€èˆ¬ç ”ç©¶",
                "keywords": state.query.question.split()[:5],
                "sub_questions": [state.query.question],
                "domain": "æœªçŸ¥",
                "complexity": "medium"
            }


class PlanningModule:
    """ç ”ç©¶è¦åŠƒæ¨¡çµ„"""

    PROMPT = """åŸºæ–¼å•é¡Œåˆ†æï¼Œåˆ¶å®šç ”ç©¶è¨ˆç•«ã€‚

åŸå§‹å•é¡Œ: {question}
å•é¡Œåˆ†æ: {understanding}

è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
    "search_queries": ["æœå°‹æŸ¥è©¢1", "æœå°‹æŸ¥è©¢2"],
    "search_strategy": "æœå°‹ç­–ç•¥èªªæ˜",
    "expected_sources": 5
}}"""

    def __init__(self, llm_client):
        self.llm = llm_client

    async def process(
        self,
        state: ResearchState,
        understanding: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ¶å®šç ”ç©¶è¨ˆç•«"""
        response = await self.llm.generate(
            messages=[{
                "role": "user",
                "content": self.PROMPT.format(
                    question=state.query.question,
                    understanding=json.dumps(understanding, ensure_ascii=False)
                )
            }],
            json_mode=True,
            temperature=0.5
        )

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            keywords = understanding.get("keywords", [])
            return {
                "search_queries": [
                    state.query.question,
                    " ".join(keywords[:3]) if keywords else state.query.question
                ],
                "search_strategy": "åŸºæœ¬é—œéµè©æœå°‹",
                "expected_sources": 5
            }


class SearchModule:
    """æœå°‹æ¨¡çµ„"""

    def __init__(self, llm_client, search_manager=None):
        self.llm = llm_client
        self.search_manager = search_manager
        self._last_findings: List[ResearchFinding] = []

    async def search(self, query: str) -> List[ResearchFinding]:
        """åŸ·è¡Œæœå°‹"""
        findings = []

        if self.search_manager:
            try:
                results = await self.search_manager.search(query, num_results=5)
                for result in results:
                    findings.append(ResearchFinding(
                        content=result.snippet,
                        source_url=result.url,
                        relevance_score=getattr(result, 'relevance_score', 0.5)
                    ))
            except Exception as e:
                print(f"    æœå°‹éŒ¯èª¤: {e}")
        else:
            # æ¨¡æ“¬æœå°‹
            await asyncio.sleep(0.1)
            findings = [
                ResearchFinding(
                    content=f"é—œæ–¼ã€Œ{query}ã€çš„æœå°‹çµæœ {i+1} - é€™æ˜¯ä¸€æ®µæ¨¡æ“¬çš„å…§å®¹æ‘˜è¦ï¼ŒåŒ…å«ç›¸é—œçš„ç ”ç©¶è³‡è¨Šã€‚",
                    source_url=f"https://example.com/result/{hash(query) % 1000}/{i}",
                    relevance_score=0.9 - i * 0.1
                )
                for i in range(3)
            ]

        self._last_findings = findings
        return findings


class AnalysisModule:
    """åˆ†ææ¨¡çµ„"""

    PROMPT = """åˆ†æä»¥ä¸‹æ”¶é›†åˆ°çš„è³‡è¨Šï¼Œæå–é—œéµç™¼ç¾ã€‚

ç ”ç©¶å•é¡Œ: {question}

æ”¶é›†çš„è³‡è¨Š:
{findings}

è«‹æä¾›è©³ç´°åˆ†æã€‚"""

    def __init__(self, llm_client):
        self.llm = llm_client

    async def process(self, state: ResearchState) -> Dict[str, Any]:
        """åˆ†ææ”¶é›†çš„è³‡è¨Š"""
        findings_text = "\n\n".join([
            f"ä¾†æº {i+1} ({f.source_url}):\n{f.content}"
            for i, f in enumerate(state.findings[:10])
        ])

        response = await self.llm.generate(
            messages=[{
                "role": "user",
                "content": self.PROMPT.format(
                    question=state.query.question,
                    findings=findings_text
                )
            }],
            temperature=0.5
        )

        return {
            "analysis": response,
            "findings_count": len(state.findings),
            "sources_used": len(set(f.source_url for f in state.findings))
        }


class VerificationModule:
    """é©—è­‰æ¨¡çµ„"""

    PROMPT = """é©—è­‰åˆ†æçµæœçš„æº–ç¢ºæ€§ã€‚

åˆ†æçµæœ: {analysis}
ä¾†æºæ•¸é‡: {sources_count}

ä»¥ JSON æ ¼å¼è¿”å›é©—è­‰çµæœã€‚"""

    def __init__(self, llm_client):
        self.llm = llm_client

    async def process(
        self,
        state: ResearchState,
        analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """é©—è­‰åˆ†æçµæœ"""
        response = await self.llm.generate(
            messages=[{
                "role": "user",
                "content": self.PROMPT.format(
                    analysis=analysis.get("analysis", ""),
                    sources_count=analysis.get("sources_used", 0)
                )
            }],
            json_mode=True,
            temperature=0.3
        )

        try:
            result = json.loads(response)
            return result.get("verified_claims", [])
        except json.JSONDecodeError:
            return []


class ReportingModule:
    """å ±å‘Šç”Ÿæˆæ¨¡çµ„"""

    PROMPT = """åŸºæ–¼ç ”ç©¶çµæœï¼Œç”Ÿæˆå ±å‘Šã€‚

ç ”ç©¶å•é¡Œ: {question}
åˆ†æçµæœ: {analysis}
ä¾†æºæ•¸é‡: {sources_count}

ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
    "summary": "æ‘˜è¦",
    "key_findings": ["ç™¼ç¾1", "ç™¼ç¾2"],
    "detailed_analysis": "è©³ç´°åˆ†æ",
    "confidence_score": 0.0-1.0
}}"""

    def __init__(self, llm_client):
        self.llm = llm_client

    async def generate(
        self,
        state: ResearchState,
        analysis: Dict[str, Any]
    ) -> ResearchReport:
        """ç”Ÿæˆå ±å‘Š"""
        response = await self.llm.generate(
            messages=[{
                "role": "user",
                "content": self.PROMPT.format(
                    question=state.query.question,
                    analysis=analysis.get("analysis", ""),
                    sources_count=len(state.findings)
                )
            }],
            json_mode=True,
            temperature=0.5
        )

        try:
            data = json.loads(response)
        except json.JSONDecodeError:
            data = {
                "summary": analysis.get("analysis", "")[:200],
                "key_findings": ["ç ”ç©¶å®Œæˆ"],
                "detailed_analysis": analysis.get("analysis", ""),
                "confidence_score": 0.7
            }

        sources = [
            {"title": f"ä¾†æº {i+1}", "url": f.source_url}
            for i, f in enumerate(state.findings[:10])
        ]

        return ResearchReport(
            query=state.query,
            summary=data.get("summary", ""),
            key_findings=data.get("key_findings", []),
            detailed_analysis=data.get("detailed_analysis", ""),
            sources=sources,
            confidence_score=data.get("confidence_score", 0.7),
            metadata={
                "tool_calls": state.tool_calls,
                "findings_count": len(state.findings),
                "elapsed_seconds": (datetime.now() - state.start_time).total_seconds()
            }
        )


# =============================================================================
# ç ”ç©¶å”èª¿å™¨
# =============================================================================

class ResearchCoordinator:
    """
    ç ”ç©¶å”èª¿å™¨

    â€¹1â€º ç®¡ç†ç ”ç©¶æµç¨‹
    â€¹2â€º å”èª¿å„æ¨¡çµ„å·¥ä½œ
    â€¹3â€º ç¶­è­·ç ”ç©¶ç‹€æ…‹
    """

    def __init__(
        self,
        llm_client=None,
        search_manager=None,
        memory_manager=None,
        max_iterations: int = 20,
        max_sources: int = 10
    ):
        self.llm = llm_client or MockLLMClient()
        self.search_manager = search_manager
        self.memory_manager = memory_manager
        self.max_iterations = max_iterations
        self.max_sources = max_sources

        self._understanding = UnderstandingModule(self.llm)
        self._planning = PlanningModule(self.llm)
        self._search = SearchModule(self.llm, self.search_manager)
        self._analysis = AnalysisModule(self.llm)
        self._verification = VerificationModule(self.llm)
        self._reporting = ReportingModule(self.llm)

    async def research(self, question: str, context: str = "") -> ResearchReport:
        """åŸ·è¡Œå®Œæ•´ç ”ç©¶æµç¨‹"""
        query = ResearchQuery(question=question, context=context)
        state = ResearchState(query=query)

        print(f"\n{'='*60}")
        print(f"ğŸ”¬ é–‹å§‹ç ”ç©¶: {question[:50]}...")
        print(f"{'='*60}")

        try:
            # å•é¡Œç†è§£
            state.phase = ResearchPhase.UNDERSTANDING
            print(f"\n[1/6] ğŸ“– ç†è§£å•é¡Œ...")
            understanding = await self._understanding.process(state)
            state.tool_calls += 1

            # ç ”ç©¶è¦åŠƒ
            state.phase = ResearchPhase.PLANNING
            print(f"\n[2/6] ğŸ“ åˆ¶å®šç ”ç©¶è¨ˆç•«...")
            plan = await self._planning.process(state, understanding)
            state.search_queries = plan.get("search_queries", [])
            state.tool_calls += 1

            # è³‡è¨Šæ”¶é›†
            state.phase = ResearchPhase.SEARCHING
            print(f"\n[3/6] ğŸ” æ”¶é›†è³‡è¨Š...")
            for i, sq in enumerate(state.search_queries[:5], 1):
                print(f"    æœå°‹ {i}: {sq}")
                findings = await self._search.search(sq)
                state.findings.extend(findings[:3])
                state.tool_calls += 1

            print(f"    å…±æ”¶é›† {len(state.findings)} æ¢è³‡è¨Š")

            # åˆ†ææ•´åˆ
            state.phase = ResearchPhase.ANALYZING
            print(f"\n[4/6] ğŸ§  åˆ†æè³‡è¨Š...")
            analysis = await self._analysis.process(state)
            state.tool_calls += 1

            # äº‹å¯¦æŸ¥è­‰
            state.phase = ResearchPhase.VERIFYING
            print(f"\n[5/6] âœ“ é©—è­‰äº‹å¯¦...")
            verified = await self._verification.process(state, analysis)
            state.verified_facts = verified
            state.tool_calls += 1

            # å ±å‘Šç”Ÿæˆ
            state.phase = ResearchPhase.REPORTING
            print(f"\n[6/6] ğŸ“„ ç”Ÿæˆå ±å‘Š...")
            report = await self._reporting.generate(state, analysis)
            state.phase = ResearchPhase.COMPLETED
            state.tool_calls += 1

            # çµ±è¨ˆ
            elapsed = (datetime.now() - state.start_time).total_seconds()
            print(f"\n{'='*60}")
            print(f"âœ… ç ”ç©¶å®Œæˆï¼")
            print(f"   è€—æ™‚: {elapsed:.1f} ç§’")
            print(f"   å·¥å…·èª¿ç”¨: {state.tool_calls} æ¬¡")
            print(f"   ä¾†æºæ•¸é‡: {len(state.findings)}")
            print(f"   ä¿¡å¿ƒåˆ†æ•¸: {report.confidence_score:.0%}")
            print(f"{'='*60}")

            return report

        except Exception as e:
            state.errors.append(str(e))
            print(f"\nâŒ ç ”ç©¶å¤±æ•—: {e}")
            raise


# =============================================================================
# æ·±åº¦ç ”ç©¶ä»£ç†äºº
# =============================================================================

class DeepResearchAgent:
    """
    æ·±åº¦ç ”ç©¶ä»£ç†äºº

    â€¹1â€º æ•´åˆæ‰€æœ‰ç ”ç©¶èƒ½åŠ›
    â€¹2â€º æ”¯æ´å¤šè¼ªç ”ç©¶
    â€¹3â€º æä¾›å®Œæ•´çš„ç ”ç©¶å ±å‘Š
    """

    def __init__(
        self,
        llm_client=None,
        search_manager=None,
        memory_manager=None,
        config: Dict[str, Any] = None
    ):
        config = config or {}

        self.coordinator = ResearchCoordinator(
            llm_client=llm_client,
            search_manager=search_manager,
            memory_manager=memory_manager,
            max_iterations=config.get("max_iterations", 20),
            max_sources=config.get("max_sources", 10)
        )

        self._history: List[ResearchReport] = []

    async def research(
        self,
        question: str,
        context: str = "",
        verify: bool = True
    ) -> ResearchReport:
        """åŸ·è¡Œç ”ç©¶"""
        report = await self.coordinator.research(question, context)
        self._history.append(report)
        return report

    async def follow_up(self, follow_up_question: str) -> ResearchReport:
        """è¿½å•ç ”ç©¶"""
        if not self._history:
            return await self.research(follow_up_question)

        previous = self._history[-1]
        context = f"ä¹‹å‰çš„ç ”ç©¶å•é¡Œ: {previous.query.question}\næ‘˜è¦: {previous.summary}"

        return await self.research(follow_up_question, context=context)

    def get_history(self) -> List[ResearchReport]:
        """ç²å–ç ”ç©¶æ­·å²"""
        return self._history.copy()

    def clear_history(self) -> None:
        """æ¸…é™¤ç ”ç©¶æ­·å²"""
        self._history.clear()


# =============================================================================
# ç¤ºç¯„
# =============================================================================

async def demo():
    """ç¤ºç¯„ç ”ç©¶ä»£ç†äººåŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ”¬ æ·±åº¦ç ”ç©¶ä»£ç†äººç¤ºç¯„")
    print("=" * 60)

    agent = DeepResearchAgent(
        config={
            "max_iterations": 10,
            "max_sources": 5
        }
    )

    question = "åˆ†æ 2024 å¹´ AI æ™¶ç‰‡å¸‚å ´çš„ç«¶çˆ­æ ¼å±€"

    report = await agent.research(question, verify=True)

    print("\n" + "=" * 60)
    print("ğŸ“„ ç ”ç©¶å ±å‘Š")
    print("=" * 60)
    print(report.to_markdown())

    # è¿½å•
    print("\n" + "-" * 40)
    print("ğŸ“ åŸ·è¡Œè¿½å•ç ”ç©¶...")
    print("-" * 40)

    follow_up = "NVIDIA çš„ç«¶çˆ­å„ªå‹¢æ˜¯ä»€éº¼ï¼Ÿ"
    follow_up_report = await agent.follow_up(follow_up)

    print("\nè¿½å•å ±å‘Šæ‘˜è¦:")
    print(follow_up_report.summary)


def main():
    parser = argparse.ArgumentParser(description="æ·±åº¦ç ”ç©¶ä»£ç†äºº")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")
    parser.add_argument("-q", "--question", type=str, help="ç ”ç©¶å•é¡Œ")

    args = parser.parse_args()

    if args.question:
        async def research_question():
            agent = DeepResearchAgent()
            report = await agent.research(args.question)
            print(report.to_markdown())

        asyncio.run(research_question())
    else:
        asyncio.run(demo())


if __name__ == "__main__":
    main()
