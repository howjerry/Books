"""
cognitive_research_agent.py

å…·å‚™èªçŸ¥æ¡†æ¶çš„æ·±åº¦ç ”ç©¶ä»£ç†äºº
éµå¾ª ISP æ¨¡å‹ï¼ˆè³‡è¨Šæœå°‹éç¨‹ï¼‰é€²è¡Œç³»çµ±æ€§ç ”ç©¶

ä½¿ç”¨æ–¹å¼ï¼š
    agent = CognitiveResearchAgent()
    report = agent.research("é‡å­è¨ˆç®—å°é‡‘èæ¥­çš„å½±éŸ¿")
"""

import os
import json
import time
from enum import Enum
from dataclasses import dataclass, field
from typing import Optional
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


# =============================================================================
# ç ”ç©¶éšæ®µå®šç¾©
# =============================================================================

class ResearchPhase(Enum):
    """ç ”ç©¶éšæ®µæšèˆ‰"""
    INIT = "init"              # åˆå§‹åŒ–ï¼šæ¥æ”¶ä»»å‹™
    DECOMPOSE = "decompose"    # åˆ†è§£ï¼šæ‹†åˆ†å•é¡Œ
    EXPLORE = "explore"        # æ¢ç´¢ï¼šå»£æ³›è’é›†
    FOCUS = "focus"            # èšç„¦ï¼šæ·±å…¥é©—è­‰
    SYNTHESIZE = "synthesize"  # ç¶œåˆï¼šæ•´åˆçŸ¥è­˜
    REPORT = "report"          # å ±å‘Šï¼šç”¢å‡ºçµæœ


@dataclass
class PhaseConfig:
    """éšæ®µé…ç½®"""
    max_iterations: int = 5
    min_sources: int = 2
    confidence_threshold: float = 0.7
    allowed_tools: list = field(default_factory=lambda: ["search"])


# å„éšæ®µçš„é è¨­é…ç½®
PHASE_CONFIGS = {
    ResearchPhase.DECOMPOSE: PhaseConfig(
        max_iterations=2,
        min_sources=0,
        confidence_threshold=0.8,
        allowed_tools=[]  # ç´”æ¨ç†ï¼Œä¸éœ€å·¥å…·
    ),
    ResearchPhase.EXPLORE: PhaseConfig(
        max_iterations=10,
        min_sources=3,
        confidence_threshold=0.5,
        allowed_tools=["search", "browse"]
    ),
    ResearchPhase.FOCUS: PhaseConfig(
        max_iterations=8,
        min_sources=2,
        confidence_threshold=0.7,
        allowed_tools=["search", "browse", "academic_search"]
    ),
    ResearchPhase.SYNTHESIZE: PhaseConfig(
        max_iterations=3,
        min_sources=0,
        confidence_threshold=0.8,
        allowed_tools=[]  # ç´”æ¨ç†ï¼Œä¸éœ€å·¥å…·
    ),
}


# =============================================================================
# è­‰æ“šç³»çµ±
# =============================================================================

class SourceType(Enum):
    """ä¾†æºé¡å‹"""
    ACADEMIC = "academic"    # å­¸è¡“è«–æ–‡
    OFFICIAL = "official"    # å®˜æ–¹æ©Ÿæ§‹
    INDUSTRY = "industry"    # ç”¢æ¥­å ±å‘Š
    NEWS = "news"           # æ–°èåª’é«”
    SOCIAL = "social"       # ç¤¾ç¾¤åª’é«”
    UNKNOWN = "unknown"     # æœªçŸ¥


class EvidenceStrength(Enum):
    """è­‰æ“šå¼·åº¦"""
    STRONG = 3
    MODERATE = 2
    WEAK = 1


@dataclass
class Evidence:
    """è­‰æ“šè³‡æ–™çµæ§‹"""
    claim: str                      # è²æ˜å…§å®¹
    source_url: str                 # ä¾†æºç¶²å€
    source_type: SourceType         # ä¾†æºé¡å‹
    strength: EvidenceStrength      # è­‰æ“šå¼·åº¦
    timestamp: str = ""             # è³‡æ–™æ™‚é–“
    author: str = ""                # ä½œè€…

    def calculate_weight(self) -> float:
        """è¨ˆç®—è­‰æ“šæ¬Šé‡"""
        type_weights = {
            SourceType.ACADEMIC: 1.0,
            SourceType.OFFICIAL: 0.9,
            SourceType.INDUSTRY: 0.7,
            SourceType.NEWS: 0.5,
            SourceType.SOCIAL: 0.2,
            SourceType.UNKNOWN: 0.3
        }

        strength_multiplier = {
            EvidenceStrength.STRONG: 1.5,
            EvidenceStrength.MODERATE: 1.0,
            EvidenceStrength.WEAK: 0.5
        }

        base = type_weights.get(self.source_type, 0.3)
        multiplier = strength_multiplier.get(self.strength, 1.0)

        return base * multiplier


@dataclass
class Conclusion:
    """ç ”ç©¶çµè«–"""
    statement: str
    confidence: float
    supporting_evidence: list = field(default_factory=list)
    conflicting_evidence: list = field(default_factory=list)


# =============================================================================
# æœå°‹å·¥å…·
# =============================================================================

class SearchTool:
    """ç¶²è·¯æœå°‹å·¥å…·"""

    def __init__(self):
        self.api_key = os.getenv("SERPER_API_KEY")

    def search(self, query: str, num_results: int = 5) -> list[dict]:
        """åŸ·è¡Œæœå°‹"""
        if not self.api_key:
            return self._mock_search(query)

        import requests

        response = requests.post(
            "https://google.serper.dev/search",
            headers={"X-API-KEY": self.api_key},
            json={"q": query, "num": num_results}
        )

        if response.status_code == 200:
            data = response.json()
            results = []
            for item in data.get("organic", [])[:num_results]:
                results.append({
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "link": item.get("link", "")
                })
            return results

        return self._mock_search(query)

    def _mock_search(self, query: str) -> list[dict]:
        """æ¨¡æ“¬æœå°‹ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰"""
        return [
            {
                "title": f"æœå°‹çµæœ 1ï¼š{query}",
                "snippet": f"é€™æ˜¯é—œæ–¼ {query} çš„æ¨¡æ“¬æœå°‹çµæœã€‚åœ¨å¯¦éš›ç’°å¢ƒä¸­ï¼Œé€™è£¡æœƒé¡¯ç¤ºçœŸå¯¦çš„æœå°‹çµæœã€‚",
                "link": "https://example.com/result1"
            },
            {
                "title": f"æœå°‹çµæœ 2ï¼š{query} è©³è§£",
                "snippet": f"æ·±å…¥åˆ†æ {query} çš„å„å€‹é¢å‘ï¼Œæä¾›å°ˆæ¥­è¦‹è§£å’Œæ•¸æ“šæ”¯æŒã€‚",
                "link": "https://example.com/result2"
            },
            {
                "title": f"æœå°‹çµæœ 3ï¼š{query} æœ€æ–°ç™¼å±•",
                "snippet": f"è¿½è¹¤ {query} çš„æœ€æ–°è¶¨å‹¢å’Œç™¼å±•å‹•æ…‹ï¼Œæä¾›å‰ç»æ€§è§€é»ã€‚",
                "link": "https://example.com/result3"
            }
        ]


# =============================================================================
# Prompt æ¨¡æ¿
# =============================================================================

DECOMPOSITION_PROMPT = """ä½ æ˜¯ä¸€ä½ç ”ç©¶ç­–ç•¥å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹è¤‡é›œå•é¡Œåˆ†è§£ç‚ºå¯ç®¡ç†çš„å­å•é¡Œã€‚

ç ”ç©¶å•é¡Œï¼š{query}

è«‹éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š

1. **è­˜åˆ¥æ ¸å¿ƒæ¦‚å¿µ**ï¼šæ‰¾å‡ºå•é¡Œä¸­çš„ 3-5 å€‹é—œéµè¡“èª

2. **åˆ†è§£å­å•é¡Œ**ï¼šå°‡å•é¡Œæ‹†åˆ†ç‚º 3-6 å€‹å…·é«”çš„å­å•é¡Œ
   - æ¯å€‹å­å•é¡Œæ‡‰è©²å¯ä»¥ç¨ç«‹ç ”ç©¶
   - å­å•é¡Œæ‡‰è©²æ¶µè“‹å•é¡Œçš„å„å€‹é¢å‘

3. **ç¢ºå®šå„ªå…ˆç´š**ï¼šæ¨™è¨»å“ªäº›å•é¡Œæ‡‰è©²å…ˆå›ç­”

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼š
```json
{{
  "core_concepts": ["æ¦‚å¿µ1", "æ¦‚å¿µ2", ...],
  "sub_questions": [
    {{
      "id": "Q1",
      "question": "å­å•é¡Œæè¿°",
      "priority": 1,
      "search_queries": ["å»ºè­°çš„æœå°‹é—œéµå­—"]
    }}
  ],
  "research_strategy": "ç°¡è¿°ç ”ç©¶ç­–ç•¥"
}}
```"""

EXPLORATION_PROMPT = """ä½ æ˜¯ä¸€ä½è³‡æ·±ç ”ç©¶å“¡ï¼Œæ­£åœ¨é€²è¡Œæ¢ç´¢éšæ®µçš„ç ”ç©¶ã€‚

ç ”ç©¶å•é¡Œï¼š{query}
ç•¶å‰å­å•é¡Œï¼š{sub_question}

ä½ éœ€è¦å»£æ³›è’é›†ç›¸é—œè³‡è¨Šã€‚ä»¥ä¸‹æ˜¯æœå°‹çµæœï¼š

{search_results}

è«‹åˆ†æé€™äº›çµæœï¼š

1. **é—œéµç™¼ç¾**ï¼šæå– 3-5 å€‹é‡è¦çš„äº‹å¯¦æˆ–è§€é»

2. **è³‡è¨Šç¼ºå£**ï¼šè­˜åˆ¥é‚„éœ€è¦é€²ä¸€æ­¥ç ”ç©¶çš„æ–¹é¢

3. **çŸ›ç›¾ä¹‹è™•**ï¼šæ¨™è¨»ä»»ä½•çŸ›ç›¾æˆ–ä¸ä¸€è‡´çš„è³‡è¨Š

4. **ä¸‹ä¸€æ­¥**ï¼šå»ºè­°æ¥ä¸‹ä¾†çš„æœå°‹æ–¹å‘

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼š
```json
{{
  "key_findings": [
    {{
      "finding": "ç™¼ç¾å…§å®¹",
      "source": "ä¾†æº",
      "confidence": 0.8
    }}
  ],
  "information_gaps": ["ç¼ºå£1", "ç¼ºå£2"],
  "contradictions": ["çŸ›ç›¾1"],
  "next_searches": ["ä¸‹ä¸€å€‹æœå°‹è©"]
}}
```"""

VERIFICATION_PROMPT = """ä½ æ˜¯ä¸€ä½äº‹å¯¦æŸ¥æ ¸å°ˆå®¶ï¼Œæ­£åœ¨é©—è­‰ç ”ç©¶ç™¼ç¾ã€‚

åŸå§‹è²æ˜ï¼š{claim}

å·²è’é›†çš„è­‰æ“šï¼š
{evidence_list}

è«‹é€²è¡Œåš´æ ¼é©—è­‰ï¼š

1. **è­‰æ“šè©•ä¼°**ï¼šè©•ä¼°æ¯æ¢è­‰æ“šçš„å¯ä¿¡åº¦å’Œç›¸é—œæ€§

2. **äº¤å‰é©—è­‰**ï¼šä¸åŒä¾†æºæ˜¯å¦æ”¯æŒç›¸åŒçµè«–ï¼Ÿ

3. **çŸ›ç›¾åˆ†æ**ï¼šå¦‚æœ‰çŸ›ç›¾ï¼Œåˆ†æå¯èƒ½åŸå› 

4. **ç½®ä¿¡åº¦åˆ¤å®š**ï¼šçµ¦å‡ºæœ€çµ‚ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼š
```json
{{
  "verification_result": "confirmed" | "partially_confirmed" | "unconfirmed" | "contradicted",
  "confidence": 0.75,
  "reasoning": "é©—è­‰æ¨ç†éç¨‹",
  "verified_claim": "ç¶“é©—è­‰å¾Œçš„è²æ˜ï¼ˆå¯èƒ½èˆ‡åŸå§‹æœ‰ä¿®æ­£ï¼‰"
}}
```"""

SYNTHESIS_PROMPT = """ä½ æ˜¯ä¸€ä½çŸ¥è­˜ç¶œåˆå°ˆå®¶ï¼Œæ­£åœ¨æ•´åˆç ”ç©¶æˆæœã€‚

ç ”ç©¶å•é¡Œï¼š{query}

å·²é©—è­‰çš„ç™¼ç¾ï¼š
{verified_findings}

è«‹é€²è¡ŒçŸ¥è­˜ç¶œåˆï¼š

1. **æ ¸å¿ƒè«–é»**ï¼šæç…‰ 3-5 å€‹æ ¸å¿ƒçµè«–

2. **é—œä¿‚æ¢³ç†**ï¼šé€™äº›çµè«–ä¹‹é–“æœ‰ä»€éº¼é—œè¯ï¼Ÿ

3. **ä¸ç¢ºå®šæ€§**ï¼šæ˜ç¢ºæ¨™è¨»ä»ä¸ç¢ºå®šçš„éƒ¨åˆ†

4. **å»ºè­°**ï¼šåŸºæ–¼ç ”ç©¶çµæœæå‡ºå»ºè­°

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼š
```json
{{
  "core_conclusions": [
    {{
      "conclusion": "çµè«–å…§å®¹",
      "confidence": 0.8,
      "supporting_evidence": ["è­‰æ“š1", "è­‰æ“š2"]
    }}
  ],
  "relationships": "çµè«–ä¹‹é–“çš„é—œä¿‚èªªæ˜",
  "uncertainties": ["ä¸ç¢ºå®šé»1", "ä¸ç¢ºå®šé»2"],
  "recommendations": ["å»ºè­°1", "å»ºè­°2"]
}}
```"""

REPORT_PROMPT = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­å ±å‘Šæ’°å¯«è€…ã€‚è«‹åŸºæ–¼ç ”ç©¶çµæœæ’°å¯«çµæ§‹åŒ–å ±å‘Šã€‚

ç ”ç©¶å•é¡Œï¼š{query}

ç ”ç©¶çµè«–ï¼š
{conclusions}

è«‹æ’°å¯«ä¸€ä»½å°ˆæ¥­çš„ç ”ç©¶å ±å‘Šï¼ŒåŒ…å«ï¼š

1. **æ‘˜è¦**ï¼ˆ100-150 å­—ï¼‰ï¼šæ¦‚è¿°ä¸»è¦ç™¼ç¾

2. **ç ”ç©¶èƒŒæ™¯**ï¼šèªªæ˜å•é¡Œçš„é‡è¦æ€§

3. **ä¸»è¦ç™¼ç¾**ï¼šè©³ç´°èªªæ˜æ ¸å¿ƒçµè«–
   - æ¯å€‹ç™¼ç¾éœ€æ¨™è¨»ç½®ä¿¡åº¦
   - æä¾›æ”¯æŒè­‰æ“š

4. **è¨è«–**ï¼šåˆ†æç™¼ç¾çš„æ„ç¾©å’Œå±€é™

5. **çµè«–èˆ‡å»ºè­°**ï¼šç¸½çµä¸¦æå‡ºè¡Œå‹•å»ºè­°

6. **åƒè€ƒä¾†æº**ï¼šåˆ—å‡ºä¸»è¦è³‡è¨Šä¾†æº

è«‹ç›´æ¥è¼¸å‡º Markdown æ ¼å¼çš„å ±å‘Šã€‚"""


# =============================================================================
# èªçŸ¥ç ”ç©¶ä»£ç†äºº
# =============================================================================

class CognitiveResearchAgent:
    """
    å…·å‚™èªçŸ¥æ¡†æ¶çš„æ·±åº¦ç ”ç©¶ä»£ç†äºº

    å¯¦ç¾ ISPï¼ˆè³‡è¨Šæœå°‹éç¨‹ï¼‰æ¨¡å‹çš„å…­å€‹éšæ®µï¼š
    åˆå§‹åŒ– â†’ å•é¡Œåˆ†è§£ â†’ æ¢ç´¢ â†’ èšç„¦é©—è­‰ â†’ ç¶œåˆ â†’ å ±å‘Š
    """

    def __init__(
        self,
        model: str = "gpt-4o-mini",
        verbose: bool = True
    ):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model
        self.verbose = verbose
        self.search_tool = SearchTool()

        # ç ”ç©¶ç‹€æ…‹
        self.current_phase = ResearchPhase.INIT
        self.query = ""
        self.sub_questions = []
        self.findings = []
        self.evidence_pool = []
        self.conclusions = []
        self.interaction_count = 0

    def research(self, query: str) -> str:
        """
        åŸ·è¡Œå®Œæ•´ç ”ç©¶æµç¨‹

        Args:
            query: ç ”ç©¶å•é¡Œ

        Returns:
            çµæ§‹åŒ–ç ”ç©¶å ±å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰
        """
        start_time = time.time()
        self.query = query

        self._log(f"\n{'='*60}")
        self._log(f"ğŸ”¬ é–‹å§‹ç ”ç©¶ï¼š{query}")
        self._log(f"{'='*60}\n")

        try:
            # éšæ®µ 1ï¼šå•é¡Œåˆ†è§£
            self._phase_decompose()

            # éšæ®µ 2ï¼šæ¢ç´¢
            self._phase_explore()

            # éšæ®µ 3ï¼šèšç„¦é©—è­‰
            self._phase_focus()

            # éšæ®µ 4ï¼šç¶œåˆ
            self._phase_synthesize()

            # éšæ®µ 5ï¼šç”Ÿæˆå ±å‘Š
            report = self._phase_report()

            elapsed = time.time() - start_time
            self._log(f"\n{'='*60}")
            self._log(f"âœ… ç ”ç©¶å®Œæˆ")
            self._log(f"â±ï¸  ç¸½è€—æ™‚ï¼š{elapsed:.1f} ç§’")
            self._log(f"ğŸ”„ ç¸½äº¤äº’æ¬¡æ•¸ï¼š{self.interaction_count}")
            self._log(f"{'='*60}\n")

            return report

        except Exception as e:
            self._log(f"âŒ ç ”ç©¶éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            raise

    def _phase_decompose(self):
        """å•é¡Œåˆ†è§£éšæ®µ"""
        self.current_phase = ResearchPhase.DECOMPOSE
        self._log_phase("å•é¡Œåˆ†è§£", "å°‡è¤‡é›œå•é¡Œæ‹†åˆ†ç‚ºå¯ç®¡ç†çš„å­å•é¡Œ")

        response = self._call_llm(
            DECOMPOSITION_PROMPT.format(query=self.query)
        )

        try:
            # å˜—è©¦è§£æ JSON
            result = self._extract_json(response)
            self.sub_questions = result.get("sub_questions", [])

            self._log(f"   ğŸ“‹ æ ¸å¿ƒæ¦‚å¿µï¼š{result.get('core_concepts', [])}")
            self._log(f"   ğŸ“‹ å­å•é¡Œæ•¸é‡ï¼š{len(self.sub_questions)}")

            for i, sq in enumerate(self.sub_questions, 1):
                self._log(f"      Q{i}: {sq.get('question', '')}")

        except Exception as e:
            self._log(f"   âš ï¸ è§£æå¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å•é¡Œï¼š{e}")
            self.sub_questions = [{
                "id": "Q1",
                "question": self.query,
                "priority": 1,
                "search_queries": [self.query]
            }]

    def _phase_explore(self):
        """æ¢ç´¢éšæ®µï¼šå»£æ³›è’é›†è³‡è¨Š"""
        self.current_phase = ResearchPhase.EXPLORE
        self._log_phase("æ¢ç´¢éšæ®µ", "å»£æ³›è’é›†ç›¸é—œè³‡è¨Š")

        config = PHASE_CONFIGS[ResearchPhase.EXPLORE]

        for sq in self.sub_questions[:3]:  # æœ€å¤šè™•ç† 3 å€‹å­å•é¡Œ
            question = sq.get("question", "")
            search_queries = sq.get("search_queries", [question])

            self._log(f"\n   ğŸ” æ¢ç´¢å­å•é¡Œï¼š{question}")

            for search_query in search_queries[:2]:  # æ¯å€‹å­å•é¡Œæœ€å¤š 2 æ¬¡æœå°‹
                # åŸ·è¡Œæœå°‹
                results = self.search_tool.search(search_query)
                self.interaction_count += 1

                # åˆ†æçµæœ
                results_text = self._format_search_results(results)

                analysis = self._call_llm(
                    EXPLORATION_PROMPT.format(
                        query=self.query,
                        sub_question=question,
                        search_results=results_text
                    )
                )

                try:
                    result = self._extract_json(analysis)
                    findings = result.get("key_findings", [])
                    self.findings.extend(findings)

                    self._log(f"      âœ… ç™¼ç¾ {len(findings)} é …")

                except Exception as e:
                    self._log(f"      âš ï¸ åˆ†æå¤±æ•—ï¼š{e}")

    def _phase_focus(self):
        """èšç„¦é©—è­‰éšæ®µ"""
        self.current_phase = ResearchPhase.FOCUS
        self._log_phase("èšç„¦é©—è­‰", "äº¤å‰é©—è­‰é—œéµç™¼ç¾")

        verified_findings = []

        for i, finding in enumerate(self.findings[:5]):  # æœ€å¤šé©—è­‰ 5 é …
            claim = finding.get("finding", "") if isinstance(finding, dict) else str(finding)

            self._log(f"\n   ğŸ” é©—è­‰ï¼š{claim[:50]}...")

            # æœå°‹é©—è­‰è³‡æ–™
            verify_results = self.search_tool.search(f"verify {claim[:50]}")
            self.interaction_count += 1

            evidence_text = self._format_search_results(verify_results)

            # é©—è­‰
            verification = self._call_llm(
                VERIFICATION_PROMPT.format(
                    claim=claim,
                    evidence_list=evidence_text
                )
            )

            try:
                result = self._extract_json(verification)
                confidence = result.get("confidence", 0.5)

                if confidence >= 0.6:
                    verified_findings.append({
                        "claim": result.get("verified_claim", claim),
                        "confidence": confidence,
                        "reasoning": result.get("reasoning", "")
                    })
                    self._log(f"      âœ… å·²é©—è­‰ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.0%}ï¼‰")
                else:
                    self._log(f"      âš ï¸ æœªé€šéé©—è­‰ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.0%}ï¼‰")

            except Exception as e:
                self._log(f"      âŒ é©—è­‰å¤±æ•—ï¼š{e}")

        self.findings = verified_findings
        self._log(f"\n   ğŸ“Š é©—è­‰é€šéï¼š{len(verified_findings)} é …")

    def _phase_synthesize(self):
        """ç¶œåˆéšæ®µï¼šæ•´åˆç ”ç©¶æˆæœ"""
        self.current_phase = ResearchPhase.SYNTHESIZE
        self._log_phase("çŸ¥è­˜ç¶œåˆ", "æ•´åˆç ”ç©¶æˆæœå½¢æˆçµè«–")

        findings_text = json.dumps(self.findings, ensure_ascii=False, indent=2)

        synthesis = self._call_llm(
            SYNTHESIS_PROMPT.format(
                query=self.query,
                verified_findings=findings_text
            )
        )

        try:
            result = self._extract_json(synthesis)
            self.conclusions = result.get("core_conclusions", [])

            self._log(f"   ğŸ“ å½¢æˆ {len(self.conclusions)} å€‹æ ¸å¿ƒçµè«–")

            for i, c in enumerate(self.conclusions, 1):
                conclusion = c.get("conclusion", "")
                confidence = c.get("confidence", 0)
                self._log(f"      {i}. {conclusion[:50]}... (ç½®ä¿¡åº¦ï¼š{confidence:.0%})")

        except Exception as e:
            self._log(f"   âš ï¸ ç¶œåˆå¤±æ•—ï¼š{e}")
            self.conclusions = [{
                "conclusion": f"é—œæ–¼ã€Œ{self.query}ã€çš„ç ”ç©¶çµè«–éœ€è¦é€²ä¸€æ­¥åˆ†æ",
                "confidence": 0.5,
                "supporting_evidence": [f.get("claim", "") for f in self.findings]
            }]

    def _phase_report(self) -> str:
        """å ±å‘Šç”Ÿæˆéšæ®µ"""
        self.current_phase = ResearchPhase.REPORT
        self._log_phase("ç”Ÿæˆå ±å‘Š", "æ’°å¯«çµæ§‹åŒ–ç ”ç©¶å ±å‘Š")

        conclusions_text = json.dumps(self.conclusions, ensure_ascii=False, indent=2)

        report = self._call_llm(
            REPORT_PROMPT.format(
                query=self.query,
                conclusions=conclusions_text
            )
        )

        # æ·»åŠ å…ƒè³‡æ–™
        report += f"\n\n---\n\n"
        report += f"**ç ”ç©¶çµ±è¨ˆ**\n\n"
        report += f"- ç ”ç©¶å•é¡Œï¼š{self.query}\n"
        report += f"- å­å•é¡Œæ•¸ï¼š{len(self.sub_questions)}\n"
        report += f"- å·²é©—è­‰ç™¼ç¾ï¼š{len(self.findings)}\n"
        report += f"- æ ¸å¿ƒçµè«–ï¼š{len(self.conclusions)}\n"
        report += f"- ç¸½äº¤äº’æ¬¡æ•¸ï¼š{self.interaction_count}\n"
        report += f"- ä½¿ç”¨æ¨¡å‹ï¼š{self.model}\n"

        return report

    def _call_llm(self, prompt: str) -> str:
        """å‘¼å« LLM"""
        self.interaction_count += 1

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )

        return response.choices[0].message.content

    def _extract_json(self, text: str) -> dict:
        """å¾æ–‡å­—ä¸­æå– JSON"""
        import re

        # å˜—è©¦æ‰¾åˆ° JSON å€å¡Š
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', text)
        if json_match:
            return json.loads(json_match.group(1))

        # å˜—è©¦ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except:
            pass

        # è¿”å›ç©ºå­—å…¸
        return {}

    def _format_search_results(self, results: list[dict]) -> str:
        """æ ¼å¼åŒ–æœå°‹çµæœ"""
        formatted = []
        for i, r in enumerate(results, 1):
            formatted.append(
                f"{i}. **{r.get('title', '')}**\n"
                f"   {r.get('snippet', '')}\n"
                f"   ä¾†æºï¼š{r.get('link', '')}"
            )
        return "\n\n".join(formatted)

    def _log(self, message: str):
        """è¼¸å‡ºæ—¥èªŒ"""
        if self.verbose:
            print(message)

    def _log_phase(self, phase_name: str, description: str):
        """è¼¸å‡ºéšæ®µè³‡è¨Š"""
        self._log(f"\nğŸ“ {phase_name}")
        self._log(f"   {description}")
        self._log(f"   {'â”€'*40}")


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(
        description="èªçŸ¥ç ”ç©¶ä»£ç†äºº - å…·å‚™èªçŸ¥æ¡†æ¶çš„æ·±åº¦ç ”ç©¶å·¥å…·"
    )
    parser.add_argument(
        "-q", "--query",
        type=str,
        help="ç ”ç©¶å•é¡Œ"
    )
    parser.add_argument(
        "-i", "--interactive",
        action="store_true",
        help="äº’å‹•æ¨¡å¼"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="ä½¿ç”¨çš„æ¨¡å‹ (é è¨­: gpt-4o-mini)"
    )
    parser.add_argument(
        "-o", "--output",
        type=str,
        help="è¼¸å‡ºå ±å‘Šåˆ°æª”æ¡ˆ"
    )

    args = parser.parse_args()

    # æª¢æŸ¥ API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ éŒ¯èª¤ï¼šè«‹è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("   è¤‡è£½ .env.example ç‚º .env ä¸¦å¡«å…¥ä½ çš„ API Key")
        return

    # å»ºç«‹ä»£ç†äºº
    agent = CognitiveResearchAgent(
        model=args.model,
        verbose=True
    )

    if args.interactive:
        # äº’å‹•æ¨¡å¼
        print("\nğŸ”¬ èªçŸ¥ç ”ç©¶ä»£ç†äºº - äº’å‹•æ¨¡å¼")
        print("è¼¸å…¥ç ”ç©¶å•é¡Œï¼Œæˆ–è¼¸å…¥ 'quit' é€€å‡º\n")

        while True:
            query = input("ğŸ“ ç ”ç©¶å•é¡Œï¼š").strip()
            if query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è¦‹ï¼")
                break

            if query:
                report = agent.research(query)
                print(f"\n{report}\n")

                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        f.write(report)
                    print(f"ğŸ“„ å ±å‘Šå·²ä¿å­˜è‡³ï¼š{args.output}")

    elif args.query:
        # æŒ‡å®šå•é¡Œæ¨¡å¼
        report = agent.research(args.query)
        print(f"\n{report}")

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nğŸ“„ å ±å‘Šå·²ä¿å­˜è‡³ï¼š{args.output}")

    else:
        # ç¤ºç¯„æ¨¡å¼
        demo_query = "äººå·¥æ™ºæ…§å°è»Ÿé«”å·¥ç¨‹å¸«å°±æ¥­å¸‚å ´çš„å½±éŸ¿æ˜¯ä»€éº¼ï¼Ÿ"
        print(f"\nğŸ¯ ç¤ºç¯„ç ”ç©¶å•é¡Œï¼š{demo_query}\n")

        report = agent.research(demo_query)
        print(f"\n{report}")


if __name__ == "__main__":
    main()
