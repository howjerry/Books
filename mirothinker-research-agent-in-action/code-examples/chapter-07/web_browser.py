#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 7 ç« ï¼šæœå°‹èˆ‡æª¢ç´¢å¼•æ“
ç¶²é ç€è¦½å™¨å¯¦ç¾

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†ç¶²é å…§å®¹ç²å–èˆ‡æå–ï¼š
1. HTTP è«‹æ±‚èˆ‡å…§å®¹ç²å–
2. HTML è§£æèˆ‡ç´”æ–‡å­—æå–
3. çµæ§‹åŒ–å…§å®¹æå–

ä½¿ç”¨æ–¹å¼ï¼š
    python web_browser.py --demo
    python web_browser.py --url "https://example.com"
"""

import asyncio
import re
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin, urlparse

import aiohttp
from dotenv import load_dotenv

load_dotenv()


# =============================================================================
# è³‡æ–™çµæ§‹
# =============================================================================

@dataclass
class WebPage:
    """
    ç¶²é å…§å®¹

    â€¹1â€º åŒ…å«åŸå§‹ HTML å’Œæå–å¾Œçš„ç´”æ–‡å­—
    â€¹2â€º è¨˜éŒ„æå–çš„çµæ§‹åŒ–è³‡è¨Š
    """
    url: str
    title: str
    content: str
    html: Optional[str] = None
    links: List[str] = field(default_factory=list)
    images: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    fetched_at: datetime = field(default_factory=datetime.now)
    status_code: int = 200

    @property
    def word_count(self) -> int:
        return len(self.content.split())

    @property
    def char_count(self) -> int:
        return len(self.content)

    def to_dict(self) -> dict:
        return {
            "url": self.url,
            "title": self.title,
            "content": self.content[:1000] + "..." if len(self.content) > 1000 else self.content,
            "word_count": self.word_count,
            "char_count": self.char_count,
            "link_count": len(self.links),
            "image_count": len(self.images),
            "fetched_at": self.fetched_at.isoformat(),
            "status_code": self.status_code
        }


# =============================================================================
# ç¶²é ç€è¦½å™¨
# =============================================================================

class WebBrowser:
    """
    ç¶²é ç€è¦½å™¨

    â€¹1â€º ç²å–ç¶²é å…§å®¹
    â€¹2â€º æå–ç´”æ–‡å­—
    â€¹3â€º è™•ç†å„ç¨®æ ¼å¼
    """

    def __init__(
        self,
        timeout: float = 30.0,
        max_content_length: int = 100000,
        user_agent: str = "MiroThinker/1.0 Research Agent"
    ):
        self.timeout = timeout
        self.max_content_length = max_content_length
        self.user_agent = user_agent

    async def browse(self, url: str) -> WebPage:
        """
        ç€è¦½ç¶²é 

        â€¹1â€º ç²å–å…§å®¹
        â€¹2â€º æå–ç´”æ–‡å­—
        â€¹3â€º è§£æé€£çµå’Œåœ–ç‰‡
        """
        async with aiohttp.ClientSession() as session:
            headers = {"User-Agent": self.user_agent}

            try:
                async with session.get(
                    url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=self.timeout),
                    allow_redirects=True
                ) as response:
                    status_code = response.status

                    if status_code != 200:
                        return WebPage(
                            url=url,
                            title="",
                            content=f"ç„¡æ³•ç²å–ç¶²é ï¼šHTTP {status_code}",
                            status_code=status_code
                        )

                    content_type = response.headers.get("Content-Type", "")

                    # æ ¹æ“šå…§å®¹é¡å‹è™•ç†
                    if "application/pdf" in content_type:
                        return WebPage(
                            url=url,
                            title="PDF æ–‡ä»¶",
                            content="PDF è§£æéœ€è¦é¡å¤–å¥—ä»¶ï¼ˆPyMuPDFï¼‰",
                            status_code=status_code,
                            metadata={"content_type": "application/pdf"}
                        )

                    html = await response.text()

                    if len(html) > self.max_content_length:
                        html = html[:self.max_content_length]

                    return self._process_html(url, html)

            except asyncio.TimeoutError:
                return WebPage(
                    url=url,
                    title="",
                    content="ç¶²é è¼‰å…¥è¶…æ™‚",
                    status_code=408
                )
            except Exception as e:
                return WebPage(
                    url=url,
                    title="",
                    content=f"ç²å–å¤±æ•—ï¼š{str(e)}",
                    status_code=500
                )

    def _process_html(self, url: str, html: str) -> WebPage:
        """è™•ç† HTML å…§å®¹"""
        # æå–æ¨™é¡Œ
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        title = title_match.group(1).strip() if title_match else ""

        # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
        html_clean = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html_clean = re.sub(r'<style[^>]*>.*?</style>', '', html_clean, flags=re.DOTALL | re.IGNORECASE)
        html_clean = re.sub(r'<!--.*?-->', '', html_clean, flags=re.DOTALL)

        # æå–ç´”æ–‡å­—
        text = re.sub(r'<[^>]+>', ' ', html_clean)
        text = self._decode_html_entities(text)
        text = re.sub(r'\s+', ' ', text).strip()

        # æå–é€£çµ
        links = []
        for match in re.finditer(r'href=["\']([^"\']+)["\']', html, re.IGNORECASE):
            link = match.group(1)
            if link.startswith("http"):
                links.append(link)
            elif link.startswith("/"):
                links.append(urljoin(url, link))

        # æå–åœ–ç‰‡
        images = []
        for match in re.finditer(r'<img[^>]+src=["\']([^"\']+)["\']', html, re.IGNORECASE):
            img = match.group(1)
            if img.startswith("http"):
                images.append(img)
            elif img.startswith("/"):
                images.append(urljoin(url, img))

        return WebPage(
            url=url,
            title=title,
            content=text,
            html=html,
            links=links[:50],
            images=images[:20],
            metadata={"content_type": "text/html"}
        )

    def _decode_html_entities(self, text: str) -> str:
        """è§£ç¢¼ HTML å¯¦é«”"""
        entities = {
            "&nbsp;": " ",
            "&amp;": "&",
            "&lt;": "<",
            "&gt;": ">",
            "&quot;": '"',
            "&#39;": "'",
            "&apos;": "'",
        }
        for entity, char in entities.items():
            text = text.replace(entity, char)
        return text

    async def batch_browse(
        self,
        urls: List[str],
        concurrency: int = 5
    ) -> List[WebPage]:
        """æ‰¹æ¬¡ç€è¦½å¤šå€‹ç¶²é """
        semaphore = asyncio.Semaphore(concurrency)

        async def browse_one(url: str) -> WebPage:
            async with semaphore:
                return await self.browse(url)

        tasks = [browse_one(url) for url in urls]
        return await asyncio.gather(*tasks)


# =============================================================================
# å…§å®¹æå–å™¨
# =============================================================================

class ContentExtractor:
    """
    æ™ºèƒ½å…§å®¹æå–å™¨

    â€¹1â€º è­˜åˆ¥ä¸»è¦å…§å®¹å€åŸŸ
    â€¹2â€º éæ¿¾å»£å‘Šå’Œå°èˆª
    â€¹3â€º ä¿ç•™çµæ§‹åŒ–è³‡è¨Š
    """

    CONTENT_PATTERNS = [
        r'<article[^>]*>(.*?)</article>',
        r'<main[^>]*>(.*?)</main>',
        r'class="[^"]*(?:article|content|main|post|entry|story)[^"]*"[^>]*>(.*?)</div>',
    ]

    def __init__(self, min_content_length: int = 100):
        self.min_content_length = min_content_length

    def extract(self, html: str) -> str:
        """æå–ä¸»è¦å…§å®¹"""
        # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
        html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)

        # å˜—è©¦è­˜åˆ¥ä¸»è¦å…§å®¹å€åŸŸ
        main_content = self._find_main_content(html)

        if main_content and len(main_content) > self.min_content_length:
            return self._clean_text(main_content)

        return self._clean_text(html)

    def _find_main_content(self, html: str) -> Optional[str]:
        """è­˜åˆ¥ä¸»è¦å…§å®¹å€åŸŸ"""
        for pattern in self.CONTENT_PATTERNS:
            match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(1) if match.groups() else match.group(0)
        return None

    def _clean_text(self, html: str) -> str:
        """æ¸…ç†ä¸¦æå–ç´”æ–‡å­—"""
        text = re.sub(r'<[^>]+>', ' ', html)
        text = re.sub(r'&nbsp;', ' ', text)
        text = re.sub(r'&amp;', '&', text)
        text = re.sub(r'&lt;', '<', text)
        text = re.sub(r'&gt;', '>', text)
        text = re.sub(r'&quot;', '"', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def extract_structured(self, html: str) -> Dict[str, Any]:
        """æå–çµæ§‹åŒ–è³‡è¨Š"""
        result = {
            "title": "",
            "headings": [],
            "paragraphs": [],
            "lists": []
        }

        # æå–æ¨™é¡Œ
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        if title_match:
            result["title"] = self._clean_text(title_match.group(1))

        # æå–æ¨™é¡Œå±¤ç´š
        for level in range(1, 7):
            for match in re.finditer(rf'<h{level}[^>]*>(.*?)</h{level}>', html, re.IGNORECASE | re.DOTALL):
                result["headings"].append({
                    "level": level,
                    "text": self._clean_text(match.group(1))
                })

        # æå–æ®µè½
        for match in re.finditer(r'<p[^>]*>(.*?)</p>', html, re.IGNORECASE | re.DOTALL):
            text = self._clean_text(match.group(1))
            if len(text) > 20:
                result["paragraphs"].append(text)

        return result


# =============================================================================
# ç¤ºç¯„
# =============================================================================

async def demo():
    """ç¤ºç¯„ç€è¦½åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸŒ ç¶²é ç€è¦½å™¨ç¤ºç¯„")
    print("=" * 60)

    browser = WebBrowser()
    extractor = ContentExtractor()

    # æ¸¬è©¦ URL
    test_url = "https://httpbin.org/html"

    print(f"\nğŸ“ ç€è¦½: {test_url}")
    print("-" * 40)

    page = await browser.browse(test_url)

    print(f"æ¨™é¡Œ: {page.title}")
    print(f"ç‹€æ…‹ç¢¼: {page.status_code}")
    print(f"å…§å®¹é•·åº¦: {page.char_count} å­—ç¬¦")
    print(f"é€£çµæ•¸: {len(page.links)}")
    print(f"åœ–ç‰‡æ•¸: {len(page.images)}")

    print(f"\nå…§å®¹é è¦½:")
    print(page.content[:500] + "..." if len(page.content) > 500 else page.content)

    # çµæ§‹åŒ–æå–
    if page.html:
        print("\n" + "-" * 40)
        print("ğŸ“‹ çµæ§‹åŒ–æå–")
        structured = extractor.extract_structured(page.html)
        print(f"æ¨™é¡Œ: {structured['title']}")
        print(f"æ¨™é¡Œæ•¸: {len(structured['headings'])}")
        print(f"æ®µè½æ•¸: {len(structured['paragraphs'])}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ç¶²é ç€è¦½å™¨")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")
    parser.add_argument("--url", type=str, help="è¦ç€è¦½çš„ URL")

    args = parser.parse_args()

    if args.url:
        async def browse_url():
            browser = WebBrowser()
            page = await browser.browse(args.url)
            print(f"æ¨™é¡Œ: {page.title}")
            print(f"å…§å®¹é•·åº¦: {page.char_count} å­—ç¬¦")
            print(f"\n{page.content[:1000]}...")

        asyncio.run(browse_url())
    else:
        asyncio.run(demo())


if __name__ == "__main__":
    main()
