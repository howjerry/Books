#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 7 ç« ï¼šæœå°‹èˆ‡æª¢ç´¢å¼•æ“
RAG æª¢ç´¢ç³»çµ±å¯¦ç¾

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†å®Œæ•´çš„ RAG ç³»çµ±ï¼š
1. æ–‡ä»¶åˆ†å¡Š
2. å‘é‡ç´¢å¼•
3. ç›¸ä¼¼åº¦æª¢ç´¢

ä½¿ç”¨æ–¹å¼ï¼š
    python rag_retriever.py --demo
"""

import asyncio
import hashlib
import os
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np
from dotenv import load_dotenv
from openai import AsyncOpenAI

load_dotenv()


# =============================================================================
# è³‡æ–™çµæ§‹
# =============================================================================

@dataclass
class DocumentChunk:
    """
    æ–‡ä»¶ç‰‡æ®µ

    â€¹1â€º åŒ…å«åŸå§‹å…§å®¹å’Œä¾†æºè³‡è¨Š
    â€¹2â€º æ”¯æ´å‘é‡åµŒå…¥
    """
    content: str
    source_url: str
    chunk_index: int
    total_chunks: int
    embedding: Optional[List[float]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def id(self) -> str:
        url_hash = hashlib.md5(self.source_url.encode()).hexdigest()[:8]
        return f"chunk_{url_hash}_{self.chunk_index}"

    @property
    def token_count(self) -> int:
        return len(self.content) // 3

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "content": self.content[:200] + "..." if len(self.content) > 200 else self.content,
            "source_url": self.source_url,
            "chunk_index": self.chunk_index,
            "total_chunks": self.total_chunks,
            "token_count": self.token_count
        }


# =============================================================================
# æ–‡ä»¶åˆ†å¡Šå™¨
# =============================================================================

class DocumentChunker:
    """
    æ–‡ä»¶åˆ†å¡Šå™¨

    â€¹1â€º æ”¯æ´å¤šç¨®åˆ†å¡Šç­–ç•¥
    â€¹2â€º ä¿æŒèªç¾©å®Œæ•´æ€§
    â€¹3â€º è™•ç†é‡ç–Šä»¥é¿å…è³‡è¨Šä¸Ÿå¤±
    """

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: Optional[List[str]] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", "ã€‚", ".", " "]

    def chunk(
        self,
        text: str,
        source_url: str = "",
        **metadata
    ) -> List[DocumentChunk]:
        """å°‡æ–‡ä»¶åˆ†å‰²æˆç‰‡æ®µ"""
        if not text.strip():
            return []

        chunks = []
        current_pos = 0
        chunk_index = 0

        while current_pos < len(text):
            end_pos = current_pos + self.chunk_size

            if end_pos >= len(text):
                chunk_text = text[current_pos:].strip()
                if chunk_text:
                    chunks.append(DocumentChunk(
                        content=chunk_text,
                        source_url=source_url,
                        chunk_index=chunk_index,
                        total_chunks=0,
                        metadata=metadata
                    ))
                break

            # å°‹æ‰¾æœ€ä½³åˆ†å‰²é»
            best_split = end_pos
            for separator in self.separators:
                search_start = max(current_pos + self.chunk_size // 2, current_pos)
                sep_pos = text.rfind(separator, search_start, end_pos + 50)
                if sep_pos > current_pos:
                    best_split = sep_pos + len(separator)
                    break

            chunk_text = text[current_pos:best_split].strip()
            if chunk_text:
                chunks.append(DocumentChunk(
                    content=chunk_text,
                    source_url=source_url,
                    chunk_index=chunk_index,
                    total_chunks=0,
                    metadata=metadata
                ))
                chunk_index += 1

            current_pos = best_split - self.chunk_overlap

        # æ›´æ–°ç¸½ç‰‡æ®µæ•¸
        for chunk in chunks:
            chunk.total_chunks = len(chunks)

        return chunks


# =============================================================================
# å‘é‡ç´¢å¼•
# =============================================================================

class VectorIndex:
    """
    å‘é‡ç´¢å¼•

    â€¹1â€º é«˜æ•ˆçš„ç›¸ä¼¼åº¦æœå°‹
    â€¹2â€º æ”¯æ´å¢é‡æ›´æ–°
    """

    def __init__(self, dimension: int = 1536):
        self.dimension = dimension
        self._chunks: List[DocumentChunk] = []
        self._embeddings: Optional[np.ndarray] = None

    def add(self, chunk: DocumentChunk) -> None:
        """æ·»åŠ ç‰‡æ®µ"""
        if chunk.embedding is None:
            raise ValueError("ç‰‡æ®µå¿…é ˆåŒ…å«åµŒå…¥å‘é‡")

        self._chunks.append(chunk)

        embedding = np.array(chunk.embedding).reshape(1, -1)
        if self._embeddings is None:
            self._embeddings = embedding
        else:
            self._embeddings = np.vstack([self._embeddings, embedding])

    def add_batch(self, chunks: List[DocumentChunk]) -> None:
        """æ‰¹æ¬¡æ·»åŠ ç‰‡æ®µ"""
        for chunk in chunks:
            self.add(chunk)

    def search(
        self,
        query_embedding: List[float],
        top_k: int = 5,
        min_score: float = 0.0
    ) -> List[Tuple[DocumentChunk, float]]:
        """æœå°‹æœ€ç›¸é—œçš„ç‰‡æ®µ"""
        if self._embeddings is None or len(self._chunks) == 0:
            return []

        query = np.array(query_embedding)

        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        norms = np.linalg.norm(self._embeddings, axis=1)
        query_norm = np.linalg.norm(query)

        if query_norm == 0:
            return []

        similarities = np.dot(self._embeddings, query) / (norms * query_norm + 1e-8)

        # ç²å– top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            score = float(similarities[idx])
            if score >= min_score:
                results.append((self._chunks[idx], score))

        return results

    @property
    def size(self) -> int:
        return len(self._chunks)

    def clear(self) -> None:
        """æ¸…ç©ºç´¢å¼•"""
        self._chunks = []
        self._embeddings = None


# =============================================================================
# åµŒå…¥å™¨
# =============================================================================

class Embedder:
    """åµŒå…¥ç”Ÿæˆå™¨"""

    def __init__(
        self,
        client: Optional[AsyncOpenAI] = None,
        model: str = "text-embedding-3-small"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model
        self._cache: Dict[str, List[float]] = {}

    async def embed(self, text: str) -> List[float]:
        """ç”ŸæˆåµŒå…¥"""
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self._cache:
            return self._cache[cache_key]

        response = await self.client.embeddings.create(
            model=self.model,
            input=text
        )

        embedding = response.data[0].embedding
        self._cache[cache_key] = embedding
        return embedding

    async def batch_embed(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹æ¬¡ç”ŸæˆåµŒå…¥"""
        tasks = [self.embed(t) for t in texts]
        return await asyncio.gather(*tasks)


class SimpleEmbedder:
    """ç°¡å–®åµŒå…¥å™¨ï¼ˆä¸éœ€è¦ APIï¼‰"""

    def __init__(self, dimensions: int = 128):
        self.dimensions = dimensions

    def embed(self, text: str) -> List[float]:
        """ç”Ÿæˆç°¡å–®åµŒå…¥"""
        embedding = []

        embedding.append(len(text) / 1000)

        for c in "abcdefghijklmnopqrstuvwxyz":
            embedding.append(text.lower().count(c) / max(len(text), 1))

        embedding.append(sum(c.isdigit() for c in text) / max(len(text), 1))
        embedding.append(sum(c in ".,!?;:" for c in text) / max(len(text), 1))
        embedding.append(text.count(" ") / max(len(text), 1))

        while len(embedding) < self.dimensions:
            embedding.append(0.0)

        embedding = embedding[:self.dimensions]

        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = [e / norm for e in embedding]

        return embedding

    async def embed_async(self, text: str) -> List[float]:
        return self.embed(text)


# =============================================================================
# RAG æª¢ç´¢å™¨
# =============================================================================

class RAGRetriever:
    """
    RAG æª¢ç´¢å™¨

    â€¹1â€º æ•´åˆåˆ†å¡Šã€ç´¢å¼•å’Œæª¢ç´¢
    â€¹2â€º æ”¯æ´å¤šç¨®æª¢ç´¢ç­–ç•¥
    â€¹3â€º æä¾›ä¸Šä¸‹æ–‡å¢å¼·
    """

    def __init__(
        self,
        embedder=None,
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        self.embedder = embedder or SimpleEmbedder()
        self.chunker = DocumentChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.index = VectorIndex(dimension=128)

    async def add_document(
        self,
        content: str,
        source_url: str = "",
        **metadata
    ) -> int:
        """æ·»åŠ æ–‡ä»¶åˆ°ç´¢å¼•"""
        chunks = self.chunker.chunk(content, source_url, **metadata)

        for chunk in chunks:
            if hasattr(self.embedder, 'embed_async'):
                embedding = await self.embedder.embed_async(chunk.content)
            else:
                embedding = self.embedder.embed(chunk.content)
            chunk.embedding = embedding
            self.index.add(chunk)

        return len(chunks)

    async def retrieve(
        self,
        query: str,
        top_k: int = 5,
        min_score: float = 0.3
    ) -> List[Tuple[DocumentChunk, float]]:
        """æª¢ç´¢ç›¸é—œç‰‡æ®µ"""
        if hasattr(self.embedder, 'embed_async'):
            query_embedding = await self.embedder.embed_async(query)
        else:
            query_embedding = self.embedder.embed(query)
        return self.index.search(query_embedding, top_k, min_score)

    async def retrieve_with_context(
        self,
        query: str,
        top_k: int = 5
    ) -> str:
        """æª¢ç´¢ä¸¦ç”Ÿæˆä¸Šä¸‹æ–‡"""
        results = await self.retrieve(query, top_k)

        if not results:
            return "æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚"

        context_parts = []
        for chunk, score in results:
            source = chunk.source_url or "æœªçŸ¥ä¾†æº"
            context_parts.append(
                f"[ä¾†æº: {source}]\n"
                f"[ç›¸é—œåº¦: {score:.2f}]\n"
                f"{chunk.content}"
            )

        return "\n\n---\n\n".join(context_parts)

    @property
    def document_count(self) -> int:
        return self.index.size

    def get_statistics(self) -> Dict[str, Any]:
        return {
            "total_chunks": self.index.size,
            "chunk_size": self.chunker.chunk_size,
            "chunk_overlap": self.chunker.chunk_overlap
        }


# =============================================================================
# ç¤ºç¯„
# =============================================================================

async def demo():
    """ç¤ºç¯„ RAG åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ“š RAG æª¢ç´¢ç³»çµ±ç¤ºç¯„")
    print("=" * 60)

    # å‰µå»º RAG æª¢ç´¢å™¨
    rag = RAGRetriever(
        embedder=SimpleEmbedder(dimensions=128),
        chunk_size=200,
        chunk_overlap=20
    )

    # æ·»åŠ ç¤ºç¯„æ–‡ä»¶
    documents = [
        {
            "content": """
            NVIDIA æ˜¯å…¨çƒæœ€å¤§çš„ AI æ™¶ç‰‡ä¾›æ‡‰å•†ï¼Œå¸‚å ´ä»½é¡ç´„ 80%ã€‚
            å…¶ GPU ç”¢å“ç·šåŒ…æ‹¬ A100ã€H100 å’Œæœ€æ–°çš„ H200ã€‚
            CUDA ç”Ÿæ…‹ç³»çµ±æ“æœ‰è¶…é 400 è¬é–‹ç™¼è€…ã€‚
            Tensor Core å°ˆç‚ºæ·±åº¦å­¸ç¿’å„ªåŒ–ï¼Œæ”¯æ´ FP8 ç²¾åº¦ã€‚
            """,
            "source": "https://example.com/nvidia-analysis"
        },
        {
            "content": """
            AMD æ˜¯ NVIDIA çš„ä¸»è¦ç«¶çˆ­å°æ‰‹ï¼Œå¸‚å ´ä»½é¡ç´„ 10%ã€‚
            MI300 ç³»åˆ—æ˜¯ AMD çš„æ——è‰¦ AI åŠ é€Ÿå™¨ã€‚
            ROCm æ˜¯ AMD çš„ GPU è¨ˆç®—å¹³å°ï¼Œå°æ¨™ CUDAã€‚
            AMD åœ¨æ€§åƒ¹æ¯”æ–¹é¢æœ‰ä¸€å®šå„ªå‹¢ã€‚
            """,
            "source": "https://example.com/amd-analysis"
        },
        {
            "content": """
            Intel æ­£åœ¨ç©æ¥µé€²å…¥ AI æ™¶ç‰‡å¸‚å ´ã€‚
            Gaudi ç³»åˆ—æ˜¯ Intel çš„ AI åŠ é€Ÿå™¨ç”¢å“ç·šã€‚
            Intel æ”¶è³¼äº† Habana Labs ä»¥åŠ å¼· AI èƒ½åŠ›ã€‚
            ç›®å‰å¸‚å ´ä»½é¡ç´„ 5%ï¼Œä½†æ­£åœ¨å¿«é€Ÿæˆé•·ã€‚
            """,
            "source": "https://example.com/intel-analysis"
        }
    ]

    print("\nğŸ“¥ æ·»åŠ æ–‡ä»¶...")
    for doc in documents:
        chunks = await rag.add_document(doc["content"], doc["source"])
        print(f"   æ·»åŠ  {chunks} å€‹ç‰‡æ®µ: {doc['source']}")

    print(f"\nğŸ“Š ç´¢å¼•çµ±è¨ˆ: {rag.get_statistics()}")

    # åŸ·è¡Œæª¢ç´¢
    queries = [
        "NVIDIA çš„å¸‚å ´ä»½é¡æ˜¯å¤šå°‘ï¼Ÿ",
        "AMD çš„ç«¶çˆ­å„ªå‹¢æ˜¯ä»€éº¼ï¼Ÿ",
        "Intel åœ¨ AI é ˜åŸŸçš„ç­–ç•¥"
    ]

    for query in queries:
        print(f"\nğŸ” æŸ¥è©¢: {query}")
        print("-" * 40)

        results = await rag.retrieve(query, top_k=2)

        for chunk, score in results:
            print(f"[{score:.3f}] {chunk.content[:100]}...")
            print(f"        ä¾†æº: {chunk.source_url}")

    # ç”Ÿæˆä¸Šä¸‹æ–‡
    print("\n" + "=" * 60)
    print("ğŸ“ ç”Ÿæˆç ”ç©¶ä¸Šä¸‹æ–‡")
    print("=" * 60)

    context = await rag.retrieve_with_context("AI æ™¶ç‰‡å¸‚å ´ç«¶çˆ­æ ¼å±€", top_k=3)
    print(context)


def main():
    import argparse

    parser = argparse.ArgumentParser(description="RAG æª¢ç´¢ç³»çµ±")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")

    args = parser.parse_args()
    asyncio.run(demo())


if __name__ == "__main__":
    main()
