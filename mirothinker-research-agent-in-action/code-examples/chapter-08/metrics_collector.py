#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 8 ç« ï¼šç’°å¢ƒæ­å»ºèˆ‡éƒ¨ç½²
ç›£æ§æŒ‡æ¨™æ”¶é›†å™¨

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†ç”Ÿç”¢ç›£æ§åŠŸèƒ½ï¼š
1. Prometheus æŒ‡æ¨™
2. å¥åº·æª¢æŸ¥
3. æ•ˆèƒ½è¿½è¹¤

ä½¿ç”¨æ–¹å¼ï¼š
    python metrics_collector.py --demo
    python metrics_collector.py --port 9100
"""

import argparse
import asyncio
import random
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv

load_dotenv()


# =============================================================================
# è³‡æ–™çµæ§‹
# =============================================================================

@dataclass
class MetricPoint:
    """æŒ‡æ¨™æ•¸æ“šé»"""
    name: str
    value: float
    labels: Dict[str, str] = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)


@dataclass
class HealthStatus:
    """å¥åº·ç‹€æ…‹"""
    status: str  # "healthy", "degraded", "unhealthy"
    components: Dict[str, bool] = field(default_factory=dict)
    latency_ms: float = 0.0
    message: str = ""


# =============================================================================
# æŒ‡æ¨™é¡å‹
# =============================================================================

class Counter:
    """è¨ˆæ•¸å™¨æŒ‡æ¨™"""

    def __init__(self, name: str, description: str, labels: List[str] = None):
        self.name = name
        self.description = description
        self.labels = labels or []
        self._values: Dict[tuple, float] = {}

    def inc(self, value: float = 1.0, **labels) -> None:
        """å¢åŠ è¨ˆæ•¸"""
        key = tuple(labels.get(l, "") for l in self.labels)
        self._values[key] = self._values.get(key, 0) + value

    def get(self, **labels) -> float:
        """ç²å–è¨ˆæ•¸"""
        key = tuple(labels.get(l, "") for l in self.labels)
        return self._values.get(key, 0)

    def to_prometheus(self) -> str:
        """è½‰æ›ç‚º Prometheus æ ¼å¼"""
        lines = [
            f"# HELP {self.name} {self.description}",
            f"# TYPE {self.name} counter"
        ]
        for key, value in self._values.items():
            label_str = ",".join(f'{l}="{v}"' for l, v in zip(self.labels, key))
            if label_str:
                lines.append(f"{self.name}{{{label_str}}} {value}")
            else:
                lines.append(f"{self.name} {value}")
        return "\n".join(lines)


class Gauge:
    """å„€è¡¨æŒ‡æ¨™"""

    def __init__(self, name: str, description: str, labels: List[str] = None):
        self.name = name
        self.description = description
        self.labels = labels or []
        self._values: Dict[tuple, float] = {}

    def set(self, value: float, **labels) -> None:
        """è¨­ç½®å€¼"""
        key = tuple(labels.get(l, "") for l in self.labels)
        self._values[key] = value

    def get(self, **labels) -> float:
        """ç²å–å€¼"""
        key = tuple(labels.get(l, "") for l in self.labels)
        return self._values.get(key, 0)

    def to_prometheus(self) -> str:
        """è½‰æ›ç‚º Prometheus æ ¼å¼"""
        lines = [
            f"# HELP {self.name} {self.description}",
            f"# TYPE {self.name} gauge"
        ]
        for key, value in self._values.items():
            label_str = ",".join(f'{l}="{v}"' for l, v in zip(self.labels, key))
            if label_str:
                lines.append(f"{self.name}{{{label_str}}} {value}")
            else:
                lines.append(f"{self.name} {value}")
        return "\n".join(lines)


class Histogram:
    """ç›´æ–¹åœ–æŒ‡æ¨™"""

    def __init__(
        self,
        name: str,
        description: str,
        labels: List[str] = None,
        buckets: List[float] = None
    ):
        self.name = name
        self.description = description
        self.labels = labels or []
        self.buckets = buckets or [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
        self._observations: Dict[tuple, List[float]] = {}

    def observe(self, value: float, **labels) -> None:
        """è¨˜éŒ„è§€å¯Ÿå€¼"""
        key = tuple(labels.get(l, "") for l in self.labels)
        if key not in self._observations:
            self._observations[key] = []
        self._observations[key].append(value)

    def percentile(self, p: float, **labels) -> float:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        key = tuple(labels.get(l, "") for l in self.labels)
        observations = self._observations.get(key, [])
        if not observations:
            return 0.0
        sorted_obs = sorted(observations)
        idx = int(len(sorted_obs) * p)
        return sorted_obs[min(idx, len(sorted_obs) - 1)]

    def to_prometheus(self) -> str:
        """è½‰æ›ç‚º Prometheus æ ¼å¼"""
        lines = [
            f"# HELP {self.name} {self.description}",
            f"# TYPE {self.name} histogram"
        ]

        for key, observations in self._observations.items():
            label_str = ",".join(f'{l}="{v}"' for l, v in zip(self.labels, key))
            base_labels = f"{{{label_str}}}" if label_str else ""

            # è¨ˆç®— bucket
            for bucket in self.buckets:
                count = sum(1 for o in observations if o <= bucket)
                bucket_labels = f'{{le="{bucket}"{("," + label_str) if label_str else ""}}}'
                lines.append(f"{self.name}_bucket{bucket_labels} {count}")

            # +Inf bucket
            inf_labels = f'{{le="+Inf"{("," + label_str) if label_str else ""}}}'
            lines.append(f"{self.name}_bucket{inf_labels} {len(observations)}")

            # sum å’Œ count
            lines.append(f"{self.name}_sum{base_labels} {sum(observations)}")
            lines.append(f"{self.name}_count{base_labels} {len(observations)}")

        return "\n".join(lines)


# =============================================================================
# æŒ‡æ¨™æ”¶é›†å™¨
# =============================================================================

class MetricsCollector:
    """
    æŒ‡æ¨™æ”¶é›†å™¨

    â€¹1â€º è«‹æ±‚æŒ‡æ¨™
    â€¹2â€º æ¨¡å‹æŒ‡æ¨™
    â€¹3â€º GPU æŒ‡æ¨™
    """

    def __init__(self):
        # â€¹1â€º è«‹æ±‚æŒ‡æ¨™
        self.request_total = Counter(
            "research_agent_requests_total",
            "Total number of requests",
            ["endpoint", "status"]
        )

        self.request_latency = Histogram(
            "research_agent_request_latency_seconds",
            "Request latency in seconds",
            ["endpoint"],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
        )

        # â€¹2â€º æ¨¡å‹æŒ‡æ¨™
        self.tokens_generated = Counter(
            "research_agent_tokens_generated_total",
            "Total tokens generated"
        )

        self.tokens_per_second = Gauge(
            "research_agent_tokens_per_second",
            "Current tokens per second throughput"
        )

        self.active_sequences = Gauge(
            "research_agent_active_sequences",
            "Number of active sequences being processed"
        )

        # â€¹3â€º GPU æŒ‡æ¨™
        self.gpu_memory_used = Gauge(
            "research_agent_gpu_memory_used_bytes",
            "GPU memory used in bytes",
            ["gpu_id"]
        )

        self.gpu_utilization = Gauge(
            "research_agent_gpu_utilization_percent",
            "GPU utilization percentage",
            ["gpu_id"]
        )

        self.kv_cache_usage = Gauge(
            "research_agent_kv_cache_usage_percent",
            "KV cache usage percentage"
        )

    def record_request(self, endpoint: str, status: str, latency: float) -> None:
        """è¨˜éŒ„è«‹æ±‚"""
        self.request_total.inc(endpoint=endpoint, status=status)
        self.request_latency.observe(latency, endpoint=endpoint)

    def record_tokens(self, count: int) -> None:
        """è¨˜éŒ„ç”Ÿæˆçš„ tokens"""
        self.tokens_generated.inc(count)

    def update_throughput(self, tokens_per_sec: float) -> None:
        """æ›´æ–°ååé‡"""
        self.tokens_per_second.set(tokens_per_sec)

    def update_sequences(self, count: int) -> None:
        """æ›´æ–°æ´»èºåºåˆ—æ•¸"""
        self.active_sequences.set(count)

    def update_gpu_metrics(
        self,
        gpu_id: str,
        memory_used: int,
        utilization: float
    ) -> None:
        """æ›´æ–° GPU æŒ‡æ¨™"""
        self.gpu_memory_used.set(memory_used, gpu_id=gpu_id)
        self.gpu_utilization.set(utilization, gpu_id=gpu_id)

    def update_kv_cache(self, usage_percent: float) -> None:
        """æ›´æ–° KV Cache ä½¿ç”¨ç‡"""
        self.kv_cache_usage.set(usage_percent)

    def to_prometheus(self) -> str:
        """å°å‡ºç‚º Prometheus æ ¼å¼"""
        metrics = [
            self.request_total,
            self.request_latency,
            self.tokens_generated,
            self.tokens_per_second,
            self.active_sequences,
            self.gpu_memory_used,
            self.gpu_utilization,
            self.kv_cache_usage,
        ]
        return "\n\n".join(m.to_prometheus() for m in metrics)


# =============================================================================
# å¥åº·æª¢æŸ¥å™¨
# =============================================================================

class HealthChecker:
    """
    å¥åº·æª¢æŸ¥å™¨

    â€¹1â€º æª¢æŸ¥å„çµ„ä»¶ç‹€æ…‹
    â€¹2â€º è¨ˆç®—æ•´é«”å¥åº·åº¦
    â€¹3â€º æä¾›è©³ç´°å ±å‘Š
    """

    def __init__(self, components: Dict[str, str] = None):
        """
        åˆå§‹åŒ–å¥åº·æª¢æŸ¥å™¨

        components: çµ„ä»¶åç¨±åˆ°å¥åº·æª¢æŸ¥ URL çš„æ˜ å°„
        """
        self.components = components or {
            "model_server": "http://localhost:8000/health",
            "agent_server": "http://localhost:8080/health",
            "redis": "redis://localhost:6379",
        }

    async def check_all(self) -> HealthStatus:
        """æª¢æŸ¥æ‰€æœ‰çµ„ä»¶"""
        import aiohttp

        component_status = {}
        start_time = time.time()

        for name, url in self.components.items():
            try:
                if url.startswith("redis://"):
                    # æ¨¡æ“¬ Redis æª¢æŸ¥
                    component_status[name] = True
                else:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            url,
                            timeout=aiohttp.ClientTimeout(total=5)
                        ) as resp:
                            component_status[name] = resp.status == 200
            except Exception:
                component_status[name] = False

        latency = (time.time() - start_time) * 1000

        # è¨ˆç®—æ•´é«”ç‹€æ…‹
        healthy_count = sum(1 for v in component_status.values() if v)
        total_count = len(component_status)

        if healthy_count == total_count:
            status = "healthy"
            message = "All components operational"
        elif healthy_count >= total_count * 0.5:
            status = "degraded"
            failed = [k for k, v in component_status.items() if not v]
            message = f"Some components failed: {', '.join(failed)}"
        else:
            status = "unhealthy"
            message = "Most components failed"

        return HealthStatus(
            status=status,
            components=component_status,
            latency_ms=latency,
            message=message
        )


# =============================================================================
# æ•ˆèƒ½è¿½è¹¤å™¨
# =============================================================================

class PerformanceTracker:
    """
    æ•ˆèƒ½è¿½è¹¤å™¨

    â€¹1â€º è¿½è¹¤è«‹æ±‚æ•ˆèƒ½
    â€¹2â€º è¨ˆç®—çµ±è¨ˆæ•¸æ“š
    â€¹3â€º ç”Ÿæˆå ±å‘Š
    """

    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self._latencies: List[float] = []
        self._throughputs: List[float] = []
        self._errors: int = 0
        self._total_requests: int = 0

    def record(
        self,
        latency_ms: float,
        tokens: int = 0,
        success: bool = True
    ) -> None:
        """è¨˜éŒ„ä¸€æ¬¡è«‹æ±‚"""
        self._latencies.append(latency_ms)
        if len(self._latencies) > self.window_size:
            self._latencies.pop(0)

        if latency_ms > 0 and tokens > 0:
            throughput = tokens / (latency_ms / 1000)
            self._throughputs.append(throughput)
            if len(self._throughputs) > self.window_size:
                self._throughputs.pop(0)

        self._total_requests += 1
        if not success:
            self._errors += 1

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆæ•¸æ“š"""
        if not self._latencies:
            return {
                "p50_ms": 0,
                "p95_ms": 0,
                "p99_ms": 0,
                "avg_ms": 0,
                "throughput": 0,
                "error_rate": 0,
                "total_requests": 0
            }

        sorted_latencies = sorted(self._latencies)
        n = len(sorted_latencies)

        return {
            "p50_ms": sorted_latencies[n // 2],
            "p95_ms": sorted_latencies[int(n * 0.95)],
            "p99_ms": sorted_latencies[int(n * 0.99)],
            "avg_ms": sum(sorted_latencies) / n,
            "throughput": sum(self._throughputs) / len(self._throughputs) if self._throughputs else 0,
            "error_rate": self._errors / self._total_requests if self._total_requests > 0 else 0,
            "total_requests": self._total_requests
        }

    def generate_report(self) -> str:
        """ç”Ÿæˆæ•ˆèƒ½å ±å‘Š"""
        stats = self.get_stats()

        lines = [
            "=" * 50,
            "æ•ˆèƒ½å ±å‘Š",
            "=" * 50,
            f"ç¸½è«‹æ±‚æ•¸: {stats['total_requests']:,}",
            f"éŒ¯èª¤ç‡: {stats['error_rate']:.2%}",
            "",
            "å»¶é² (ms):",
            f"  P50: {stats['p50_ms']:.1f}",
            f"  P95: {stats['p95_ms']:.1f}",
            f"  P99: {stats['p99_ms']:.1f}",
            f"  å¹³å‡: {stats['avg_ms']:.1f}",
            "",
            f"ååé‡: {stats['throughput']:.1f} tokens/s",
            "=" * 50
        ]

        return "\n".join(lines)


# =============================================================================
# å‘Šè­¦ç®¡ç†å™¨
# =============================================================================

@dataclass
class AlertRule:
    """å‘Šè­¦è¦å‰‡"""
    name: str
    condition: str  # ä¾‹å¦‚: "latency_p99 > 5000"
    severity: str   # "warning", "critical"
    message: str


class AlertManager:
    """
    å‘Šè­¦ç®¡ç†å™¨

    â€¹1â€º å®šç¾©å‘Šè­¦è¦å‰‡
    â€¹2â€º è©•ä¼°æ¢ä»¶
    â€¹3â€º è§¸ç™¼å‘Šè­¦
    """

    DEFAULT_RULES = [
        AlertRule(
            name="high_latency",
            condition="latency_p99 > 5000",
            severity="warning",
            message="P99 å»¶é²è¶…é 5 ç§’"
        ),
        AlertRule(
            name="critical_latency",
            condition="latency_p99 > 10000",
            severity="critical",
            message="P99 å»¶é²è¶…é 10 ç§’"
        ),
        AlertRule(
            name="high_error_rate",
            condition="error_rate > 0.05",
            severity="warning",
            message="éŒ¯èª¤ç‡è¶…é 5%"
        ),
        AlertRule(
            name="gpu_memory_high",
            condition="gpu_memory_percent > 0.95",
            severity="warning",
            message="GPU é¡¯å­˜ä½¿ç”¨ç‡è¶…é 95%"
        ),
    ]

    def __init__(self, rules: List[AlertRule] = None):
        self.rules = rules or self.DEFAULT_RULES
        self._active_alerts: Dict[str, AlertRule] = {}

    def evaluate(self, metrics: Dict[str, float]) -> List[AlertRule]:
        """è©•ä¼°å‘Šè­¦æ¢ä»¶"""
        triggered = []

        for rule in self.rules:
            # è§£ææ¢ä»¶
            parts = rule.condition.split()
            if len(parts) != 3:
                continue

            metric_name, operator, threshold = parts
            threshold = float(threshold)
            value = metrics.get(metric_name, 0)

            # è©•ä¼°æ¢ä»¶
            is_triggered = False
            if operator == ">":
                is_triggered = value > threshold
            elif operator == "<":
                is_triggered = value < threshold
            elif operator == ">=":
                is_triggered = value >= threshold
            elif operator == "<=":
                is_triggered = value <= threshold

            if is_triggered:
                triggered.append(rule)
                self._active_alerts[rule.name] = rule
            elif rule.name in self._active_alerts:
                del self._active_alerts[rule.name]

        return triggered

    def get_active_alerts(self) -> List[AlertRule]:
        """ç²å–ç•¶å‰æ´»èºçš„å‘Šè­¦"""
        return list(self._active_alerts.values())


# =============================================================================
# ç¤ºç¯„
# =============================================================================

async def demo():
    """ç¤ºç¯„ç›£æ§åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ“Š ç›£æ§æŒ‡æ¨™æ”¶é›†å™¨ç¤ºç¯„")
    print("=" * 60)

    # å‰µå»ºæŒ‡æ¨™æ”¶é›†å™¨
    collector = MetricsCollector()

    # æ¨¡æ“¬ä¸€äº›è«‹æ±‚
    print("\næ¨¡æ“¬è«‹æ±‚...")
    endpoints = ["/v1/chat/completions", "/v1/embeddings", "/health"]

    for i in range(50):
        endpoint = random.choice(endpoints)
        status = "success" if random.random() > 0.05 else "error"
        latency = random.uniform(0.5, 3.0)

        collector.record_request(endpoint, status, latency)
        collector.record_tokens(random.randint(100, 500))

    # æ›´æ–° GPU æŒ‡æ¨™
    for gpu_id in ["0", "1", "2", "3"]:
        collector.update_gpu_metrics(
            gpu_id,
            memory_used=int(70e9 + random.uniform(-5e9, 5e9)),
            utilization=random.uniform(80, 95)
        )

    collector.update_throughput(random.uniform(40, 60))
    collector.update_sequences(random.randint(50, 150))
    collector.update_kv_cache(random.uniform(60, 80))

    # é¡¯ç¤º Prometheus æ ¼å¼
    print("\n" + "-" * 40)
    print("Prometheus æŒ‡æ¨™æ ¼å¼:")
    print("-" * 40)
    prometheus_output = collector.to_prometheus()
    # åªé¡¯ç¤ºå‰å¹¾è¡Œ
    lines = prometheus_output.split("\n")
    for line in lines[:30]:
        print(line)
    if len(lines) > 30:
        print(f"... é‚„æœ‰ {len(lines) - 30} è¡Œ")

    # æ•ˆèƒ½è¿½è¹¤
    print("\n" + "-" * 40)
    print("æ•ˆèƒ½è¿½è¹¤:")
    print("-" * 40)

    tracker = PerformanceTracker()
    for i in range(100):
        latency = random.uniform(100, 2000)
        tokens = random.randint(100, 500)
        success = random.random() > 0.02
        tracker.record(latency, tokens, success)

    print(tracker.generate_report())

    # å‘Šè­¦è©•ä¼°
    print("\n" + "-" * 40)
    print("å‘Šè­¦è©•ä¼°:")
    print("-" * 40)

    alert_manager = AlertManager()

    # æ¨¡æ“¬ä¸€äº›æŒ‡æ¨™å€¼
    test_metrics = {
        "latency_p99": 6000,  # è¶…é 5000ms è­¦å‘Šé–¾å€¼
        "error_rate": 0.03,
        "gpu_memory_percent": 0.92
    }

    triggered = alert_manager.evaluate(test_metrics)
    if triggered:
        print("è§¸ç™¼çš„å‘Šè­¦:")
        for alert in triggered:
            print(f"  [{alert.severity.upper()}] {alert.name}: {alert.message}")
    else:
        print("ç„¡å‘Šè­¦")


def main():
    parser = argparse.ArgumentParser(description="ç›£æ§æŒ‡æ¨™æ”¶é›†å™¨")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")
    parser.add_argument("--port", type=int, default=9100, help="Prometheus ç«¯å£")

    args = parser.parse_args()
    asyncio.run(demo())


if __name__ == "__main__":
    main()
