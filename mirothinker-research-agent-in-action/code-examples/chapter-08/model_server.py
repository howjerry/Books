#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 8 ç« ï¼šç’°å¢ƒæ­å»ºèˆ‡éƒ¨ç½²
vLLM æ¨¡å‹æœå‹™å™¨é…ç½®

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†æ¨¡å‹æœå‹™å™¨çš„é…ç½®èˆ‡ç®¡ç†ï¼š
1. vLLM æœå‹™å™¨é…ç½®
2. å¥åº·æª¢æŸ¥
3. è«‹æ±‚è™•ç†

ä½¿ç”¨æ–¹å¼ï¼š
    python model_server.py --demo
    python model_server.py --config prod
"""

import asyncio
import argparse
import json
import os
import subprocess
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

import aiohttp
from dotenv import load_dotenv

load_dotenv()


# =============================================================================
# é…ç½®é¡
# =============================================================================

@dataclass
class VLLMConfig:
    """
    vLLM æœå‹™å™¨é…ç½®

    â€¹1â€º åŸºç¤æ¨¡å‹é…ç½®
    â€¹2â€º æ€§èƒ½å„ªåŒ–åƒæ•¸
    â€¹3â€º è³‡æºé™åˆ¶
    """
    # â€¹1â€º åŸºç¤é…ç½®
    model_name: str = "Qwen/Qwen2.5-7B-Instruct"
    host: str = "0.0.0.0"
    port: int = 8000

    # â€¹2â€º æ€§èƒ½å„ªåŒ–
    tensor_parallel_size: int = 1       # GPU æ•¸é‡
    max_model_len: int = 8192           # æœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦
    gpu_memory_utilization: float = 0.85  # é¡¯å­˜åˆ©ç”¨ç‡

    # â€¹3â€º æ‰¹æ¬¡è™•ç†
    max_num_seqs: int = 64              # æœ€å¤§ä¸¦ç™¼åºåˆ—
    max_num_batched_tokens: int = 8192  # æœ€å¤§æ‰¹æ¬¡ tokens

    # é‡åŒ–é…ç½®
    quantization: Optional[str] = None  # "awq", "gptq", "squeezellm"
    dtype: str = "auto"                 # "auto", "float16", "bfloat16"

    # é¡å¤–é¸é …
    trust_remote_code: bool = True
    enforce_eager: bool = False

    def to_cli_args(self) -> List[str]:
        """è½‰æ›ç‚º CLI åƒæ•¸"""
        args = [
            "--model", self.model_name,
            "--host", self.host,
            "--port", str(self.port),
            "--tensor-parallel-size", str(self.tensor_parallel_size),
            "--max-model-len", str(self.max_model_len),
            "--gpu-memory-utilization", str(self.gpu_memory_utilization),
            "--max-num-seqs", str(self.max_num_seqs),
            "--max-num-batched-tokens", str(self.max_num_batched_tokens),
            "--dtype", self.dtype,
        ]

        if self.quantization:
            args.extend(["--quantization", self.quantization])

        if self.trust_remote_code:
            args.append("--trust-remote-code")

        if self.enforce_eager:
            args.append("--enforce-eager")

        return args

    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            "model_name": self.model_name,
            "host": self.host,
            "port": self.port,
            "tensor_parallel_size": self.tensor_parallel_size,
            "max_model_len": self.max_model_len,
            "gpu_memory_utilization": self.gpu_memory_utilization,
            "max_num_seqs": self.max_num_seqs,
            "quantization": self.quantization,
            "dtype": self.dtype
        }

    def estimate_memory_gb(self) -> float:
        """ä¼°ç®—é¡¯å­˜éœ€æ±‚ï¼ˆGBï¼‰"""
        # ç°¡åŒ–ä¼°ç®—ï¼šæ ¹æ“šæ¨¡å‹åç¨±æ¨æ¸¬åƒæ•¸é‡
        if "72B" in self.model_name or "72b" in self.model_name:
            base_memory = 144
        elif "32B" in self.model_name or "32b" in self.model_name:
            base_memory = 64
        elif "14B" in self.model_name or "14b" in self.model_name:
            base_memory = 28
        elif "7B" in self.model_name or "7b" in self.model_name:
            base_memory = 14
        elif "8B" in self.model_name or "8b" in self.model_name:
            base_memory = 16
        else:
            base_memory = 16

        # é‡åŒ–å½±éŸ¿
        if self.quantization in ["awq", "gptq"]:
            base_memory *= 0.25
        elif self.quantization == "squeezellm":
            base_memory *= 0.25

        # KV Cache é¡å¤–é–‹éŠ·ï¼ˆç´„ 20%ï¼‰
        return base_memory * 1.2


# =============================================================================
# é è¨­é…ç½®æ¨¡æ¿
# =============================================================================

# é–‹ç™¼ç’°å¢ƒï¼ˆå–®å¡ RTX 4090ï¼‰
DEV_CONFIG = VLLMConfig(
    model_name="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    max_model_len=8192,
    gpu_memory_utilization=0.85,
    max_num_seqs=32,
    enforce_eager=True,  # é–‹ç™¼æ™‚ä½¿ç”¨ï¼Œæ–¹ä¾¿èª¿è©¦
)

# æ¸¬è©¦ç’°å¢ƒï¼ˆå–®å¡ A100 40GBï¼‰
TEST_CONFIG = VLLMConfig(
    model_name="Qwen/Qwen2.5-14B-Instruct",
    tensor_parallel_size=1,
    max_model_len=16384,
    gpu_memory_utilization=0.88,
    max_num_seqs=64,
)

# ç”Ÿç”¢ç’°å¢ƒï¼ˆ4x A100 80GBï¼‰
PROD_CONFIG_72B = VLLMConfig(
    model_name="Qwen/Qwen2.5-72B-Instruct",
    tensor_parallel_size=4,
    max_model_len=32768,
    gpu_memory_utilization=0.90,
    max_num_seqs=256,
    max_num_batched_tokens=32768,
)

# ç”Ÿç”¢ç’°å¢ƒï¼ˆ2x A100 40GB + AWQ é‡åŒ–ï¼‰
PROD_CONFIG_72B_QUANTIZED = VLLMConfig(
    model_name="Qwen/Qwen2.5-72B-Instruct-AWQ",
    tensor_parallel_size=2,
    max_model_len=16384,
    gpu_memory_utilization=0.90,
    quantization="awq",
    max_num_seqs=128,
)

# é…ç½®æ˜ å°„
CONFIGS = {
    "dev": DEV_CONFIG,
    "test": TEST_CONFIG,
    "prod": PROD_CONFIG_72B,
    "prod-quantized": PROD_CONFIG_72B_QUANTIZED,
}


# =============================================================================
# æ¨¡å‹æœå‹™å™¨
# =============================================================================

class ModelServer:
    """
    æ¨¡å‹æœå‹™å™¨ç®¡ç†

    â€¹1â€º å•Ÿå‹• vLLM æœå‹™
    â€¹2â€º å¥åº·æª¢æŸ¥
    â€¹3â€º è«‹æ±‚è™•ç†
    """

    def __init__(self, config: VLLMConfig):
        self.config = config
        self.base_url = f"http://{config.host}:{config.port}"
        self._process = None
        self._started = False

    async def start(self, wait_ready: bool = True) -> None:
        """å•Ÿå‹•æœå‹™å™¨"""
        args = ["python", "-m", "vllm.entrypoints.openai.api_server"]
        args.extend(self.config.to_cli_args())

        print(f"å•Ÿå‹• vLLM æœå‹™å™¨...")
        print(f"é…ç½®: {json.dumps(self.config.to_dict(), indent=2, ensure_ascii=False)}")
        print(f"å‘½ä»¤: {' '.join(args)}")

        self._process = subprocess.Popen(
            args,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )

        if wait_ready:
            await self._wait_for_ready()

        self._started = True

    async def _wait_for_ready(self, timeout: int = 300) -> None:
        """ç­‰å¾…æœå‹™å°±ç·’"""
        print(f"ç­‰å¾…æœå‹™å°±ç·’ï¼ˆæœ€å¤š {timeout} ç§’ï¼‰...")

        for i in range(timeout):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"{self.base_url}/health",
                        timeout=aiohttp.ClientTimeout(total=5)
                    ) as resp:
                        if resp.status == 200:
                            print(f"âœ“ æœå‹™å·²å°±ç·’ï¼ˆè€—æ™‚ {i+1} ç§’ï¼‰")
                            return
            except Exception:
                pass

            await asyncio.sleep(1)

            if i > 0 and i % 30 == 0:
                print(f"  ä»åœ¨å•Ÿå‹•ä¸­... ({i} ç§’)")

        raise TimeoutError(f"æœå‹™å•Ÿå‹•è¶…æ™‚ï¼ˆ{timeout} ç§’ï¼‰")

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/health",
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as resp:
                    if resp.status == 200:
                        return {"status": "healthy", "code": 200}
                    else:
                        return {"status": "unhealthy", "code": resp.status}
        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def get_models(self) -> List[str]:
        """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.base_url}/v1/models") as resp:
                data = await resp.json()
                return [m["id"] for m in data.get("data", [])]

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        stream: bool = False,
        **kwargs
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": self.config.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": stream,
                **kwargs
            }

            async with session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)
            ) as resp:
                if resp.status != 200:
                    error = await resp.text()
                    raise Exception(f"API éŒ¯èª¤: {resp.status} - {error}")

                data = await resp.json()
                return data["choices"][0]["message"]["content"]

    async def generate_stream(
        self,
        prompt: str,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        **kwargs
    ):
        """ä¸²æµç”Ÿæˆæ–‡æœ¬"""
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": self.config.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": True,
                **kwargs
            }

            async with session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload
            ) as resp:
                async for line in resp.content:
                    line = line.decode("utf-8").strip()
                    if line.startswith("data: ") and line != "data: [DONE]":
                        data = json.loads(line[6:])
                        delta = data["choices"][0].get("delta", {})
                        if "content" in delta:
                            yield delta["content"]

    def stop(self) -> None:
        """åœæ­¢æœå‹™å™¨"""
        if self._process:
            self._process.terminate()
            try:
                self._process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self._process.kill()
            print("æœå‹™å™¨å·²åœæ­¢")

    @property
    def is_running(self) -> bool:
        return self._started and self._process and self._process.poll() is None


# =============================================================================
# éƒ¨ç½²æ±ºç­–å·¥å…·
# =============================================================================

def deployment_decision(
    monthly_queries: int,
    avg_tokens_per_query: int,
    latency_requirement_ms: int,
    budget_monthly_usd: float
) -> Dict[str, Any]:
    """
    éƒ¨ç½²æ±ºç­–æ¡†æ¶

    â€¹1â€º è¨ˆç®—æœˆåº¦æ¨ç†æˆæœ¬
    â€¹2â€º è©•ä¼°å»¶é²éœ€æ±‚
    â€¹3â€º çµ¦å‡ºå»ºè­°
    """
    # â€¹1â€º ä¼°ç®—é›²ç«¯ API æˆæœ¬ï¼ˆä»¥ GPT-4o ç‚ºä¾‹ï¼‰
    total_tokens = monthly_queries * avg_tokens_per_query
    api_cost_input = (total_tokens * 0.6) / 1_000_000 * 2.50  # è¼¸å…¥ $2.50/M
    api_cost_output = (total_tokens * 0.4) / 1_000_000 * 10.00  # è¼¸å‡º $10.00/M
    api_cost = api_cost_input + api_cost_output

    # â€¹2â€º ä¼°ç®—è‡ªå»ºæˆæœ¬ï¼ˆä»¥ 4x A100 é›²ç«¯ç§Ÿç”¨ç‚ºä¾‹ï¼‰
    # é›²ç«¯ç§Ÿç”¨ï¼šç´„ $4/hr Ã— 4 GPU Ã— 24hr Ã— 30 days = $11,520/month
    self_hosted_cloud = 11520

    # è‡ªè³¼ç¡¬é«”ï¼ˆæ”¤æ 3 å¹´ï¼‰+ é›»è²»
    # $60,000 / 36 months + $500/month é›»è²» = $2,167/month
    self_hosted_owned = 2167

    # â€¹3â€º æ±ºç­–é‚è¼¯
    result = {
        "monthly_queries": monthly_queries,
        "avg_tokens_per_query": avg_tokens_per_query,
        "total_tokens": total_tokens,
        "api_cost": round(api_cost, 2),
        "self_hosted_cloud_cost": self_hosted_cloud,
        "self_hosted_owned_cost": self_hosted_owned,
        "budget": budget_monthly_usd,
        "latency_requirement_ms": latency_requirement_ms
    }

    if latency_requirement_ms < 500:
        result["recommendation"] = "è‡ªå»ºéƒ¨ç½²ï¼ˆä½å»¶é²éœ€æ±‚ï¼‰"
        result["reason"] = "ä½æ–¼ 500ms å»¶é²éœ€æ±‚ï¼Œç¶²è·¯å»¶é²ä¸å¯æ¥å—"
    elif api_cost < budget_monthly_usd * 0.3:
        result["recommendation"] = "é›²ç«¯ API"
        result["reason"] = f"API æˆæœ¬ ${api_cost:.0f}/æœˆ é ä½æ–¼é ç®—"
    elif api_cost < self_hosted_owned * 0.8:
        result["recommendation"] = "é›²ç«¯ API"
        result["reason"] = f"API æˆæœ¬ ${api_cost:.0f}/æœˆ ä½æ–¼è‡ªå»ºæˆæœ¬"
    elif api_cost < self_hosted_owned * 2:
        result["recommendation"] = "æ··åˆæ–¹æ¡ˆ"
        result["reason"] = "API èˆ‡è‡ªå»ºæˆæœ¬ç›¸è¿‘ï¼Œå»ºè­°æ··åˆä½¿ç”¨"
    else:
        result["recommendation"] = "è‡ªå»ºéƒ¨ç½²ï¼ˆè‡ªè³¼ç¡¬é«”ï¼‰"
        result["reason"] = f"API æˆæœ¬ ${api_cost:.0f}/æœˆ é é«˜æ–¼è‡ªå»ºæˆæœ¬ ${self_hosted_owned}/æœˆ"

    return result


# =============================================================================
# ç¤ºç¯„
# =============================================================================

async def demo():
    """ç¤ºç¯„æ¨¡å‹æœå‹™å™¨åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸš€ vLLM æ¨¡å‹æœå‹™å™¨ç¤ºç¯„")
    print("=" * 60)

    # é¡¯ç¤ºå¯ç”¨é…ç½®
    print("\nğŸ“‹ å¯ç”¨é…ç½®:")
    for name, config in CONFIGS.items():
        memory = config.estimate_memory_gb()
        print(f"  {name}:")
        print(f"    æ¨¡å‹: {config.model_name}")
        print(f"    GPU æ•¸é‡: {config.tensor_parallel_size}")
        print(f"    æœ€å¤§ä¸Šä¸‹æ–‡: {config.max_model_len}")
        print(f"    é ä¼°é¡¯å­˜: {memory:.1f} GB")

    # éƒ¨ç½²æ±ºç­–ç¤ºç¯„
    print("\n" + "-" * 40)
    print("ğŸ“Š éƒ¨ç½²æ±ºç­–åˆ†æ")
    print("-" * 40)

    scenarios = [
        {"name": "å°å‹åœ˜éšŠ", "queries": 5000, "tokens": 2000, "latency": 5000, "budget": 500},
        {"name": "ä¸­å‹ä¼æ¥­", "queries": 50000, "tokens": 5000, "latency": 3000, "budget": 5000},
        {"name": "å¤§å‹ä¼æ¥­", "queries": 500000, "tokens": 8000, "latency": 1000, "budget": 50000},
    ]

    for scenario in scenarios:
        print(f"\nå ´æ™¯: {scenario['name']}")
        result = deployment_decision(
            monthly_queries=scenario["queries"],
            avg_tokens_per_query=scenario["tokens"],
            latency_requirement_ms=scenario["latency"],
            budget_monthly_usd=scenario["budget"]
        )
        print(f"  æœˆæŸ¥è©¢é‡: {result['monthly_queries']:,}")
        print(f"  API æˆæœ¬: ${result['api_cost']:,.0f}/æœˆ")
        print(f"  è‡ªå»ºæˆæœ¬: ${result['self_hosted_owned_cost']:,}/æœˆ")
        print(f"  âœ“ å»ºè­°: {result['recommendation']}")
        print(f"    åŸå› : {result['reason']}")

    # é…ç½®ç”Ÿæˆç¤ºç¯„
    print("\n" + "-" * 40)
    print("âš™ï¸ CLI åƒæ•¸ç”Ÿæˆ")
    print("-" * 40)

    config = CONFIGS["dev"]
    args = config.to_cli_args()
    print(f"\né–‹ç™¼ç’°å¢ƒå•Ÿå‹•å‘½ä»¤:")
    print(f"  python -m vllm.entrypoints.openai.api_server \\")
    for i, arg in enumerate(args):
        if arg.startswith("--"):
            print(f"    {arg}", end="")
        else:
            print(f" {arg} \\")
    print()


def main():
    parser = argparse.ArgumentParser(description="vLLM æ¨¡å‹æœå‹™å™¨")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")
    parser.add_argument("--config", type=str, choices=list(CONFIGS.keys()),
                        default="dev", help="ä½¿ç”¨é è¨­é…ç½®")
    parser.add_argument("--start", action="store_true", help="å•Ÿå‹•æœå‹™å™¨")

    args = parser.parse_args()

    if args.start:
        config = CONFIGS[args.config]
        server = ModelServer(config)
        asyncio.run(server.start())
    else:
        asyncio.run(demo())


if __name__ == "__main__":
    main()
