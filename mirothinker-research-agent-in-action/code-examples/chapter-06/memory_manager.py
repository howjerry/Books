#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 6 ç« ï¼šé•·çŸ­æ™‚è¨˜æ†¶ç®¡ç†
çµ±ä¸€è¨˜æ†¶ç®¡ç†ç³»çµ±å®Œæ•´å¯¦ç¾

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†å®Œæ•´çš„ä¸‰å±¤è¨˜æ†¶æ¶æ§‹ï¼ŒåŒ…å«ï¼š
1. å·¥ä½œè¨˜æ†¶ï¼ˆWorking Memoryï¼‰- ç•¶å‰ä»»å‹™ä¸Šä¸‹æ–‡
2. æƒ…ç¯€è¨˜æ†¶ï¼ˆEpisodic Memoryï¼‰- ç ”ç©¶æ­¥é©Ÿæ­·ç¨‹
3. èªç¾©è¨˜æ†¶ï¼ˆSemantic Memoryï¼‰- æŒä¹…åŒ–çŸ¥è­˜åº«

ä½¿ç”¨æ–¹å¼ï¼š
    python memory_manager.py --demo
    python memory_manager.py --test
"""

import asyncio
import hashlib
import json
import os
import time
from abc import ABC, abstractmethod
from collections import OrderedDict
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, Generator, Iterator, List, Optional, Tuple

import numpy as np
from dotenv import load_dotenv
from openai import AsyncOpenAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()


# =============================================================================
# åŸºç¤è³‡æ–™çµæ§‹
# =============================================================================

class MemoryType(Enum):
    """è¨˜æ†¶é¡å‹"""
    WORKING = "working"      # å·¥ä½œè¨˜æ†¶
    EPISODIC = "episodic"    # æƒ…ç¯€è¨˜æ†¶
    SEMANTIC = "semantic"    # èªç¾©è¨˜æ†¶


class MemoryPriority(Enum):
    """è¨˜æ†¶å„ªå…ˆç´š"""
    CRITICAL = 4    # é—œéµè³‡è¨Šï¼Œä¸å¯åˆªé™¤
    HIGH = 3        # é«˜å„ªå…ˆç´š
    MEDIUM = 2      # ä¸­ç­‰å„ªå…ˆç´š
    LOW = 1         # ä½å„ªå…ˆç´š


@dataclass
class MemoryItem:
    """
    è¨˜æ†¶é …ç›®

    â€¹1â€º æ¯å€‹è¨˜æ†¶é …ç›®éƒ½æœ‰å”¯ä¸€è­˜åˆ¥ç¢¼
    â€¹2â€º åŒ…å«é‡è¦æ€§è©•åˆ†å’Œå­˜å–è¨ˆæ•¸
    â€¹3â€º æ”¯æ´å‘é‡åµŒå…¥ä»¥ä¾¿èªç¾©æª¢ç´¢
    """
    content: str
    memory_type: MemoryType
    created_at: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    importance: float = 0.5
    priority: MemoryPriority = MemoryPriority.MEDIUM
    embedding: Optional[List[float]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    _id: str = field(default="", init=False)

    def __post_init__(self):
        content_hash = hashlib.md5(self.content.encode()).hexdigest()[:12]
        timestamp = self.created_at.strftime("%Y%m%d%H%M%S")
        self._id = f"mem_{timestamp}_{content_hash}"

    @property
    def id(self) -> str:
        return self._id

    @property
    def token_count(self) -> int:
        """ä¼°ç®— token æ•¸é‡"""
        return len(self.content) // 3

    @property
    def recency_score(self) -> float:
        """è¨ˆç®—æ–°é®®åº¦åˆ†æ•¸ï¼ˆ0-1ï¼‰"""
        age = (datetime.now() - self.last_accessed).total_seconds()
        decay_rate = 0.693 / 3600  # ln(2) / 1 hour
        return min(1.0, max(0.0, np.exp(-decay_rate * age)))

    @property
    def relevance_score(self) -> float:
        """è¨ˆç®—ç¶œåˆç›¸é—œæ€§åˆ†æ•¸"""
        frequency_score = min(1.0, self.access_count / 10)
        return (
            self.importance * 0.4 +
            frequency_score * 0.3 +
            self.recency_score * 0.3
        )

    def access(self) -> None:
        """è¨˜éŒ„ä¸€æ¬¡å­˜å–"""
        self.last_accessed = datetime.now()
        self.access_count += 1

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "content": self.content,
            "memory_type": self.memory_type.value,
            "created_at": self.created_at.isoformat(),
            "last_accessed": self.last_accessed.isoformat(),
            "access_count": self.access_count,
            "importance": self.importance,
            "priority": self.priority.value,
            "token_count": self.token_count,
            "relevance_score": self.relevance_score,
            "metadata": self.metadata
        }


# =============================================================================
# å·¥ä½œè¨˜æ†¶
# =============================================================================

class WorkingMemory:
    """
    å·¥ä½œè¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º ä½¿ç”¨ LRUï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ç­–ç•¥ç®¡ç†å®¹é‡
    â€¹2â€º æ”¯æ´å„ªå…ˆç´šä¿è­·ï¼Œé—œéµè³‡è¨Šä¸è¢«é©…é€
    â€¹3â€º æä¾›å¿«é€Ÿçš„ key-value å­˜å–
    """

    def __init__(
        self,
        max_tokens: int = 8000,
        protected_ratio: float = 0.2
    ):
        self.max_tokens = max_tokens
        self.protected_tokens = int(max_tokens * protected_ratio)
        self._items: OrderedDict[str, MemoryItem] = OrderedDict()
        self._current_tokens = 0

    @property
    def available_tokens(self) -> int:
        return self.max_tokens - self._current_tokens

    @property
    def utilization(self) -> float:
        return self._current_tokens / self.max_tokens if self.max_tokens > 0 else 0

    def add(
        self,
        content: str,
        importance: float = 0.5,
        priority: MemoryPriority = MemoryPriority.MEDIUM,
        **metadata
    ) -> MemoryItem:
        """æ·»åŠ è¨˜æ†¶é …ç›®"""
        item = MemoryItem(
            content=content,
            memory_type=MemoryType.WORKING,
            importance=importance,
            priority=priority,
            metadata=metadata
        )

        # æª¢æŸ¥æ˜¯å¦éœ€è¦é©…é€
        while (
            self._current_tokens + item.token_count > self.max_tokens
            and self._items
        ):
            evicted = self._evict_one()
            if evicted is None:
                break

        self._items[item.id] = item
        self._items.move_to_end(item.id)
        self._current_tokens += item.token_count

        return item

    def get(self, item_id: str) -> Optional[MemoryItem]:
        """ç²å–è¨˜æ†¶é …ç›®"""
        if item_id not in self._items:
            return None

        item = self._items[item_id]
        item.access()
        self._items.move_to_end(item_id)
        return item

    def search(self, query: str, limit: int = 5) -> List[MemoryItem]:
        """æœå°‹ç›¸é—œè¨˜æ†¶"""
        query_lower = query.lower()
        results = []

        for item in self._items.values():
            content_lower = item.content.lower()
            if query_lower in content_lower:
                score = item.relevance_score + 0.3
                results.append((score, item))

        results.sort(key=lambda x: x[0], reverse=True)
        return [item for _, item in results[:limit]]

    def _evict_one(self) -> Optional[MemoryItem]:
        """é©…é€ä¸€å€‹é …ç›®"""
        candidates = []

        for item_id, item in self._items.items():
            if item.priority == MemoryPriority.CRITICAL:
                continue
            candidates.append((item.priority.value, item.relevance_score, item_id))

        if not candidates:
            return None

        candidates.sort(key=lambda x: (x[0], x[1]))
        evict_id = candidates[0][2]

        item = self._items.pop(evict_id)
        self._current_tokens -= item.token_count
        return item

    def clear(self, keep_critical: bool = True) -> int:
        """æ¸…ç©ºå·¥ä½œè¨˜æ†¶"""
        if keep_critical:
            to_remove = [
                item_id for item_id, item in self._items.items()
                if item.priority != MemoryPriority.CRITICAL
            ]
            for item_id in to_remove:
                item = self._items.pop(item_id)
                self._current_tokens -= item.token_count
            return len(to_remove)
        else:
            count = len(self._items)
            self._items.clear()
            self._current_tokens = 0
            return count

    def to_prompt(self) -> str:
        """å°‡å·¥ä½œè¨˜æ†¶è½‰æ›ç‚º prompt æ ¼å¼"""
        if not self._items:
            return ""

        lines = ["[å·¥ä½œè¨˜æ†¶]"]
        for item in self._items.values():
            priority_marker = {
                MemoryPriority.CRITICAL: "ğŸ”´",
                MemoryPriority.HIGH: "ğŸŸ ",
                MemoryPriority.MEDIUM: "ğŸŸ¡",
                MemoryPriority.LOW: "âšª"
            }.get(item.priority, "âšª")
            lines.append(f"{priority_marker} {item.content}")

        return "\n".join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        priority_counts = {}
        for item in self._items.values():
            key = item.priority.name
            priority_counts[key] = priority_counts.get(key, 0) + 1

        return {
            "item_count": len(self._items),
            "total_tokens": self._current_tokens,
            "max_tokens": self.max_tokens,
            "utilization": self.utilization,
            "available_tokens": self.available_tokens,
            "priority_distribution": priority_counts
        }

    def __iter__(self) -> Iterator[MemoryItem]:
        return iter(self._items.values())

    def __len__(self) -> int:
        return len(self._items)


# =============================================================================
# æƒ…ç¯€è¨˜æ†¶
# =============================================================================

@dataclass
class Episode:
    """
    æƒ…ç¯€ï¼ˆç ”ç©¶æ­¥é©Ÿï¼‰

    â€¹1â€º æ¯å€‹æƒ…ç¯€æ˜¯ä¸€å€‹å®Œæ•´çš„ ReAct å¾ªç’°
    â€¹2â€º åŒ…å«æ€è€ƒã€è¡Œå‹•ã€è§€å¯Ÿ
    """
    step_number: int
    thought: str
    action: Optional[Dict[str, Any]] = None
    observation: Optional[str] = None
    summary: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.now)
    importance: float = 0.5

    @property
    def token_count(self) -> int:
        total = len(self.thought) // 3
        if self.action:
            total += len(json.dumps(self.action)) // 3
        if self.observation:
            total += len(self.observation) // 3
        return total

    @property
    def compressed_token_count(self) -> int:
        if self.summary:
            return len(self.summary) // 3
        return self.token_count

    def compress(self, summary: str) -> None:
        self.summary = summary

    def to_prompt(self, use_summary: bool = False) -> str:
        if use_summary and self.summary:
            return f"[æ­¥é©Ÿ {self.step_number}] {self.summary}"

        lines = [f"[æ­¥é©Ÿ {self.step_number}]"]
        lines.append(f"æ€è€ƒï¼š{self.thought}")

        if self.action:
            tool_name = self.action.get("tool_name", "unknown")
            lines.append(f"è¡Œå‹•ï¼šèª¿ç”¨ {tool_name}")

        if self.observation:
            obs = self.observation
            if len(obs) > 500:
                obs = obs[:500] + "..."
            lines.append(f"è§€å¯Ÿï¼š{obs}")

        return "\n".join(lines)

    def to_dict(self) -> dict:
        return {
            "step_number": self.step_number,
            "thought": self.thought,
            "action": self.action,
            "observation": self.observation,
            "summary": self.summary,
            "created_at": self.created_at.isoformat(),
            "importance": self.importance,
            "token_count": self.token_count
        }


class EpisodicMemory:
    """
    æƒ…ç¯€è¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º é †åºå­˜å„²ç ”ç©¶æ­¥é©Ÿ
    â€¹2â€º æ”¯æ´æ»‘å‹•è¦–çª—å’Œé¸æ“‡æ€§å£“ç¸®
    â€¹3â€º æä¾›éˆæ´»çš„æª¢ç´¢æ©Ÿåˆ¶
    """

    def __init__(
        self,
        max_tokens: int = 32000,
        compression_threshold: float = 0.8,
        window_size: int = 10
    ):
        self.max_tokens = max_tokens
        self.compression_threshold = compression_threshold
        self.window_size = window_size
        self._episodes: List[Episode] = []
        self._current_tokens = 0
        self._compressor: Optional[Callable] = None

    def set_compressor(self, compressor: Callable[[str], str]) -> None:
        """è¨­ç½®å£“ç¸®å™¨"""
        self._compressor = compressor

    @property
    def episode_count(self) -> int:
        return len(self._episodes)

    @property
    def utilization(self) -> float:
        return self._current_tokens / self.max_tokens if self.max_tokens > 0 else 0

    def add_episode(
        self,
        thought: str,
        action: Optional[Dict[str, Any]] = None,
        observation: Optional[str] = None,
        importance: float = 0.5
    ) -> Episode:
        """æ·»åŠ æ–°æƒ…ç¯€"""
        episode = Episode(
            step_number=len(self._episodes) + 1,
            thought=thought,
            action=action,
            observation=observation,
            importance=importance
        )

        self._episodes.append(episode)
        self._current_tokens += episode.token_count

        if self.utilization > self.compression_threshold:
            self._trigger_compression()

        return episode

    def _trigger_compression(self) -> int:
        """è§¸ç™¼å£“ç¸®"""
        if not self._compressor:
            return 0

        compressed_count = 0
        target_tokens = int(self.max_tokens * 0.6)

        compressible = self._episodes[:-self.window_size] if len(self._episodes) > self.window_size else []

        for episode in compressible:
            if episode.summary:
                continue

            if self._current_tokens <= target_tokens:
                break

            original_content = episode.to_prompt(use_summary=False)
            summary = self._compressor(original_content)

            old_tokens = episode.token_count
            episode.compress(summary)
            new_tokens = episode.compressed_token_count

            self._current_tokens -= (old_tokens - new_tokens)
            compressed_count += 1

        return compressed_count

    def get_recent(self, n: int = 5) -> List[Episode]:
        """ç²å–æœ€è¿‘ N å€‹æƒ…ç¯€"""
        return self._episodes[-n:]

    def get_by_importance(
        self,
        min_importance: float = 0.7,
        limit: int = 10
    ) -> List[Episode]:
        """æŒ‰é‡è¦æ€§ç²å–æƒ…ç¯€"""
        important = [ep for ep in self._episodes if ep.importance >= min_importance]
        return sorted(important, key=lambda x: x.importance, reverse=True)[:limit]

    def search(self, query: str, limit: int = 5) -> List[Episode]:
        """æœå°‹ç›¸é—œæƒ…ç¯€"""
        query_lower = query.lower()
        results = []

        for episode in self._episodes:
            content = episode.to_prompt().lower()
            if query_lower in content:
                match_count = content.count(query_lower)
                score = match_count * 0.1 + episode.importance
                results.append((score, episode))

        results.sort(key=lambda x: x[0], reverse=True)
        return [ep for _, ep in results[:limit]]

    def to_prompt(
        self,
        use_summary_for_old: bool = True,
        include_all: bool = False
    ) -> str:
        """ç”Ÿæˆ prompt"""
        if not self._episodes:
            return ""

        lines = ["[ç ”ç©¶æ­·ç¨‹]"]
        summary_cutoff = len(self._episodes) - self.window_size

        for i, episode in enumerate(self._episodes):
            use_summary = use_summary_for_old and i < summary_cutoff
            lines.append(episode.to_prompt(use_summary=use_summary))
            lines.append("")

        return "\n".join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        compressed = sum(1 for ep in self._episodes if ep.summary)

        return {
            "episode_count": len(self._episodes),
            "compressed_count": compressed,
            "compression_rate": compressed / len(self._episodes) if self._episodes else 0,
            "total_tokens": self._current_tokens,
            "max_tokens": self.max_tokens,
            "utilization": self.utilization
        }

    def __iter__(self) -> Iterator[Episode]:
        return iter(self._episodes)

    def __len__(self) -> int:
        return len(self._episodes)


# =============================================================================
# èªç¾©è¨˜æ†¶
# =============================================================================

@dataclass
class KnowledgeChunk:
    """çŸ¥è­˜ç‰‡æ®µ"""
    content: str
    embedding: List[float]
    source_episodes: List[int]
    created_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)
    _id: str = field(default="", init=False)

    def __post_init__(self):
        content_hash = hashlib.md5(self.content.encode()).hexdigest()[:12]
        self._id = f"know_{content_hash}"

    @property
    def id(self) -> str:
        return self._id

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "content": self.content,
            "source_episodes": self.source_episodes,
            "created_at": self.created_at.isoformat(),
            "metadata": self.metadata
        }


class SemanticMemory:
    """
    èªç¾©è¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é€²è¡Œèªç¾©æª¢ç´¢
    â€¹2â€º æ”¯æ´çŸ¥è­˜æ•´åˆï¼ˆå»é‡ã€åˆä½µï¼‰
    """

    def __init__(
        self,
        embedding_dim: int = 1536,
        similarity_threshold: float = 0.85
    ):
        self.embedding_dim = embedding_dim
        self.similarity_threshold = similarity_threshold
        self._chunks: Dict[str, KnowledgeChunk] = {}
        self._embeddings: Optional[np.ndarray] = None
        self._chunk_ids: List[str] = []
        self._embedder: Optional[Callable] = None

    def set_embedder(self, embedder: Callable[[str], List[float]]) -> None:
        """è¨­ç½®åµŒå…¥å‡½æ•¸"""
        self._embedder = embedder

    def add_knowledge(
        self,
        content: str,
        embedding: Optional[List[float]] = None,
        source_episodes: Optional[List[int]] = None,
        **metadata
    ) -> Optional[KnowledgeChunk]:
        """æ·»åŠ çŸ¥è­˜ç‰‡æ®µ"""
        if embedding is None:
            if self._embedder is None:
                raise ValueError("æœªè¨­ç½®åµŒå…¥å‡½æ•¸")
            embedding = self._embedder(content)

        # æª¢æŸ¥é‡è¤‡
        if self._chunks:
            similar_id, similarity = self._find_similar(embedding)
            if similarity > self.similarity_threshold:
                self._merge_knowledge(similar_id, content, source_episodes or [])
                return self._chunks[similar_id]

        chunk = KnowledgeChunk(
            content=content,
            embedding=embedding,
            source_episodes=source_episodes or [],
            metadata=metadata
        )

        self._chunks[chunk.id] = chunk
        self._update_index(chunk)

        return chunk

    def _update_index(self, chunk: KnowledgeChunk) -> None:
        """æ›´æ–°å‘é‡ç´¢å¼•"""
        new_embedding = np.array(chunk.embedding).reshape(1, -1)

        if self._embeddings is None:
            self._embeddings = new_embedding
        else:
            self._embeddings = np.vstack([self._embeddings, new_embedding])

        self._chunk_ids.append(chunk.id)

    def _find_similar(self, embedding: List[float]) -> Tuple[Optional[str], float]:
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„çŸ¥è­˜ç‰‡æ®µ"""
        if self._embeddings is None or len(self._chunk_ids) == 0:
            return None, 0.0

        query = np.array(embedding)
        norms = np.linalg.norm(self._embeddings, axis=1)
        query_norm = np.linalg.norm(query)

        if query_norm == 0:
            return None, 0.0

        similarities = np.dot(self._embeddings, query) / (norms * query_norm + 1e-8)
        max_idx = np.argmax(similarities)
        max_similarity = similarities[max_idx]

        return self._chunk_ids[max_idx], float(max_similarity)

    def _merge_knowledge(
        self,
        chunk_id: str,
        new_content: str,
        source_episodes: List[int]
    ) -> None:
        """åˆä½µçŸ¥è­˜åˆ°ç¾æœ‰ç‰‡æ®µ"""
        chunk = self._chunks[chunk_id]
        chunk.source_episodes.extend(source_episodes)
        chunk.source_episodes = list(set(chunk.source_episodes))

        if new_content not in chunk.content:
            chunk.metadata["merged_contents"] = chunk.metadata.get("merged_contents", [])
            chunk.metadata["merged_contents"].append(new_content)

    def search(
        self,
        query: str,
        limit: int = 5,
        min_similarity: float = 0.5
    ) -> List[Tuple[KnowledgeChunk, float]]:
        """èªç¾©æœå°‹"""
        if self._embedder is None:
            raise ValueError("æœªè¨­ç½®åµŒå…¥å‡½æ•¸")

        if not self._chunks:
            return []

        query_embedding = self._embedder(query)
        query_vec = np.array(query_embedding)

        norms = np.linalg.norm(self._embeddings, axis=1)
        query_norm = np.linalg.norm(query_vec)

        if query_norm == 0:
            return []

        similarities = np.dot(self._embeddings, query_vec) / (norms * query_norm + 1e-8)

        results = []
        for i, sim in enumerate(similarities):
            if sim >= min_similarity:
                chunk_id = self._chunk_ids[i]
                chunk = self._chunks[chunk_id]
                results.append((chunk, float(sim)))

        results.sort(key=lambda x: x[1], reverse=True)
        return results[:limit]

    def get_all_knowledge(self) -> List[KnowledgeChunk]:
        """ç²å–æ‰€æœ‰çŸ¥è­˜"""
        return list(self._chunks.values())

    def to_prompt(self, query: Optional[str] = None, limit: int = 5) -> str:
        """ç”Ÿæˆ prompt"""
        if not self._chunks:
            return ""

        lines = ["[çŸ¥è­˜åº«]"]

        if query and self._embedder:
            results = self.search(query, limit=limit)
            for chunk, similarity in results:
                lines.append(f"[ç›¸é—œåº¦: {similarity:.2f}] {chunk.content}")
        else:
            for chunk in list(self._chunks.values())[:limit]:
                lines.append(f"â€¢ {chunk.content}")

        return "\n".join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        return {
            "chunk_count": len(self._chunks),
            "total_sources": sum(len(c.source_episodes) for c in self._chunks.values()),
            "embedding_dim": self.embedding_dim
        }

    def __len__(self) -> int:
        return len(self._chunks)


# =============================================================================
# è¼”åŠ©çµ„ä»¶
# =============================================================================

class EpisodeCompressor:
    """æƒ…ç¯€å£“ç¸®å™¨"""

    COMPRESSION_PROMPT = """è«‹å°‡ä»¥ä¸‹ç ”ç©¶æ­¥é©Ÿå£“ç¸®ç‚ºç°¡æ½”æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. ä¿ç•™é—œéµç™¼ç¾å’Œçµè«–
2. ä¿ç•™é‡è¦çš„æ•¸æ“šå’Œäº‹å¯¦
3. çœç•¥éç¨‹ç´°ç¯€
4. æ§åˆ¶åœ¨ 50 å­—ä»¥å…§

åŸå§‹å…§å®¹ï¼š
{content}

æ‘˜è¦ï¼š"""

    def __init__(
        self,
        client: Optional[AsyncOpenAI] = None,
        model: str = "gpt-4o-mini"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model

    async def compress(self, content: str) -> str:
        """å£“ç¸®å–®å€‹æƒ…ç¯€"""
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": self.COMPRESSION_PROMPT.format(content=content)
            }],
            max_tokens=100,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()

    def compress_sync(self, content: str) -> str:
        """åŒæ­¥å£“ç¸®"""
        return asyncio.get_event_loop().run_until_complete(self.compress(content))


class EmbeddingGenerator:
    """åµŒå…¥ç”Ÿæˆå™¨"""

    def __init__(
        self,
        client: Optional[AsyncOpenAI] = None,
        model: str = "text-embedding-3-small"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model
        self._cache: Dict[str, List[float]] = {}

    async def embed(self, text: str) -> List[float]:
        """ç”ŸæˆåµŒå…¥"""
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self._cache:
            return self._cache[cache_key]

        response = await self.client.embeddings.create(
            model=self.model,
            input=text
        )

        embedding = response.data[0].embedding
        self._cache[cache_key] = embedding
        return embedding

    def embed_sync(self, text: str) -> List[float]:
        """åŒæ­¥ç”ŸæˆåµŒå…¥"""
        return asyncio.get_event_loop().run_until_complete(self.embed(text))


# =============================================================================
# çµ±ä¸€è¨˜æ†¶ç®¡ç†å™¨
# =============================================================================

class UnifiedMemoryManager:
    """
    çµ±ä¸€è¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º æ•´åˆå·¥ä½œã€æƒ…ç¯€ã€èªç¾©ä¸‰å±¤è¨˜æ†¶
    â€¹2â€º è‡ªå‹•è™•ç†è¨˜æ†¶å±¤ç´šé–“çš„è½‰æ›
    â€¹3â€º æä¾›çµ±ä¸€çš„æŸ¥è©¢ä»‹é¢
    """

    def __init__(
        self,
        working_memory_tokens: int = 8000,
        episodic_memory_tokens: int = 32000,
        client: Optional[AsyncOpenAI] = None,
        model: str = "gpt-4o-mini"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model

        self.working = WorkingMemory(max_tokens=working_memory_tokens)
        self.episodic = EpisodicMemory(max_tokens=episodic_memory_tokens)
        self.semantic = SemanticMemory()

        self._compressor = EpisodeCompressor(client=self.client, model=model)
        self._embedder = EmbeddingGenerator(client=self.client)

        self.episodic.set_compressor(self._compressor.compress_sync)
        self.semantic.set_embedder(self._embedder.embed_sync)

    async def process_step(
        self,
        thought: str,
        action: Optional[Dict[str, Any]] = None,
        observation: Optional[str] = None,
        importance: float = 0.5
    ) -> Episode:
        """è™•ç†ç ”ç©¶æ­¥é©Ÿ"""
        episode = self.episodic.add_episode(
            thought=thought,
            action=action,
            observation=observation,
            importance=importance
        )

        self.working.add(
            content=f"æ­¥é©Ÿ {episode.step_number}: {thought[:100]}...",
            importance=importance,
            priority=MemoryPriority.MEDIUM if importance < 0.7 else MemoryPriority.HIGH,
            source="episode",
            step_number=episode.step_number
        )

        if importance >= 0.8 and observation:
            await self._extract_knowledge(episode)

        return episode

    async def _extract_knowledge(self, episode: Episode) -> None:
        """å¾æƒ…ç¯€ä¸­æå–çŸ¥è­˜"""
        extraction_prompt = f"""å¾ä»¥ä¸‹ç ”ç©¶æ­¥é©Ÿä¸­æå–å¯é‡ç”¨çš„çŸ¥è­˜é»ã€‚
åªæå–äº‹å¯¦æ€§è³‡è¨Šï¼Œä¸åŒ…å«éç¨‹æè¿°ã€‚
æ¯å€‹çŸ¥è­˜é»ä¸€è¡Œï¼Œæœ€å¤š 3 æ¢ã€‚

ç ”ç©¶æ­¥é©Ÿï¼š
{episode.to_prompt()}

çŸ¥è­˜é»ï¼š"""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": extraction_prompt}],
            max_tokens=200,
            temperature=0.3
        )

        knowledge_text = response.choices[0].message.content.strip()

        for line in knowledge_text.split("\n"):
            line = line.strip()
            if line and not line.startswith("#"):
                embedding = await self._embedder.embed(line)
                self.semantic.add_knowledge(
                    content=line,
                    embedding=embedding,
                    source_episodes=[episode.step_number]
                )

    async def query(
        self,
        query: str,
        include_working: bool = True,
        include_episodic: bool = True,
        include_semantic: bool = True,
        max_tokens: int = 4000
    ) -> str:
        """çµ±ä¸€æŸ¥è©¢ä»‹é¢"""
        results = []
        current_tokens = 0

        if include_working:
            working_prompt = self.working.to_prompt()
            working_tokens = len(working_prompt) // 3
            if current_tokens + working_tokens <= max_tokens:
                results.append(working_prompt)
                current_tokens += working_tokens

        if include_semantic and self.semantic._embedder:
            try:
                semantic_results = self.semantic.search(query, limit=5)
                for chunk, similarity in semantic_results:
                    chunk_tokens = len(chunk.content) // 3
                    if current_tokens + chunk_tokens > max_tokens:
                        break
                    results.append(f"[çŸ¥è­˜] {chunk.content}")
                    current_tokens += chunk_tokens
            except Exception:
                pass

        if include_episodic:
            recent = self.episodic.get_recent(5)
            for episode in recent:
                ep_content = episode.to_prompt(use_summary=True)
                ep_tokens = len(ep_content) // 3
                if current_tokens + ep_tokens > max_tokens:
                    break
                results.append(ep_content)
                current_tokens += ep_tokens

        return "\n\n".join(results)

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–å®Œæ•´çµ±è¨ˆ"""
        return {
            "working_memory": self.working.get_statistics(),
            "episodic_memory": self.episodic.get_statistics(),
            "semantic_memory": self.semantic.get_statistics(),
            "total_tokens": (
                self.working._current_tokens +
                self.episodic._current_tokens
            )
        }


# =============================================================================
# ç¤ºç¯„èˆ‡æ¸¬è©¦
# =============================================================================

def demo_memory_system():
    """ç¤ºç¯„è¨˜æ†¶ç³»çµ±"""
    print("=" * 60)
    print("ğŸ§  è¨˜æ†¶ç®¡ç†ç³»çµ±ç¤ºç¯„")
    print("=" * 60)

    # 1. å·¥ä½œè¨˜æ†¶ç¤ºç¯„
    print("\nğŸ“ 1. å·¥ä½œè¨˜æ†¶ç¤ºç¯„")
    print("-" * 40)

    working = WorkingMemory(max_tokens=1000)

    working.add(
        content="ç ”ç©¶å•é¡Œï¼šåˆ†æ 2024 å¹´ AI æ™¶ç‰‡å¸‚å ´",
        importance=1.0,
        priority=MemoryPriority.CRITICAL
    )

    working.add(
        content="éœ€è¦æœå°‹å¸‚å ´æ•¸æ“šå’Œä¸»è¦å» å•†è³‡è¨Š",
        importance=0.8,
        priority=MemoryPriority.HIGH
    )

    working.add(
        content="NVIDIA å¸‚å ´ä»½é¡ç´„ 80%",
        importance=0.9,
        priority=MemoryPriority.HIGH
    )

    print(working.to_prompt())
    print(f"\nçµ±è¨ˆ: {working.get_statistics()}")

    # 2. æƒ…ç¯€è¨˜æ†¶ç¤ºç¯„
    print("\nğŸ“ 2. æƒ…ç¯€è¨˜æ†¶ç¤ºç¯„")
    print("-" * 40)

    episodic = EpisodicMemory(max_tokens=5000)

    episodic.add_episode(
        thought="éœ€è¦äº†è§£ AI æ™¶ç‰‡å¸‚å ´çš„æ•´é«”è¦æ¨¡",
        action={"tool_name": "web_search", "query": "AI chip market size 2024"},
        observation="å¸‚å ´è¦æ¨¡ç´„ 500 å„„ç¾å…ƒï¼Œå¹´å¢é•· 30%",
        importance=0.8
    )

    episodic.add_episode(
        thought="å·²ç²å¾—å¸‚å ´è¦æ¨¡ï¼Œæ¥ä¸‹ä¾†åˆ†æä¸»è¦ç«¶çˆ­è€…",
        action={"tool_name": "web_search", "query": "NVIDIA AMD Intel AI chip"},
        observation="NVIDIA 80%ï¼ŒAMD 10%ï¼ŒIntel 5%",
        importance=0.9
    )

    print(episodic.to_prompt())
    print(f"\nçµ±è¨ˆ: {episodic.get_statistics()}")

    # 3. èªç¾©è¨˜æ†¶ç¤ºç¯„ï¼ˆä½¿ç”¨æ¨¡æ“¬åµŒå…¥ï¼‰
    print("\nğŸ“ 3. èªç¾©è¨˜æ†¶ç¤ºç¯„")
    print("-" * 40)

    semantic = SemanticMemory(embedding_dim=4)

    # ä½¿ç”¨ç°¡å–®çš„æ¨¡æ“¬åµŒå…¥å‡½æ•¸
    def simple_embedder(text: str) -> List[float]:
        # ç°¡å–®çš„æ¨¡æ“¬åµŒå…¥ï¼šåŸºæ–¼æ–‡å­—é•·åº¦å’Œé¦–å­—æ¯
        return [
            len(text) / 100,
            ord(text[0]) / 255 if text else 0,
            text.count(" ") / 10,
            len(set(text)) / 50
        ]

    semantic.set_embedder(simple_embedder)

    semantic.add_knowledge(
        content="NVIDIA åœ¨ AI æ™¶ç‰‡å¸‚å ´ä½”æ“š 80% ä»½é¡",
        source_episodes=[1, 2]
    )

    semantic.add_knowledge(
        content="AMD æ­£åœ¨ç©æ¥µè¿½è¶•ï¼Œç›®å‰å¸‚å ´ä»½é¡ç´„ 10%",
        source_episodes=[2]
    )

    print(semantic.to_prompt())
    print(f"\nçµ±è¨ˆ: {semantic.get_statistics()}")

    # 4. ç¶œåˆçµ±è¨ˆ
    print("\nğŸ“ 4. ç¶œåˆçµ±è¨ˆ")
    print("-" * 40)
    print(f"å·¥ä½œè¨˜æ†¶: {len(working)} é …ï¼Œä½¿ç”¨ç‡ {working.utilization*100:.1f}%")
    print(f"æƒ…ç¯€è¨˜æ†¶: {len(episodic)} é …ï¼Œä½¿ç”¨ç‡ {episodic.utilization*100:.1f}%")
    print(f"èªç¾©è¨˜æ†¶: {len(semantic)} é …")


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="è¨˜æ†¶ç®¡ç†ç³»çµ± - ç¬¬ 6 ç« ç¯„ä¾‹")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")
    parser.add_argument("--test", action="store_true", help="åŸ·è¡Œæ¸¬è©¦")

    args = parser.parse_args()

    if args.demo or not args.test:
        demo_memory_system()


if __name__ == "__main__":
    main()
