#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 6 ç« ï¼šé•·çŸ­æ™‚è¨˜æ†¶ç®¡ç†
æƒ…ç¯€å£“ç¸®å™¨å¯¦ç¾

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†å¤šå±¤ç´šçš„æƒ…ç¯€å£“ç¸®ç­–ç•¥ï¼š
1. è¼•åº¦å£“ç¸®ï¼šä¿ç•™ 80% ç´°ç¯€
2. ä¸­åº¦å£“ç¸®ï¼šä¿ç•™ 50% ç´°ç¯€
3. é‡åº¦å£“ç¸®ï¼šåªä¿ç•™é—œéµçµè«–

ä½¿ç”¨æ–¹å¼ï¼š
    python compressor.py --demo
    python compressor.py --text "è¦å£“ç¸®çš„å…§å®¹"
"""

import asyncio
import hashlib
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional

from dotenv import load_dotenv
from openai import AsyncOpenAI

load_dotenv()


class CompressionLevel(Enum):
    """å£“ç¸®ç´šåˆ¥"""
    LIGHT = "light"      # è¼•åº¦å£“ç¸®
    MEDIUM = "medium"    # ä¸­åº¦å£“ç¸®
    HEAVY = "heavy"      # é‡åº¦å£“ç¸®


@dataclass
class CompressionResult:
    """å£“ç¸®çµæœ"""
    original: str
    compressed: str
    level: CompressionLevel
    original_tokens: int
    compressed_tokens: int
    compression_ratio: float

    def to_dict(self) -> dict:
        return {
            "level": self.level.value,
            "original_tokens": self.original_tokens,
            "compressed_tokens": self.compressed_tokens,
            "compression_ratio": self.compression_ratio
        }


class ProgressiveCompressor:
    """
    æ¼¸é€²å¼å£“ç¸®å™¨

    â€¹1â€º æ ¹æ“šè¨˜æ†¶å¹´é½¡èª¿æ•´å£“ç¸®ç¨‹åº¦
    â€¹2â€º è¶ŠèˆŠçš„è¨˜æ†¶å£“ç¸®è¶Šç‹ 
    â€¹3â€º æ”¯æ´æ‰¹æ¬¡å£“ç¸®
    """

    COMPRESSION_PROMPTS = {
        CompressionLevel.LIGHT: """è«‹è¼•åº¦å£“ç¸®ä»¥ä¸‹å…§å®¹ï¼Œä¿ç•™ 80% çš„ç´°ç¯€ã€‚
ä¿ç•™ï¼šæ‰€æœ‰æ•¸æ“šã€ä¸»è¦ç™¼ç¾ã€é‡è¦å¼•ç”¨
çœç•¥ï¼šé‡è¤‡çš„æè¿°ã€éæ¸¡èªå¥

åŸå§‹å…§å®¹ï¼š
{content}

å£“ç¸®å¾Œï¼ˆç´„ 100 å­—ï¼‰ï¼š""",

        CompressionLevel.MEDIUM: """è«‹ä¸­åº¦å£“ç¸®ä»¥ä¸‹å…§å®¹ï¼Œä¿ç•™ 50% çš„ç´°ç¯€ã€‚
ä¿ç•™ï¼šé—œéµæ•¸æ“šã€ä¸»è¦çµè«–
çœç•¥ï¼šéç¨‹æè¿°ã€æ¬¡è¦ç´°ç¯€

åŸå§‹å…§å®¹ï¼š
{content}

å£“ç¸®å¾Œï¼ˆç´„ 50 å­—ï¼‰ï¼š""",

        CompressionLevel.HEAVY: """è«‹é‡åº¦å£“ç¸®ä»¥ä¸‹å…§å®¹ï¼Œåªä¿ç•™æ ¸å¿ƒçµè«–ã€‚
ä¿ç•™ï¼šæœ€é—œéµçš„ 1-2 å€‹çµè«–
çœç•¥ï¼šæ‰€æœ‰ç´°ç¯€å’Œéç¨‹

åŸå§‹å…§å®¹ï¼š
{content}

å£“ç¸®å¾Œï¼ˆç´„ 20 å­—ï¼‰ï¼š"""
    }

    def __init__(
        self,
        client: Optional[AsyncOpenAI] = None,
        model: str = "gpt-4o-mini"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model
        self._cache: Dict[str, str] = {}

    def get_compression_level(self, age_hours: float) -> CompressionLevel:
        """æ ¹æ“šå¹´é½¡æ±ºå®šå£“ç¸®ç´šåˆ¥"""
        if age_hours < 1:
            return CompressionLevel.LIGHT
        elif age_hours < 4:
            return CompressionLevel.MEDIUM
        else:
            return CompressionLevel.HEAVY

    async def compress(
        self,
        content: str,
        level: Optional[CompressionLevel] = None,
        age_hours: Optional[float] = None
    ) -> CompressionResult:
        """
        å£“ç¸®å…§å®¹

        â€¹1â€º å¦‚æœæä¾›å¹´é½¡ï¼Œè‡ªå‹•æ±ºå®šå£“ç¸®ç´šåˆ¥
        â€¹2â€º ä½¿ç”¨å¿«å–é¿å…é‡è¤‡å£“ç¸®
        """
        # æ±ºå®šå£“ç¸®ç´šåˆ¥
        if level is None:
            if age_hours is not None:
                level = self.get_compression_level(age_hours)
            else:
                level = CompressionLevel.MEDIUM

        # æª¢æŸ¥å¿«å–
        cache_key = hashlib.md5(f"{content}:{level.value}".encode()).hexdigest()
        if cache_key in self._cache:
            compressed = self._cache[cache_key]
        else:
            # åŸ·è¡Œå£“ç¸®
            prompt = self.COMPRESSION_PROMPTS[level].format(content=content)

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
                temperature=0.3
            )

            compressed = response.choices[0].message.content.strip()
            self._cache[cache_key] = compressed

        # è¨ˆç®—çµ±è¨ˆ
        original_tokens = len(content) // 3
        compressed_tokens = len(compressed) // 3
        ratio = compressed_tokens / original_tokens if original_tokens > 0 else 1.0

        return CompressionResult(
            original=content,
            compressed=compressed,
            level=level,
            original_tokens=original_tokens,
            compressed_tokens=compressed_tokens,
            compression_ratio=ratio
        )

    async def batch_compress(
        self,
        contents: List[str],
        ages_hours: Optional[List[float]] = None,
        concurrency: int = 5
    ) -> List[CompressionResult]:
        """æ‰¹æ¬¡å£“ç¸®"""
        semaphore = asyncio.Semaphore(concurrency)

        async def compress_one(content: str, age: Optional[float]) -> CompressionResult:
            async with semaphore:
                return await self.compress(content, age_hours=age)

        if ages_hours is None:
            ages_hours = [None] * len(contents)

        tasks = [
            compress_one(content, age)
            for content, age in zip(contents, ages_hours)
        ]

        return await asyncio.gather(*tasks)

    def compress_sync(
        self,
        content: str,
        level: Optional[CompressionLevel] = None
    ) -> str:
        """åŒæ­¥å£“ç¸®ï¼ˆè¿”å›å£“ç¸®å¾Œçš„æ–‡å­—ï¼‰"""
        result = asyncio.get_event_loop().run_until_complete(
            self.compress(content, level=level)
        )
        return result.compressed


class AdaptiveCompressor:
    """
    è‡ªé©æ‡‰å£“ç¸®å™¨

    â€¹1â€º æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´å£“ç¸®ç­–ç•¥
    â€¹2â€º æ•¸æ“šå¯†é›†å‹å…§å®¹ä¿ç•™æ›´å¤šæ•¸å­—
    â€¹3â€º æ•˜è¿°å‹å…§å®¹ä¿ç•™çµæ§‹
    """

    CONTENT_TYPE_PROMPTS = {
        "data": """è«‹å£“ç¸®ä»¥ä¸‹æ•¸æ“šå¯†é›†å‹å…§å®¹ï¼Œå„ªå…ˆä¿ç•™æ‰€æœ‰æ•¸å­—å’Œç™¾åˆ†æ¯”ã€‚

åŸå§‹å…§å®¹ï¼š
{content}

å£“ç¸®å¾Œï¼ˆä¿ç•™æ‰€æœ‰æ•¸å­—ï¼‰ï¼š""",

        "narrative": """è«‹å£“ç¸®ä»¥ä¸‹æ•˜è¿°å‹å…§å®¹ï¼Œä¿ç•™ä¸»è¦è§€é»å’Œçµæ§‹ã€‚

åŸå§‹å…§å®¹ï¼š
{content}

å£“ç¸®å¾Œï¼ˆä¿ç•™ä¸»è¦è§€é»ï¼‰ï¼š""",

        "technical": """è«‹å£“ç¸®ä»¥ä¸‹æŠ€è¡“å‹å…§å®¹ï¼Œä¿ç•™è¡“èªå’Œå› æœé—œä¿‚ã€‚

åŸå§‹å…§å®¹ï¼š
{content}

å£“ç¸®å¾Œï¼ˆä¿ç•™æŠ€è¡“è¦é»ï¼‰ï¼š"""
    }

    def __init__(self, client: Optional[AsyncOpenAI] = None):
        self.client = client or AsyncOpenAI()

    def classify_content(self, content: str) -> str:
        """åˆ†é¡å…§å®¹é¡å‹"""
        # ç°¡å–®è¦å‰‡åˆ¤æ–·
        digit_ratio = sum(c.isdigit() for c in content) / len(content) if content else 0
        has_percentage = "%" in content

        if digit_ratio > 0.05 or has_percentage:
            return "data"
        elif any(kw in content.lower() for kw in ["å‡½æ•¸", "é¡åˆ¥", "api", "å¯¦ç¾", "æ¶æ§‹"]):
            return "technical"
        else:
            return "narrative"

    async def compress(self, content: str) -> str:
        """è‡ªé©æ‡‰å£“ç¸®"""
        content_type = self.classify_content(content)
        prompt = self.CONTENT_TYPE_PROMPTS[content_type].format(content=content)

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.3
        )

        return response.choices[0].message.content.strip()


# =============================================================================
# ç¤ºç¯„
# =============================================================================

async def demo():
    """ç¤ºç¯„å£“ç¸®åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ—œï¸ æƒ…ç¯€å£“ç¸®å™¨ç¤ºç¯„")
    print("=" * 60)

    compressor = ProgressiveCompressor()

    # æ¸¬è©¦å…§å®¹
    test_content = """
    [æ­¥é©Ÿ 5]
    æ€è€ƒï¼šæ ¹æ“šæœå°‹çµæœï¼Œæˆ‘éœ€è¦æ·±å…¥äº†è§£ NVIDIA çš„ GPU æ¶æ§‹å„ªå‹¢ã€‚
    ä¸»è¦é—œæ³¨ CUDA ç”Ÿæ…‹ç³»çµ±ã€Tensor Core æŠ€è¡“ã€ä»¥åŠèˆ‡ç«¶çˆ­å°æ‰‹çš„å°æ¯”ã€‚

    è¡Œå‹•ï¼šèª¿ç”¨ web_browser

    è§€å¯Ÿï¼šNVIDIA çš„ GPU æ¶æ§‹å„ªå‹¢ä¸»è¦é«”ç¾åœ¨ä¸‰å€‹æ–¹é¢ï¼š
    1. CUDA ç”Ÿæ…‹ç³»çµ±ï¼šè¶…é 400 è¬é–‹ç™¼è€…ï¼Œ10+ å¹´ç©ç´¯
    2. Tensor Coreï¼šå°ˆç‚º AI å„ªåŒ–çš„è¨ˆç®—å–®å…ƒï¼ŒFP8 ç²¾åº¦æ”¯æ´
    3. è»Ÿé«”æ£§ï¼šcuDNNã€TensorRTã€Triton æ¨ç†ä¼ºæœå™¨
    èˆ‡ AMD ç›¸æ¯”ï¼ŒNVIDIA çš„è»Ÿé«”ç”Ÿæ…‹æ›´æˆç†Ÿï¼Œä½† AMD åœ¨æ€§åƒ¹æ¯”ä¸Šæœ‰å„ªå‹¢ã€‚
    """

    print(f"\nåŸå§‹å…§å®¹é•·åº¦: {len(test_content)} å­—ç¬¦")

    # æ¸¬è©¦ä¸åŒå£“ç¸®ç´šåˆ¥
    for level in CompressionLevel:
        print(f"\n{'='*40}")
        print(f"ğŸ“Š {level.value.upper()} å£“ç¸®")
        print("=" * 40)

        result = await compressor.compress(test_content, level=level)

        print(f"å£“ç¸®å¾Œ: {result.compressed}")
        print(f"å£“ç¸®æ¯”: {result.compression_ratio*100:.1f}%")
        print(f"Token: {result.original_tokens} â†’ {result.compressed_tokens}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="æƒ…ç¯€å£“ç¸®å™¨")
    parser.add_argument("--demo", action="store_true", help="åŸ·è¡Œç¤ºç¯„")
    parser.add_argument("--text", type=str, help="è¦å£“ç¸®çš„æ–‡å­—")
    parser.add_argument(
        "--level",
        choices=["light", "medium", "heavy"],
        default="medium",
        help="å£“ç¸®ç´šåˆ¥"
    )

    args = parser.parse_args()

    if args.text:
        compressor = ProgressiveCompressor()
        level = CompressionLevel(args.level)
        result = asyncio.run(compressor.compress(args.text, level=level))
        print(f"å£“ç¸®å¾Œ: {result.compressed}")
        print(f"å£“ç¸®æ¯”: {result.compression_ratio*100:.1f}%")
    else:
        asyncio.run(demo())


if __name__ == "__main__":
    main()
