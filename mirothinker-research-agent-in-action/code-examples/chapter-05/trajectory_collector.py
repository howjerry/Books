#!/usr/bin/env python3
"""
æ·±åº¦ç ”ç©¶ä»£ç†äººå¯¦æˆ° - ç¬¬ 5 ç« ï¼šå·¥å…·èª¿ç”¨èˆ‡è»Œè·¡æ”¶é›†
è»Œè·¡æ”¶é›†å™¨å®Œæ•´å¯¦ç¾

é€™å€‹æ¨¡çµ„å¯¦ç¾äº†å®Œæ•´çš„è»Œè·¡æ”¶é›†ç³»çµ±ï¼ŒåŒ…å«ï¼š
1. è»Œè·¡è³‡æ–™çµæ§‹å®šç¾©
2. è»Œè·¡æ”¶é›†èˆ‡æŒä¹…åŒ–
3. çå‹µä¿¡è™Ÿè¨ˆç®—
4. RLEF è¨“ç·´è³‡æ–™ç”Ÿæˆ

ä½¿ç”¨æ–¹å¼ï¼š
    python trajectory_collector.py
    python trajectory_collector.py --demo
    python trajectory_collector.py --export trajectories.jsonl
"""

import asyncio
import hashlib
import json
import os
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, List, Optional

from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()


# =============================================================================
# è»Œè·¡è³‡æ–™çµæ§‹
# =============================================================================

class StepType(Enum):
    """æ­¥é©Ÿé¡å‹"""
    THOUGHT = "thought"
    ACTION = "action"
    OBSERVATION = "observation"


@dataclass
class TrajectoryStep:
    """
    è»Œè·¡æ­¥é©Ÿ

    â€¹1â€º æ¯å€‹æ­¥é©Ÿè¨˜éŒ„æ€è€ƒã€è¡Œå‹•æˆ–è§€å¯Ÿ
    â€¹2â€º åŒ…å«æ™‚é–“æˆ³å’Œå…ƒæ•¸æ“š
    """
    step_type: StepType
    content: Any
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict:
        return {
            "step_type": self.step_type.value,
            "content": self.content,
            "timestamp": self.timestamp,
            "metadata": self.metadata
        }

    @classmethod
    def from_dict(cls, data: dict) -> "TrajectoryStep":
        return cls(
            step_type=StepType(data["step_type"]),
            content=data["content"],
            timestamp=data.get("timestamp", time.time()),
            metadata=data.get("metadata", {})
        )


@dataclass
class ToolCall:
    """
    å·¥å…·èª¿ç”¨è¨˜éŒ„

    â€¹1â€º è¨˜éŒ„å·¥å…·åç¨±å’Œåƒæ•¸
    â€¹2â€º è¿½è¹¤åŸ·è¡Œçµæœå’Œæ™‚é–“
    """
    tool_name: str
    arguments: Dict[str, Any]
    result: Any = None
    success: bool = False
    execution_time: float = 0.0
    error: Optional[str] = None

    def to_dict(self) -> dict:
        return {
            "tool_name": self.tool_name,
            "arguments": self.arguments,
            "result": self.result,
            "success": self.success,
            "execution_time": self.execution_time,
            "error": self.error
        }


@dataclass
class Trajectory:
    """
    å®Œæ•´è»Œè·¡

    â€¹1â€º åŒ…å«æ‰€æœ‰ Thought-Action-Observation æ­¥é©Ÿ
    â€¹2â€º è¨˜éŒ„ä»»å‹™è³‡è¨Šå’Œæœ€çµ‚çµæœ
    â€¹3â€º æ”¯æ´çå‹µä¿¡è™Ÿæ¨™è¨»
    """
    trajectory_id: str
    task_query: str
    steps: List[TrajectoryStep] = field(default_factory=list)
    tool_calls: List[ToolCall] = field(default_factory=list)
    final_answer: Optional[str] = None
    success: bool = False
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    rewards: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def duration(self) -> float:
        """è¨ˆç®—ç¸½è€—æ™‚"""
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time

    @property
    def total_tokens(self) -> int:
        """ä¼°ç®—ç¸½ token æ•¸"""
        total = 0
        for step in self.steps:
            if isinstance(step.content, str):
                total += len(step.content) // 3
            elif isinstance(step.content, dict):
                total += len(json.dumps(step.content, ensure_ascii=False)) // 3
        return total

    def add_thought(self, content: str, **metadata) -> None:
        """æ·»åŠ æ€è€ƒæ­¥é©Ÿ"""
        self.steps.append(TrajectoryStep(
            step_type=StepType.THOUGHT,
            content=content,
            metadata=metadata
        ))

    def add_action(self, tool_name: str, arguments: dict, **metadata) -> ToolCall:
        """æ·»åŠ è¡Œå‹•æ­¥é©Ÿ"""
        tool_call = ToolCall(
            tool_name=tool_name,
            arguments=arguments
        )
        self.tool_calls.append(tool_call)

        self.steps.append(TrajectoryStep(
            step_type=StepType.ACTION,
            content={
                "tool_name": tool_name,
                "arguments": arguments
            },
            metadata=metadata
        ))

        return tool_call

    def add_observation(self, content: Any, tool_call: Optional[ToolCall] = None, **metadata) -> None:
        """æ·»åŠ è§€å¯Ÿæ­¥é©Ÿ"""
        if tool_call:
            tool_call.result = content
            tool_call.success = True

        self.steps.append(TrajectoryStep(
            step_type=StepType.OBSERVATION,
            content=content,
            metadata=metadata
        ))

    def complete(self, final_answer: str, success: bool = True) -> None:
        """å®Œæˆè»Œè·¡è¨˜éŒ„"""
        self.final_answer = final_answer
        self.success = success
        self.end_time = time.time()

    def to_dict(self) -> dict:
        return {
            "trajectory_id": self.trajectory_id,
            "task_query": self.task_query,
            "steps": [step.to_dict() for step in self.steps],
            "tool_calls": [tc.to_dict() for tc in self.tool_calls],
            "final_answer": self.final_answer,
            "success": self.success,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "duration": self.duration,
            "total_tokens": self.total_tokens,
            "rewards": self.rewards,
            "metadata": self.metadata
        }

    def to_training_format(self) -> dict:
        """è½‰æ›ç‚ºè¨“ç·´æ ¼å¼ï¼ˆRLEF æ ¼å¼ï¼‰"""
        return {
            "id": self.trajectory_id,
            "query": self.task_query,
            "trajectory": [step.to_dict() for step in self.steps],
            "answer": self.final_answer,
            "reward": sum(self.rewards.values()) if self.rewards else 0.0,
            "reward_breakdown": self.rewards,
            "metadata": {
                "duration": self.duration,
                "tool_count": len(self.tool_calls),
                "step_count": len(self.steps),
                "success": self.success
            }
        }


# =============================================================================
# çå‹µè¨ˆç®—å™¨
# =============================================================================

class RewardCalculator:
    """
    çå‹µè¨ˆç®—å™¨

    â€¹1â€º å¤šç¶­åº¦çå‹µä¿¡è™Ÿè¨­è¨ˆ
    â€¹2â€º æ”¯æ´è‡ªè¨‚æ¬Šé‡
    â€¹3â€º æä¾›çå‹µåˆ†è§£å ±å‘Š
    """

    def __init__(self, weights: Optional[Dict[str, float]] = None):
        self.weights = weights or {
            "task_completion": 0.30,
            "tool_efficiency": 0.20,
            "answer_quality": 0.25,
            "factual_accuracy": 0.15,
            "token_efficiency": 0.10
        }

    def calculate(
        self,
        trajectory: Trajectory,
        ground_truth: Optional[str] = None,
        quality_score: Optional[float] = None
    ) -> Dict[str, float]:
        """
        è¨ˆç®—è»Œè·¡çš„çå‹µä¿¡è™Ÿ

        â€¹1â€º ä»»å‹™å®Œæˆåº¦ï¼šæ˜¯å¦æˆåŠŸå®Œæˆä»»å‹™
        â€¹2â€º å·¥å…·æ•ˆç‡ï¼šå·¥å…·ä½¿ç”¨çš„æ•ˆç‡
        â€¹3â€º ç­”æ¡ˆå“è³ªï¼šç­”æ¡ˆçš„å“è³ªè©•ä¼°
        â€¹4â€º äº‹å¯¦æº–ç¢ºåº¦ï¼šèˆ‡çœŸå¯¦ç­”æ¡ˆçš„å»åˆåº¦
        â€¹5â€º Token æ•ˆç‡ï¼šä½¿ç”¨çš„ token æ•¸é‡
        """
        rewards = {}

        # 1. ä»»å‹™å®Œæˆåº¦
        rewards["task_completion"] = self._calc_task_completion(trajectory)

        # 2. å·¥å…·æ•ˆç‡
        rewards["tool_efficiency"] = self._calc_tool_efficiency(trajectory)

        # 3. ç­”æ¡ˆå“è³ª
        rewards["answer_quality"] = self._calc_answer_quality(
            trajectory, quality_score
        )

        # 4. äº‹å¯¦æº–ç¢ºåº¦
        rewards["factual_accuracy"] = self._calc_factual_accuracy(
            trajectory, ground_truth
        )

        # 5. Token æ•ˆç‡
        rewards["token_efficiency"] = self._calc_token_efficiency(trajectory)

        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        total = sum(
            rewards[k] * self.weights.get(k, 0)
            for k in rewards
        )
        rewards["total"] = total

        return rewards

    def _calc_task_completion(self, trajectory: Trajectory) -> float:
        """è¨ˆç®—ä»»å‹™å®Œæˆåº¦"""
        if not trajectory.success:
            return 0.0

        if trajectory.final_answer:
            # æœ‰ç­”æ¡ˆå¾—åŸºç¤åˆ†
            score = 0.6

            # ç­”æ¡ˆé•·åº¦åˆç†æ€§
            answer_len = len(trajectory.final_answer)
            if 100 <= answer_len <= 2000:
                score += 0.2
            elif 50 <= answer_len <= 5000:
                score += 0.1

            # æœ‰ä½¿ç”¨å·¥å…·å¾—é¡å¤–åˆ†
            if trajectory.tool_calls:
                score += 0.2

            return min(score, 1.0)

        return 0.3  # å®Œæˆä½†ç„¡ç­”æ¡ˆ

    def _calc_tool_efficiency(self, trajectory: Trajectory) -> float:
        """è¨ˆç®—å·¥å…·ä½¿ç”¨æ•ˆç‡"""
        if not trajectory.tool_calls:
            return 0.5  # æ²’æœ‰ä½¿ç”¨å·¥å…·ï¼Œä¸­æ€§è©•åƒ¹

        total_calls = len(trajectory.tool_calls)
        successful_calls = sum(1 for tc in trajectory.tool_calls if tc.success)

        # æˆåŠŸç‡
        success_rate = successful_calls / total_calls

        # å·¥å…·å¤šæ¨£æ€§ï¼ˆä½¿ç”¨ä¸åŒç¨®é¡çš„å·¥å…·ï¼‰
        unique_tools = len(set(tc.tool_name for tc in trajectory.tool_calls))
        diversity_bonus = min(unique_tools * 0.1, 0.3)

        # é¿å…éåº¦ä½¿ç”¨ï¼ˆè¶…é 10 æ¬¡æ‰£åˆ†ï¼‰
        overuse_penalty = max(0, (total_calls - 10) * 0.05)

        score = success_rate * 0.7 + diversity_bonus - overuse_penalty

        return max(0, min(score, 1.0))

    def _calc_answer_quality(
        self,
        trajectory: Trajectory,
        quality_score: Optional[float] = None
    ) -> float:
        """è¨ˆç®—ç­”æ¡ˆå“è³ª"""
        if quality_score is not None:
            return quality_score

        # ç°¡æ˜“è©•ä¼°ï¼ˆå¯¦éš›æ‡‰ä½¿ç”¨ LLM è©•ä¼°ï¼‰
        if not trajectory.final_answer:
            return 0.0

        answer = trajectory.final_answer
        score = 0.0

        # é•·åº¦è©•ä¼°
        if len(answer) >= 100:
            score += 0.3

        # çµæ§‹è©•ä¼°ï¼ˆæ˜¯å¦æœ‰æ¢ç†ï¼‰
        if any(marker in answer for marker in ['1.', 'â€¢', '-', 'é¦–å…ˆ', 'å…¶æ¬¡']):
            score += 0.2

        # å¼•ç”¨ä¾†æº
        if any(marker in answer for marker in ['æ ¹æ“š', 'ä¾†æº', 'åƒè€ƒ', 'ç ”ç©¶é¡¯ç¤º']):
            score += 0.2

        # æœ‰å…·é«”æ•¸æ“š
        import re
        if re.search(r'\d+%|\d+\.\d+|ç¬¬\d+', answer):
            score += 0.15

        # æœ‰çµè«–
        if any(marker in answer for marker in ['ç¸½çµ', 'çµè«–', 'ç¶œä¸Šæ‰€è¿°', 'å› æ­¤']):
            score += 0.15

        return min(score, 1.0)

    def _calc_factual_accuracy(
        self,
        trajectory: Trajectory,
        ground_truth: Optional[str] = None
    ) -> float:
        """è¨ˆç®—äº‹å¯¦æº–ç¢ºåº¦"""
        if not ground_truth or not trajectory.final_answer:
            return 0.5  # ç„¡æ³•è©•ä¼°æ™‚çµ¦ä¸­æ€§åˆ†æ•¸

        # ç°¡æ˜“æ–‡å­—ç›¸ä¼¼åº¦ï¼ˆå¯¦éš›æ‡‰ä½¿ç”¨æ›´è¤‡é›œçš„è©•ä¼°ï¼‰
        answer_words = set(trajectory.final_answer.lower().split())
        truth_words = set(ground_truth.lower().split())

        if not truth_words:
            return 0.5

        overlap = len(answer_words & truth_words)
        recall = overlap / len(truth_words)
        precision = overlap / len(answer_words) if answer_words else 0

        # F1 åˆ†æ•¸
        if precision + recall > 0:
            f1 = 2 * precision * recall / (precision + recall)
        else:
            f1 = 0

        return f1

    def _calc_token_efficiency(self, trajectory: Trajectory) -> float:
        """è¨ˆç®— Token æ•ˆç‡"""
        total_tokens = trajectory.total_tokens

        # ç†æƒ³ç¯„åœï¼š1000-5000 tokens
        if 1000 <= total_tokens <= 5000:
            return 1.0
        elif 500 <= total_tokens < 1000:
            return 0.8
        elif 5000 < total_tokens <= 10000:
            return 0.7
        elif 10000 < total_tokens <= 20000:
            return 0.5
        elif total_tokens < 500:
            return 0.6  # å¤ªå°‘å¯èƒ½ä¸å¤ æ·±å…¥
        else:
            return 0.3  # è¶…é 20000 æ•ˆç‡ä½


# =============================================================================
# è»Œè·¡æ”¶é›†å™¨
# =============================================================================

class TrajectoryCollector:
    """
    è»Œè·¡æ”¶é›†å™¨

    â€¹1â€º ç®¡ç†è»Œè·¡çš„ç”Ÿå‘½é€±æœŸ
    â€¹2â€º æ”¯æ´æŒä¹…åŒ–å­˜å„²
    â€¹3â€º æä¾›è»Œè·¡æŸ¥è©¢å’Œéæ¿¾
    """

    def __init__(
        self,
        storage_path: Optional[str] = None,
        auto_save: bool = True
    ):
        self.storage_path = storage_path or "./trajectories"
        self.auto_save = auto_save
        self.trajectories: Dict[str, Trajectory] = {}
        self.reward_calculator = RewardCalculator()

        # ç¢ºä¿å­˜å„²ç›®éŒ„å­˜åœ¨
        os.makedirs(self.storage_path, exist_ok=True)

    def _generate_id(self, query: str) -> str:
        """ç”Ÿæˆè»Œè·¡ ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        return f"traj_{timestamp}_{query_hash}"

    def start_trajectory(self, query: str, **metadata) -> Trajectory:
        """
        é–‹å§‹æ–°è»Œè·¡

        â€¹1â€º å‰µå»ºè»Œè·¡ç‰©ä»¶
        â€¹2â€º è¨»å†Šåˆ°æ”¶é›†å™¨
        """
        trajectory_id = self._generate_id(query)
        trajectory = Trajectory(
            trajectory_id=trajectory_id,
            task_query=query,
            metadata=metadata
        )
        self.trajectories[trajectory_id] = trajectory
        return trajectory

    def complete_trajectory(
        self,
        trajectory: Trajectory,
        final_answer: str,
        success: bool = True,
        ground_truth: Optional[str] = None,
        quality_score: Optional[float] = None
    ) -> Dict[str, float]:
        """
        å®Œæˆè»Œè·¡è¨˜éŒ„

        â€¹1â€º æ¨™è¨˜å®Œæˆ
        â€¹2â€º è¨ˆç®—çå‹µ
        â€¹3â€º è‡ªå‹•ä¿å­˜
        """
        trajectory.complete(final_answer, success)

        # è¨ˆç®—çå‹µ
        rewards = self.reward_calculator.calculate(
            trajectory,
            ground_truth=ground_truth,
            quality_score=quality_score
        )
        trajectory.rewards = rewards

        # è‡ªå‹•ä¿å­˜
        if self.auto_save:
            self.save_trajectory(trajectory)

        return rewards

    def save_trajectory(self, trajectory: Trajectory) -> str:
        """ä¿å­˜è»Œè·¡åˆ°æª”æ¡ˆ"""
        file_path = os.path.join(
            self.storage_path,
            f"{trajectory.trajectory_id}.json"
        )
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(trajectory.to_dict(), f, ensure_ascii=False, indent=2)
        return file_path

    def load_trajectory(self, trajectory_id: str) -> Optional[Trajectory]:
        """å¾æª”æ¡ˆè¼‰å…¥è»Œè·¡"""
        file_path = os.path.join(
            self.storage_path,
            f"{trajectory_id}.json"
        )
        if not os.path.exists(file_path):
            return None

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        trajectory = Trajectory(
            trajectory_id=data["trajectory_id"],
            task_query=data["task_query"],
            final_answer=data.get("final_answer"),
            success=data.get("success", False),
            start_time=data.get("start_time", time.time()),
            end_time=data.get("end_time"),
            rewards=data.get("rewards", {}),
            metadata=data.get("metadata", {})
        )

        # é‡å»ºæ­¥é©Ÿ
        for step_data in data.get("steps", []):
            trajectory.steps.append(TrajectoryStep.from_dict(step_data))

        # é‡å»ºå·¥å…·èª¿ç”¨
        for tc_data in data.get("tool_calls", []):
            trajectory.tool_calls.append(ToolCall(**tc_data))

        return trajectory

    def export_for_training(
        self,
        output_path: str,
        min_reward: float = 0.0,
        format: str = "jsonl"
    ) -> int:
        """
        åŒ¯å‡ºè¨“ç·´è³‡æ–™

        â€¹1â€º éæ¿¾ä½å“è³ªè»Œè·¡
        â€¹2â€º è½‰æ›ç‚ºè¨“ç·´æ ¼å¼
        â€¹3â€º æ”¯æ´ JSONL æ ¼å¼
        """
        exported = 0

        with open(output_path, 'w', encoding='utf-8') as f:
            for trajectory in self.trajectories.values():
                # éæ¿¾ä½å“è³ªè»Œè·¡
                total_reward = trajectory.rewards.get("total", 0)
                if total_reward < min_reward:
                    continue

                training_data = trajectory.to_training_format()

                if format == "jsonl":
                    f.write(json.dumps(training_data, ensure_ascii=False) + "\n")
                else:
                    json.dump(training_data, f, ensure_ascii=False)
                    f.write("\n")

                exported += 1

        return exported

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–è»Œè·¡çµ±è¨ˆ"""
        if not self.trajectories:
            return {"total": 0}

        total = len(self.trajectories)
        successful = sum(1 for t in self.trajectories.values() if t.success)
        rewards = [
            t.rewards.get("total", 0)
            for t in self.trajectories.values()
            if t.rewards
        ]

        return {
            "total": total,
            "successful": successful,
            "success_rate": successful / total if total > 0 else 0,
            "avg_reward": sum(rewards) / len(rewards) if rewards else 0,
            "min_reward": min(rewards) if rewards else 0,
            "max_reward": max(rewards) if rewards else 0,
            "avg_steps": sum(len(t.steps) for t in self.trajectories.values()) / total,
            "avg_tools": sum(len(t.tool_calls) for t in self.trajectories.values()) / total
        }

    def filter_trajectories(
        self,
        min_reward: Optional[float] = None,
        max_reward: Optional[float] = None,
        success_only: bool = False,
        min_tools: int = 0
    ) -> List[Trajectory]:
        """éæ¿¾è»Œè·¡"""
        result = []
        for trajectory in self.trajectories.values():
            total_reward = trajectory.rewards.get("total", 0)

            if min_reward is not None and total_reward < min_reward:
                continue
            if max_reward is not None and total_reward > max_reward:
                continue
            if success_only and not trajectory.success:
                continue
            if len(trajectory.tool_calls) < min_tools:
                continue

            result.append(trajectory)

        return result


# =============================================================================
# è»Œè·¡å›æ”¾å™¨
# =============================================================================

class TrajectoryReplayer:
    """
    è»Œè·¡å›æ”¾å™¨

    â€¹1â€º è¦–è¦ºåŒ–å±•ç¤ºè»Œè·¡
    â€¹2â€º æ”¯æ´æ­¥é©Ÿç´šåˆ¥å›æ”¾
    """

    def replay(self, trajectory: Trajectory, delay: float = 0.5) -> None:
        """å›æ”¾è»Œè·¡"""
        print("=" * 60)
        print(f"ğŸ¬ å›æ”¾è»Œè·¡: {trajectory.trajectory_id}")
        print(f"ğŸ“ ä»»å‹™: {trajectory.task_query}")
        print("=" * 60)

        for i, step in enumerate(trajectory.steps, 1):
            time.sleep(delay)

            if step.step_type == StepType.THOUGHT:
                print(f"\nğŸ’­ [{i}] æ€è€ƒ")
                print(f"   {step.content[:200]}..." if len(str(step.content)) > 200 else f"   {step.content}")

            elif step.step_type == StepType.ACTION:
                content = step.content
                print(f"\nğŸ”§ [{i}] è¡Œå‹•")
                print(f"   å·¥å…·: {content['tool_name']}")
                print(f"   åƒæ•¸: {json.dumps(content['arguments'], ensure_ascii=False)[:100]}...")

            elif step.step_type == StepType.OBSERVATION:
                print(f"\nğŸ‘ï¸ [{i}] è§€å¯Ÿ")
                content_str = str(step.content)
                print(f"   {content_str[:200]}..." if len(content_str) > 200 else f"   {content_str}")

        print("\n" + "=" * 60)
        print("ğŸ“Š è»Œè·¡æ‘˜è¦")
        print("=" * 60)
        print(f"   ç¸½æ­¥é©Ÿæ•¸: {len(trajectory.steps)}")
        print(f"   å·¥å…·èª¿ç”¨: {len(trajectory.tool_calls)}")
        print(f"   ç¸½è€—æ™‚: {trajectory.duration:.2f}s")
        print(f"   æˆåŠŸ: {'âœ…' if trajectory.success else 'âŒ'}")

        if trajectory.rewards:
            print(f"\nğŸ“ˆ çå‹µä¿¡è™Ÿ:")
            for key, value in trajectory.rewards.items():
                print(f"   â€¢ {key}: {value:.3f}")

        if trajectory.final_answer:
            print(f"\nğŸ“ æœ€çµ‚ç­”æ¡ˆ:")
            answer_preview = trajectory.final_answer[:300]
            print(f"   {answer_preview}...")


# =============================================================================
# ç¤ºç¯„åŠŸèƒ½
# =============================================================================

def demo_trajectory_collection():
    """å±•ç¤ºè»Œè·¡æ”¶é›†ç³»çµ±"""
    print("=" * 60)
    print("ğŸ“Š è»Œè·¡æ”¶é›†ç³»çµ±ç¤ºç¯„")
    print("=" * 60)

    # å‰µå»ºæ”¶é›†å™¨
    collector = TrajectoryCollector(
        storage_path="./demo_trajectories",
        auto_save=True
    )

    # æ¨¡æ“¬ç ”ç©¶ä»»å‹™
    query = "åˆ†æ 2024 å¹´å…¨çƒ AI æ™¶ç‰‡å¸‚å ´çš„ä¸»è¦ç«¶çˆ­æ ¼å±€"

    print(f"\nğŸ“ é–‹å§‹è¨˜éŒ„è»Œè·¡: {query}")

    # é–‹å§‹è»Œè·¡
    trajectory = collector.start_trajectory(
        query,
        source="demo",
        model="gpt-4o-mini"
    )

    # æ¨¡æ“¬ ReAct å¾ªç’°
    print("\nğŸ”„ æ¨¡æ“¬ ReAct å¾ªç’°...")

    # æ­¥é©Ÿ 1: æ€è€ƒ
    trajectory.add_thought(
        "é€™æ˜¯ä¸€å€‹é—œæ–¼ AI æ™¶ç‰‡å¸‚å ´çš„ç ”ç©¶å•é¡Œã€‚æˆ‘éœ€è¦ï¼š\n"
        "1. æœå°‹å¸‚å ´è¦æ¨¡æ•¸æ“š\n"
        "2. è­˜åˆ¥ä¸»è¦ç«¶çˆ­è€…\n"
        "3. åˆ†æå„å» å•†çš„å¸‚å ´ä»½é¡"
    )
    print("   ğŸ’­ æ·»åŠ æ€è€ƒæ­¥é©Ÿ")

    # æ­¥é©Ÿ 2: è¡Œå‹•
    tool_call = trajectory.add_action(
        "web_search",
        {"query": "2024 AI æ™¶ç‰‡å¸‚å ´è¦æ¨¡ NVIDIA AMD Intel", "num_results": 5}
    )
    print("   ğŸ”§ æ·»åŠ è¡Œå‹•æ­¥é©Ÿ: web_search")

    # æ­¥é©Ÿ 3: è§€å¯Ÿ
    trajectory.add_observation(
        {
            "results": [
                {"title": "2024 å…¨çƒ AI æ™¶ç‰‡å¸‚å ´å ±å‘Š", "snippet": "å¸‚å ´è¦æ¨¡é” 500 å„„ç¾å…ƒ..."},
                {"title": "NVIDIA å¸‚å ´ä»½é¡åˆ†æ", "snippet": "NVIDIA ä½”æ“š 80% ä»¥ä¸Šå¸‚å ´..."},
            ]
        },
        tool_call
    )
    print("   ğŸ‘ï¸ æ·»åŠ è§€å¯Ÿæ­¥é©Ÿ")

    # æ­¥é©Ÿ 4: é€²ä¸€æ­¥æ€è€ƒ
    trajectory.add_thought(
        "æœå°‹çµæœé¡¯ç¤º NVIDIA ä½”ä¸»å°åœ°ä½ã€‚éœ€è¦é€²ä¸€æ­¥äº†è§£ï¼š\n"
        "1. å…¶ä»–ç«¶çˆ­è€…çš„ç­–ç•¥\n"
        "2. æ–°èˆˆå» å•†çš„å´›èµ·"
    )
    print("   ğŸ’­ æ·»åŠ æ€è€ƒæ­¥é©Ÿ")

    # æ­¥é©Ÿ 5: å¦ä¸€å€‹è¡Œå‹•
    tool_call2 = trajectory.add_action(
        "web_browser",
        {"url": "https://example.com/ai-chip-report-2024", "extract_text": True}
    )
    print("   ğŸ”§ æ·»åŠ è¡Œå‹•æ­¥é©Ÿ: web_browser")

    # æ­¥é©Ÿ 6: è§€å¯Ÿ
    trajectory.add_observation(
        "æ ¹æ“šå ±å‘Šï¼Œ2024 å¹´ AI æ™¶ç‰‡å¸‚å ´å‘ˆç¾ä»¥ä¸‹æ ¼å±€ï¼š\n"
        "1. NVIDIA ä»¥ 80% å¸‚å ´ä»½é¡é ˜å…ˆ\n"
        "2. AMD ç©æ¥µè¿½è¶•ï¼Œä½” 10%\n"
        "3. Intel æ­£åœ¨è½‰å‹ï¼Œä½” 5%\n"
        "4. å…¶ä»–æ–°èˆˆå» å•†å…±ä½” 5%",
        tool_call2
    )
    print("   ğŸ‘ï¸ æ·»åŠ è§€å¯Ÿæ­¥é©Ÿ")

    # å®Œæˆè»Œè·¡
    final_answer = """
2024 å¹´å…¨çƒ AI æ™¶ç‰‡å¸‚å ´ç«¶çˆ­æ ¼å±€åˆ†æï¼š

## å¸‚å ´è¦æ¨¡
å…¨çƒ AI æ™¶ç‰‡å¸‚å ´è¦æ¨¡ç´„ 500 å„„ç¾å…ƒï¼Œå¹´å¢é•·ç‡è¶…é 30%ã€‚

## ä¸»è¦ç«¶çˆ­è€…
1. **NVIDIAï¼ˆç´„ 80%ï¼‰**ï¼šæ†‘è—‰ CUDA ç”Ÿæ…‹ç³»çµ±å’Œ GPU æ¶æ§‹å„ªå‹¢ï¼Œç‰¢ç‰¢ä½”æ“šå¸‚å ´ä¸»å°åœ°ä½ã€‚
2. **AMDï¼ˆç´„ 10%ï¼‰**ï¼šé€šé MI300 ç³»åˆ—ç©æ¥µè¿½è¶•ï¼Œåœ¨æ€§åƒ¹æ¯”æ–¹é¢æœ‰å„ªå‹¢ã€‚
3. **Intelï¼ˆç´„ 5%ï¼‰**ï¼šæ­£åœ¨å¾ CPU å‘ AI åŠ é€Ÿå™¨è½‰å‹ï¼Œæ¨å‡º Gaudi ç³»åˆ—ã€‚
4. **å…¶ä»–ï¼ˆç´„ 5%ï¼‰**ï¼šåŒ…æ‹¬è¯ç‚ºæ˜‡é¨°ã€Google TPU ç­‰ã€‚

## è¶¨å‹¢åˆ†æ
- é›²ç«¯è¨“ç·´å¸‚å ´ NVIDIA ä½”çµ•å°å„ªå‹¢
- é‚Šç·£æ¨ç†å¸‚å ´ç«¶çˆ­æ›´åŠ æ¿€çƒˆ
- é–‹æ”¾æ¨™æº–ï¼ˆå¦‚ OpenAI Tritonï¼‰å¯èƒ½æ”¹è®Šæ ¼å±€
"""

    rewards = collector.complete_trajectory(
        trajectory,
        final_answer=final_answer,
        success=True
    )

    print("\nâœ… è»Œè·¡å®Œæˆ")

    # é¡¯ç¤ºçå‹µ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ çå‹µä¿¡è™Ÿ")
    print("=" * 60)
    for key, value in rewards.items():
        bar = "â–ˆ" * int(value * 20)
        print(f"   {key:20s}: {value:.3f} {bar}")

    # çµ±è¨ˆ
    print("\n" + "=" * 60)
    print("ğŸ“Š æ”¶é›†å™¨çµ±è¨ˆ")
    print("=" * 60)
    stats = collector.get_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")

    # å›æ”¾
    print("\n")
    replayer = TrajectoryReplayer()
    replayer.replay(trajectory, delay=0.3)

    # åŒ¯å‡ºè¨“ç·´è³‡æ–™
    export_path = "./demo_trajectories/training_data.jsonl"
    count = collector.export_for_training(export_path, min_reward=0.3)
    print(f"\nğŸ“¤ å·²åŒ¯å‡º {count} æ¢è¨“ç·´è³‡æ–™åˆ° {export_path}")


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(
        description="è»Œè·¡æ”¶é›†å™¨ - ç¬¬ 5 ç« ç¯„ä¾‹"
    )
    parser.add_argument(
        "--demo",
        action="store_true",
        help="åŸ·è¡Œç¤ºç¯„æ¨¡å¼"
    )
    parser.add_argument(
        "--export",
        type=str,
        help="åŒ¯å‡ºè¨“ç·´è³‡æ–™åˆ°æŒ‡å®šè·¯å¾‘"
    )

    args = parser.parse_args()

    if args.export:
        collector = TrajectoryCollector()
        count = collector.export_for_training(args.export)
        print(f"å·²åŒ¯å‡º {count} æ¢è¨“ç·´è³‡æ–™")
    else:
        demo_trajectory_collection()


if __name__ == "__main__":
    main()
