# ç¬¬ 7 ç« ï¼šæœå°‹èˆ‡æª¢ç´¢å¼•æ“

> **æœ¬ç« ç›®æ¨™**ï¼šæŒæ¡æ·±åº¦ç ”ç©¶ä»£ç†äººçš„è³‡è¨Šç²å–èƒ½åŠ›ï¼Œå­¸æœƒå»ºæ§‹ç¶²é ç€è¦½ã€RAG æª¢ç´¢å’ŒçŸ¥è­˜åœ–è­œæ•´åˆçš„å®Œæ•´æœå°‹ç³»çµ±ã€‚

---

## 7.1 å•é¡Œï¼šä»£ç†äººå¦‚ä½•ã€Œçœ‹è¦‹ã€ä¸–ç•Œï¼Ÿ

æƒ³åƒä½ çµ¦ä»£ç†äººä¸€å€‹ç ”ç©¶ä»»å‹™ï¼šã€Œåˆ†æ 2024 å¹´ Q3 å…¨çƒåŠå°é«”ç”¢æ¥­çš„æœ€æ–°å‹•æ…‹ã€ã€‚

ä»£ç†äººéœ€è¦ï¼š
1. åœ¨ç¶²è·¯ä¸Šæœå°‹æœ€æ–°è³‡è¨Š
2. ç€è¦½ç›¸é—œç¶²é ä¸¦æå–å…§å®¹
3. å¾æµ·é‡è³‡è¨Šä¸­æ‰¾åˆ°é—œéµè³‡æ–™
4. æ•´åˆå¤šå€‹ä¾†æºå½¢æˆå®Œæ•´å ±å‘Š

é€™äº›èƒ½åŠ›çš„èƒŒå¾Œï¼Œæ˜¯ä¸€å¥—ç²¾å¯†çš„**æœå°‹èˆ‡æª¢ç´¢å¼•æ“**ã€‚

### MiroThinker çš„è³‡è¨Šç²å–æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è³‡è¨Šç²å–æ¶æ§‹                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  æœå°‹å±¤ï¼ˆDiscoveryï¼‰                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚ ç¶²é æœå°‹ â”‚  â”‚ å­¸è¡“æœå°‹ â”‚  â”‚ æ–°èæœå°‹ â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  æ“·å–å±¤ï¼ˆExtractionï¼‰                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚ ç¶²é ç€è¦½ â”‚  â”‚ PDF è§£æ â”‚  â”‚ çµæ§‹æå– â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  æª¢ç´¢å±¤ï¼ˆRetrievalï¼‰                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚ RAG æª¢ç´¢ â”‚  â”‚ å‘é‡æœå°‹ â”‚  â”‚ çŸ¥è­˜åœ–è­œ â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

æœ¬ç« å°‡å¸¶ä½ å¯¦ç¾é€™å¥—å®Œæ•´çš„æœå°‹èˆ‡æª¢ç´¢ç³»çµ±ã€‚

---

## 7.2 ç¶²é æœå°‹å¼•æ“æ•´åˆ

### 7.2.1 æœå°‹ API å°è£

é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹çµ±ä¸€çš„æœå°‹ä»‹é¢ï¼Œæ”¯æ´å¤šç¨®æœå°‹å¼•æ“ï¼š

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional
import aiohttp
import asyncio


class SearchEngine(Enum):
    """æ”¯æ´çš„æœå°‹å¼•æ“"""
    SERPER = "serper"          # Google æœå°‹ API
    TAVILY = "tavily"          # AI åŸç”Ÿæœå°‹
    BING = "bing"              # Bing æœå°‹
    DUCKDUCKGO = "duckduckgo"  # DuckDuckGoï¼ˆå…è²»ï¼‰


@dataclass
class SearchResult:
    """
    æœå°‹çµæœ

    â€¹1â€º çµ±ä¸€çš„çµæœæ ¼å¼
    â€¹2â€º åŒ…å«ä¾†æºè©•ä¼°è³‡è¨Š
    """
    title: str
    url: str
    snippet: str
    source: str
    published_date: Optional[datetime] = None
    relevance_score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict:
        return {
            "title": self.title,
            "url": self.url,
            "snippet": self.snippet,
            "source": self.source,
            "published_date": self.published_date.isoformat() if self.published_date else None,
            "relevance_score": self.relevance_score,
            "metadata": self.metadata
        }


class BaseSearchProvider(ABC):
    """æœå°‹æä¾›è€…åŸºé¡"""

    @abstractmethod
    async def search(
        self,
        query: str,
        num_results: int = 10,
        **kwargs
    ) -> List[SearchResult]:
        """åŸ·è¡Œæœå°‹"""
        pass


class SerperSearchProvider(BaseSearchProvider):
    """
    Serper API æœå°‹æä¾›è€…

    â€¹1â€º ä½¿ç”¨ Google Search API
    â€¹2â€º æ”¯æ´å¤šç¨®æœå°‹é¡å‹
    """

    BASE_URL = "https://google.serper.dev/search"

    def __init__(self, api_key: str):
        self.api_key = api_key

    async def search(
        self,
        query: str,
        num_results: int = 10,
        search_type: str = "search",
        country: str = "tw",
        language: str = "zh-TW"
    ) -> List[SearchResult]:
        """åŸ·è¡Œ Google æœå°‹"""
        async with aiohttp.ClientSession() as session:
            headers = {
                "X-API-KEY": self.api_key,
                "Content-Type": "application/json"
            }

            payload = {
                "q": query,
                "num": num_results,
                "gl": country,
                "hl": language
            }

            async with session.post(
                self.BASE_URL,
                headers=headers,
                json=payload
            ) as response:
                if response.status != 200:
                    raise Exception(f"æœå°‹å¤±æ•—: {response.status}")

                data = await response.json()
                return self._parse_results(data)

    def _parse_results(self, data: dict) -> List[SearchResult]:
        """è§£ææœå°‹çµæœ"""
        results = []

        for item in data.get("organic", []):
            result = SearchResult(
                title=item.get("title", ""),
                url=item.get("link", ""),
                snippet=item.get("snippet", ""),
                source="google",
                metadata={
                    "position": item.get("position"),
                    "sitelinks": item.get("sitelinks", [])
                }
            )
            results.append(result)

        return results


class TavilySearchProvider(BaseSearchProvider):
    """
    Tavily AI æœå°‹æä¾›è€…

    â€¹1â€º AI å„ªåŒ–çš„æœå°‹çµæœ
    â€¹2â€º è‡ªå‹•æå–é—œéµå…§å®¹
    """

    BASE_URL = "https://api.tavily.com/search"

    def __init__(self, api_key: str):
        self.api_key = api_key

    async def search(
        self,
        query: str,
        num_results: int = 10,
        search_depth: str = "advanced",
        include_answer: bool = True
    ) -> List[SearchResult]:
        """åŸ·è¡Œ Tavily æœå°‹"""
        async with aiohttp.ClientSession() as session:
            payload = {
                "api_key": self.api_key,
                "query": query,
                "max_results": num_results,
                "search_depth": search_depth,
                "include_answer": include_answer
            }

            async with session.post(self.BASE_URL, json=payload) as response:
                if response.status != 200:
                    raise Exception(f"æœå°‹å¤±æ•—: {response.status}")

                data = await response.json()
                return self._parse_results(data)

    def _parse_results(self, data: dict) -> List[SearchResult]:
        """è§£ææœå°‹çµæœ"""
        results = []

        for item in data.get("results", []):
            result = SearchResult(
                title=item.get("title", ""),
                url=item.get("url", ""),
                snippet=item.get("content", ""),
                source="tavily",
                relevance_score=item.get("score", 0.0),
                metadata={
                    "raw_content": item.get("raw_content"),
                    "answer": data.get("answer")
                }
            )
            results.append(result)

        return results


class DuckDuckGoSearchProvider(BaseSearchProvider):
    """
    DuckDuckGo æœå°‹æä¾›è€…ï¼ˆå…è²»ï¼‰

    â€¹1â€º ä¸éœ€è¦ API Key
    â€¹2â€º ä½¿ç”¨ HTML è§£æ
    """

    async def search(
        self,
        query: str,
        num_results: int = 10,
        region: str = "tw-tzh"
    ) -> List[SearchResult]:
        """åŸ·è¡Œ DuckDuckGo æœå°‹"""
        # ä½¿ç”¨ duckduckgo-search å¥—ä»¶
        try:
            from duckduckgo_search import AsyncDDGS
        except ImportError:
            raise ImportError("è«‹å®‰è£ duckduckgo-search: pip install duckduckgo-search")

        results = []
        async with AsyncDDGS() as ddgs:
            async for r in ddgs.text(
                query,
                region=region,
                max_results=num_results
            ):
                result = SearchResult(
                    title=r.get("title", ""),
                    url=r.get("href", ""),
                    snippet=r.get("body", ""),
                    source="duckduckgo"
                )
                results.append(result)

        return results
```

### 7.2.2 çµ±ä¸€æœå°‹ç®¡ç†å™¨

```python
class SearchManager:
    """
    æœå°‹ç®¡ç†å™¨

    â€¹1â€º çµ±ä¸€ç®¡ç†å¤šå€‹æœå°‹æä¾›è€…
    â€¹2â€º æ”¯æ´çµæœèšåˆå’Œå»é‡
    â€¹3â€º æä¾›å®¹éŒ¯æ©Ÿåˆ¶
    """

    def __init__(self):
        self._providers: Dict[str, BaseSearchProvider] = {}
        self._default_provider: Optional[str] = None

    def register_provider(
        self,
        name: str,
        provider: BaseSearchProvider,
        set_default: bool = False
    ) -> None:
        """è¨»å†Šæœå°‹æä¾›è€…"""
        self._providers[name] = provider
        if set_default or self._default_provider is None:
            self._default_provider = name

    async def search(
        self,
        query: str,
        num_results: int = 10,
        provider: Optional[str] = None,
        **kwargs
    ) -> List[SearchResult]:
        """
        åŸ·è¡Œæœå°‹

        â€¹1â€º ä½¿ç”¨æŒ‡å®šæˆ–é è¨­æä¾›è€…
        â€¹2â€º å¤±æ•—æ™‚è‡ªå‹•å›é€€
        """
        provider_name = provider or self._default_provider
        if not provider_name:
            raise ValueError("æ²’æœ‰å¯ç”¨çš„æœå°‹æä¾›è€…")

        try:
            provider_instance = self._providers[provider_name]
            return await provider_instance.search(query, num_results, **kwargs)
        except Exception as e:
            # å˜—è©¦å›é€€åˆ°å…¶ä»–æä¾›è€…
            for name, p in self._providers.items():
                if name != provider_name:
                    try:
                        return await p.search(query, num_results, **kwargs)
                    except:
                        continue
            raise e

    async def multi_search(
        self,
        query: str,
        num_results: int = 10,
        providers: Optional[List[str]] = None,
        deduplicate: bool = True
    ) -> List[SearchResult]:
        """
        å¤šå¼•æ“æœå°‹

        â€¹1â€º åŒæ™‚ä½¿ç”¨å¤šå€‹å¼•æ“
        â€¹2â€º åˆä½µä¸¦å»é‡çµæœ
        """
        providers = providers or list(self._providers.keys())

        tasks = [
            self.search(query, num_results, provider=p)
            for p in providers
        ]

        all_results = []
        results_list = await asyncio.gather(*tasks, return_exceptions=True)

        for results in results_list:
            if isinstance(results, Exception):
                continue
            all_results.extend(results)

        if deduplicate:
            all_results = self._deduplicate(all_results)

        return all_results

    def _deduplicate(self, results: List[SearchResult]) -> List[SearchResult]:
        """å»é‡çµæœ"""
        seen_urls = set()
        unique_results = []

        for result in results:
            # æ­£è¦åŒ– URL
            url = result.url.rstrip("/").lower()
            if url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)

        return unique_results
```

---

## 7.3 ç¶²é ç€è¦½èˆ‡å…§å®¹æ“·å–

æœå°‹åªèƒ½å¾—åˆ°æ‘˜è¦ï¼Œè¦æ·±å…¥äº†è§£å…§å®¹ï¼Œéœ€è¦ç€è¦½ç¶²é ä¸¦æ“·å–å®Œæ•´å…§å®¹ã€‚

### 7.3.1 ç¶²é ç€è¦½å™¨å¯¦ç¾

```python
import re
from typing import Tuple
from urllib.parse import urljoin, urlparse


@dataclass
class WebPage:
    """
    ç¶²é å…§å®¹

    â€¹1â€º åŒ…å«åŸå§‹ HTML å’Œæå–å¾Œçš„ç´”æ–‡å­—
    â€¹2â€º è¨˜éŒ„æå–çš„çµæ§‹åŒ–è³‡è¨Š
    """
    url: str
    title: str
    content: str
    html: Optional[str] = None
    links: List[str] = field(default_factory=list)
    images: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    fetched_at: datetime = field(default_factory=datetime.now)
    status_code: int = 200

    @property
    def word_count(self) -> int:
        return len(self.content.split())

    @property
    def char_count(self) -> int:
        return len(self.content)

    def to_dict(self) -> dict:
        return {
            "url": self.url,
            "title": self.title,
            "content": self.content[:1000] + "..." if len(self.content) > 1000 else self.content,
            "word_count": self.word_count,
            "link_count": len(self.links),
            "fetched_at": self.fetched_at.isoformat(),
            "status_code": self.status_code
        }


class WebBrowser:
    """
    ç¶²é ç€è¦½å™¨

    â€¹1â€º ç²å–ç¶²é å…§å®¹
    â€¹2â€º æå–ç´”æ–‡å­—
    â€¹3â€º è™•ç†å„ç¨®æ ¼å¼ï¼ˆHTML, PDF ç­‰ï¼‰
    """

    def __init__(
        self,
        timeout: float = 30.0,
        max_content_length: int = 100000,
        user_agent: str = "MiroThinker/1.0 Research Agent"
    ):
        self.timeout = timeout
        self.max_content_length = max_content_length
        self.user_agent = user_agent

    async def browse(self, url: str) -> WebPage:
        """
        ç€è¦½ç¶²é 

        â€¹1â€º ç²å–å…§å®¹
        â€¹2â€º æå–ç´”æ–‡å­—
        â€¹3â€º è§£æé€£çµå’Œåœ–ç‰‡
        """
        async with aiohttp.ClientSession() as session:
            headers = {"User-Agent": self.user_agent}

            try:
                async with session.get(
                    url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                ) as response:
                    status_code = response.status

                    if status_code != 200:
                        return WebPage(
                            url=url,
                            title="",
                            content=f"ç„¡æ³•ç²å–ç¶²é ï¼šHTTP {status_code}",
                            status_code=status_code
                        )

                    content_type = response.headers.get("Content-Type", "")

                    # æ ¹æ“šå…§å®¹é¡å‹è™•ç†
                    if "application/pdf" in content_type:
                        content = await response.read()
                        return await self._process_pdf(url, content)

                    html = await response.text()

                    if len(html) > self.max_content_length:
                        html = html[:self.max_content_length]

                    return self._process_html(url, html)

            except asyncio.TimeoutError:
                return WebPage(
                    url=url,
                    title="",
                    content="ç¶²é è¼‰å…¥è¶…æ™‚",
                    status_code=408
                )
            except Exception as e:
                return WebPage(
                    url=url,
                    title="",
                    content=f"ç²å–å¤±æ•—ï¼š{str(e)}",
                    status_code=500
                )

    def _process_html(self, url: str, html: str) -> WebPage:
        """è™•ç† HTML å…§å®¹"""
        # æå–æ¨™é¡Œ
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        title = title_match.group(1).strip() if title_match else ""

        # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
        html_clean = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html_clean = re.sub(r'<style[^>]*>.*?</style>', '', html_clean, flags=re.DOTALL | re.IGNORECASE)

        # æå–ç´”æ–‡å­—
        text = re.sub(r'<[^>]+>', ' ', html_clean)
        text = re.sub(r'\s+', ' ', text).strip()

        # æå–é€£çµ
        links = []
        for match in re.finditer(r'href=["\']([^"\']+)["\']', html, re.IGNORECASE):
            link = match.group(1)
            if link.startswith("http"):
                links.append(link)
            elif link.startswith("/"):
                links.append(urljoin(url, link))

        # æå–åœ–ç‰‡
        images = []
        for match in re.finditer(r'<img[^>]+src=["\']([^"\']+)["\']', html, re.IGNORECASE):
            img = match.group(1)
            if img.startswith("http"):
                images.append(img)
            elif img.startswith("/"):
                images.append(urljoin(url, img))

        return WebPage(
            url=url,
            title=title,
            content=text,
            html=html,
            links=links[:50],  # é™åˆ¶é€£çµæ•¸é‡
            images=images[:20],
            metadata={
                "content_type": "text/html"
            }
        )

    async def _process_pdf(self, url: str, content: bytes) -> WebPage:
        """è™•ç† PDF å…§å®¹"""
        try:
            import fitz  # PyMuPDF
        except ImportError:
            return WebPage(
                url=url,
                title="PDF æ–‡ä»¶",
                content="éœ€è¦å®‰è£ PyMuPDF ä¾†è§£æ PDFï¼špip install pymupdf",
                metadata={"content_type": "application/pdf"}
            )

        try:
            doc = fitz.open(stream=content, filetype="pdf")
            text_parts = []
            for page in doc:
                text_parts.append(page.get_text())

            text = "\n".join(text_parts)

            return WebPage(
                url=url,
                title=doc.metadata.get("title", "PDF æ–‡ä»¶"),
                content=text,
                metadata={
                    "content_type": "application/pdf",
                    "page_count": len(doc),
                    "author": doc.metadata.get("author")
                }
            )
        except Exception as e:
            return WebPage(
                url=url,
                title="PDF è§£æå¤±æ•—",
                content=f"ç„¡æ³•è§£æ PDFï¼š{str(e)}",
                metadata={"content_type": "application/pdf"}
            )

    async def batch_browse(
        self,
        urls: List[str],
        concurrency: int = 5
    ) -> List[WebPage]:
        """æ‰¹æ¬¡ç€è¦½å¤šå€‹ç¶²é """
        semaphore = asyncio.Semaphore(concurrency)

        async def browse_one(url: str) -> WebPage:
            async with semaphore:
                return await self.browse(url)

        tasks = [browse_one(url) for url in urls]
        return await asyncio.gather(*tasks)
```

### 7.3.2 æ™ºèƒ½å…§å®¹æå–

```python
class ContentExtractor:
    """
    æ™ºèƒ½å…§å®¹æå–å™¨

    â€¹1â€º è­˜åˆ¥ä¸»è¦å…§å®¹å€åŸŸ
    â€¹2â€º éæ¿¾å»£å‘Šå’Œå°èˆª
    â€¹3â€º ä¿ç•™çµæ§‹åŒ–è³‡è¨Š
    """

    # å¸¸è¦‹çš„éå…§å®¹å…ƒç´ 
    NOISE_PATTERNS = [
        r'class="[^"]*(?:nav|menu|sidebar|footer|header|advertisement|ad-|banner)[^"]*"',
        r'id="[^"]*(?:nav|menu|sidebar|footer|header|advertisement|ad-|banner)[^"]*"',
    ]

    # ä¸»è¦å…§å®¹å€åŸŸè­˜åˆ¥
    CONTENT_PATTERNS = [
        r'class="[^"]*(?:article|content|main|post|entry|story)[^"]*"',
        r'<article[^>]*>',
        r'<main[^>]*>',
    ]

    def __init__(self, min_content_length: int = 100):
        self.min_content_length = min_content_length

    def extract(self, html: str) -> str:
        """æå–ä¸»è¦å…§å®¹"""
        # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
        html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)

        # å˜—è©¦è­˜åˆ¥ä¸»è¦å…§å®¹å€åŸŸ
        main_content = self._find_main_content(html)

        if main_content and len(main_content) > self.min_content_length:
            return self._clean_text(main_content)

        # å›é€€åˆ°å…¨æ–‡æå–
        return self._clean_text(html)

    def _find_main_content(self, html: str) -> Optional[str]:
        """è­˜åˆ¥ä¸»è¦å…§å®¹å€åŸŸ"""
        for pattern in self.CONTENT_PATTERNS:
            match = re.search(pattern, html, re.IGNORECASE)
            if match:
                start = match.start()
                # æ‰¾åˆ°å°æ‡‰çš„çµæŸæ¨™ç±¤
                tag_match = re.match(r'<(\w+)', html[start:])
                if tag_match:
                    tag_name = tag_match.group(1)
                    # ç°¡åŒ–çš„æ¨™ç±¤åŒ¹é…
                    end_pattern = f'</{tag_name}>'
                    end_match = re.search(end_pattern, html[start:], re.IGNORECASE)
                    if end_match:
                        return html[start:start + end_match.end()]

        return None

    def _clean_text(self, html: str) -> str:
        """æ¸…ç†ä¸¦æå–ç´”æ–‡å­—"""
        # ç§»é™¤ HTML æ¨™ç±¤
        text = re.sub(r'<[^>]+>', ' ', html)
        # è™•ç† HTML å¯¦é«”
        text = re.sub(r'&nbsp;', ' ', text)
        text = re.sub(r'&amp;', '&', text)
        text = re.sub(r'&lt;', '<', text)
        text = re.sub(r'&gt;', '>', text)
        text = re.sub(r'&quot;', '"', text)
        # æ­£è¦åŒ–ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def extract_structured(self, html: str) -> Dict[str, Any]:
        """æå–çµæ§‹åŒ–è³‡è¨Š"""
        result = {
            "title": "",
            "headings": [],
            "paragraphs": [],
            "lists": [],
            "tables": []
        }

        # æå–æ¨™é¡Œ
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        if title_match:
            result["title"] = self._clean_text(title_match.group(1))

        # æå–æ¨™é¡Œå±¤ç´š
        for level in range(1, 7):
            for match in re.finditer(rf'<h{level}[^>]*>(.*?)</h{level}>', html, re.IGNORECASE | re.DOTALL):
                result["headings"].append({
                    "level": level,
                    "text": self._clean_text(match.group(1))
                })

        # æå–æ®µè½
        for match in re.finditer(r'<p[^>]*>(.*?)</p>', html, re.IGNORECASE | re.DOTALL):
            text = self._clean_text(match.group(1))
            if len(text) > 20:  # éæ¿¾å¤ªçŸ­çš„æ®µè½
                result["paragraphs"].append(text)

        return result
```

---

## 7.4 RAG æª¢ç´¢ç³»çµ±

RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯æ·±åº¦ç ”ç©¶ä»£ç†äººçš„æ ¸å¿ƒèƒ½åŠ›â€”â€”å¾å¤§é‡æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚

### 7.4.1 æ–‡ä»¶åˆ†å¡Šå™¨

```python
@dataclass
class DocumentChunk:
    """
    æ–‡ä»¶ç‰‡æ®µ

    â€¹1â€º åŒ…å«åŸå§‹å…§å®¹å’Œä¾†æºè³‡è¨Š
    â€¹2â€º æ”¯æ´å‘é‡åµŒå…¥
    """
    content: str
    source_url: str
    chunk_index: int
    total_chunks: int
    embedding: Optional[List[float]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def id(self) -> str:
        url_hash = hashlib.md5(self.source_url.encode()).hexdigest()[:8]
        return f"chunk_{url_hash}_{self.chunk_index}"


class DocumentChunker:
    """
    æ–‡ä»¶åˆ†å¡Šå™¨

    â€¹1â€º æ”¯æ´å¤šç¨®åˆ†å¡Šç­–ç•¥
    â€¹2â€º ä¿æŒèªç¾©å®Œæ•´æ€§
    â€¹3â€º è™•ç†é‡ç–Šä»¥é¿å…è³‡è¨Šä¸Ÿå¤±
    """

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: Optional[List[str]] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", "ã€‚", ".", " "]

    def chunk(
        self,
        text: str,
        source_url: str = "",
        **metadata
    ) -> List[DocumentChunk]:
        """
        å°‡æ–‡ä»¶åˆ†å‰²æˆç‰‡æ®µ

        â€¹1â€º å˜—è©¦åœ¨è‡ªç„¶é‚Šç•Œåˆ†å‰²
        â€¹2â€º ä¿æŒç‰‡æ®µå¤§å°ä¸€è‡´
        â€¹3â€º æ·»åŠ é‡ç–Šä»¥ä¿æŒä¸Šä¸‹æ–‡
        """
        if not text.strip():
            return []

        chunks = []
        current_pos = 0
        chunk_index = 0

        while current_pos < len(text):
            # è¨ˆç®—ç‰‡æ®µçµæŸä½ç½®
            end_pos = current_pos + self.chunk_size

            if end_pos >= len(text):
                # æœ€å¾Œä¸€å€‹ç‰‡æ®µ
                chunk_text = text[current_pos:].strip()
                if chunk_text:
                    chunks.append(DocumentChunk(
                        content=chunk_text,
                        source_url=source_url,
                        chunk_index=chunk_index,
                        total_chunks=0,  # ç¨å¾Œæ›´æ–°
                        metadata=metadata
                    ))
                break

            # å°‹æ‰¾æœ€ä½³åˆ†å‰²é»
            best_split = end_pos
            for separator in self.separators:
                # åœ¨ç›®æ¨™ä½ç½®é™„è¿‘å°‹æ‰¾åˆ†éš”ç¬¦
                search_start = max(current_pos + self.chunk_size // 2, current_pos)
                sep_pos = text.rfind(separator, search_start, end_pos + 50)
                if sep_pos > current_pos:
                    best_split = sep_pos + len(separator)
                    break

            chunk_text = text[current_pos:best_split].strip()
            if chunk_text:
                chunks.append(DocumentChunk(
                    content=chunk_text,
                    source_url=source_url,
                    chunk_index=chunk_index,
                    total_chunks=0,
                    metadata=metadata
                ))
                chunk_index += 1

            # ç§»å‹•åˆ°ä¸‹ä¸€å€‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
            current_pos = best_split - self.chunk_overlap

        # æ›´æ–°ç¸½ç‰‡æ®µæ•¸
        for chunk in chunks:
            chunk.total_chunks = len(chunks)

        return chunks


class SemanticChunker:
    """
    èªç¾©åˆ†å¡Šå™¨

    â€¹1â€º åŸºæ–¼èªç¾©ç›¸ä¼¼åº¦åˆ†å¡Š
    â€¹2â€º ä¿æŒèªç¾©å®Œæ•´çš„æ®µè½
    """

    def __init__(
        self,
        embedder,
        similarity_threshold: float = 0.5,
        max_chunk_size: int = 1000
    ):
        self.embedder = embedder
        self.similarity_threshold = similarity_threshold
        self.max_chunk_size = max_chunk_size

    async def chunk(
        self,
        text: str,
        source_url: str = ""
    ) -> List[DocumentChunk]:
        """èªç¾©åˆ†å¡Š"""
        # å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

        if not paragraphs:
            return []

        # è¨ˆç®—æ®µè½åµŒå…¥
        embeddings = await self._batch_embed(paragraphs)

        # åŸºæ–¼ç›¸ä¼¼åº¦åˆä½µç›¸é„°æ®µè½
        chunks = []
        current_chunk = paragraphs[0]
        current_embedding = embeddings[0]

        for i in range(1, len(paragraphs)):
            # è¨ˆç®—èˆ‡ç•¶å‰ç‰‡æ®µçš„ç›¸ä¼¼åº¦
            similarity = self._cosine_similarity(current_embedding, embeddings[i])

            if similarity > self.similarity_threshold and len(current_chunk) + len(paragraphs[i]) < self.max_chunk_size:
                # åˆä½µ
                current_chunk += "\n\n" + paragraphs[i]
                # æ›´æ–°åµŒå…¥ï¼ˆç°¡å–®å¹³å‡ï¼‰
                current_embedding = [
                    (a + b) / 2
                    for a, b in zip(current_embedding, embeddings[i])
                ]
            else:
                # é–‹å§‹æ–°ç‰‡æ®µ
                chunks.append(current_chunk)
                current_chunk = paragraphs[i]
                current_embedding = embeddings[i]

        chunks.append(current_chunk)

        return [
            DocumentChunk(
                content=chunk,
                source_url=source_url,
                chunk_index=i,
                total_chunks=len(chunks)
            )
            for i, chunk in enumerate(chunks)
        ]

    async def _batch_embed(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹æ¬¡è¨ˆç®—åµŒå…¥"""
        return [await self.embedder.embed(t) for t in texts]

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        import numpy as np
        a_arr = np.array(a)
        b_arr = np.array(b)
        return float(np.dot(a_arr, b_arr) / (np.linalg.norm(a_arr) * np.linalg.norm(b_arr) + 1e-8))
```

### 7.4.2 å‘é‡ç´¢å¼•

```python
import numpy as np


class VectorIndex:
    """
    å‘é‡ç´¢å¼•

    â€¹1â€º é«˜æ•ˆçš„ç›¸ä¼¼åº¦æœå°‹
    â€¹2â€º æ”¯æ´å¢é‡æ›´æ–°
    â€¹3â€º å¯æŒä¹…åŒ–
    """

    def __init__(self, dimension: int = 1536):
        self.dimension = dimension
        self._chunks: List[DocumentChunk] = []
        self._embeddings: Optional[np.ndarray] = None

    def add(self, chunk: DocumentChunk) -> None:
        """æ·»åŠ ç‰‡æ®µ"""
        if chunk.embedding is None:
            raise ValueError("ç‰‡æ®µå¿…é ˆåŒ…å«åµŒå…¥å‘é‡")

        self._chunks.append(chunk)

        embedding = np.array(chunk.embedding).reshape(1, -1)
        if self._embeddings is None:
            self._embeddings = embedding
        else:
            self._embeddings = np.vstack([self._embeddings, embedding])

    def add_batch(self, chunks: List[DocumentChunk]) -> None:
        """æ‰¹æ¬¡æ·»åŠ ç‰‡æ®µ"""
        for chunk in chunks:
            self.add(chunk)

    def search(
        self,
        query_embedding: List[float],
        top_k: int = 5,
        min_score: float = 0.0
    ) -> List[Tuple[DocumentChunk, float]]:
        """
        æœå°‹æœ€ç›¸é—œçš„ç‰‡æ®µ

        â€¹1â€º ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦
        â€¹2â€º è¿”å›åˆ†æ•¸å’Œç‰‡æ®µ
        """
        if self._embeddings is None or len(self._chunks) == 0:
            return []

        query = np.array(query_embedding)

        # è¨ˆç®—ç›¸ä¼¼åº¦
        norms = np.linalg.norm(self._embeddings, axis=1)
        query_norm = np.linalg.norm(query)

        if query_norm == 0:
            return []

        similarities = np.dot(self._embeddings, query) / (norms * query_norm + 1e-8)

        # ç²å– top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            score = float(similarities[idx])
            if score >= min_score:
                results.append((self._chunks[idx], score))

        return results

    @property
    def size(self) -> int:
        return len(self._chunks)

    def clear(self) -> None:
        """æ¸…ç©ºç´¢å¼•"""
        self._chunks = []
        self._embeddings = None


class RAGRetriever:
    """
    RAG æª¢ç´¢å™¨

    â€¹1â€º æ•´åˆåˆ†å¡Šã€ç´¢å¼•å’Œæª¢ç´¢
    â€¹2â€º æ”¯æ´å¤šç¨®æª¢ç´¢ç­–ç•¥
    â€¹3â€º æä¾›ä¸Šä¸‹æ–‡å¢å¼·
    """

    def __init__(
        self,
        embedder,
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        self.embedder = embedder
        self.chunker = DocumentChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.index = VectorIndex()

    async def add_document(
        self,
        content: str,
        source_url: str = "",
        **metadata
    ) -> int:
        """
        æ·»åŠ æ–‡ä»¶åˆ°ç´¢å¼•

        â€¹1â€º åˆ†å¡Š
        â€¹2â€º è¨ˆç®—åµŒå…¥
        â€¹3â€º åŠ å…¥ç´¢å¼•
        """
        chunks = self.chunker.chunk(content, source_url, **metadata)

        for chunk in chunks:
            embedding = await self.embedder.embed(chunk.content)
            chunk.embedding = embedding
            self.index.add(chunk)

        return len(chunks)

    async def retrieve(
        self,
        query: str,
        top_k: int = 5,
        min_score: float = 0.3
    ) -> List[Tuple[DocumentChunk, float]]:
        """æª¢ç´¢ç›¸é—œç‰‡æ®µ"""
        query_embedding = await self.embedder.embed(query)
        return self.index.search(query_embedding, top_k, min_score)

    async def retrieve_with_context(
        self,
        query: str,
        top_k: int = 5,
        context_window: int = 1
    ) -> str:
        """
        æª¢ç´¢ä¸¦ç”Ÿæˆä¸Šä¸‹æ–‡

        â€¹1â€º æª¢ç´¢ç›¸é—œç‰‡æ®µ
        â€¹2â€º åŒ…å«ç›¸é„°ç‰‡æ®µ
        â€¹3â€º æ ¼å¼åŒ–ç‚º prompt
        """
        results = await self.retrieve(query, top_k)

        if not results:
            return "æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚"

        context_parts = []
        for chunk, score in results:
            context_parts.append(f"[ä¾†æº: {chunk.source_url}]\n[ç›¸é—œåº¦: {score:.2f}]\n{chunk.content}")

        return "\n\n---\n\n".join(context_parts)
```

---

## 7.5 çŸ¥è­˜åœ–è­œæ•´åˆ

çŸ¥è­˜åœ–è­œå¯ä»¥æ•æ‰å¯¦é«”ä¹‹é–“çš„é—œä¿‚ï¼Œæä¾›çµæ§‹åŒ–çš„çŸ¥è­˜è¡¨ç¤ºã€‚

### 7.5.1 çŸ¥è­˜åœ–è­œå»ºæ§‹

```python
@dataclass
class Entity:
    """å¯¦é«”"""
    name: str
    entity_type: str
    properties: Dict[str, Any] = field(default_factory=dict)

    def __hash__(self):
        return hash((self.name, self.entity_type))

    def __eq__(self, other):
        return self.name == other.name and self.entity_type == other.entity_type


@dataclass
class Relation:
    """é—œä¿‚"""
    source: Entity
    target: Entity
    relation_type: str
    properties: Dict[str, Any] = field(default_factory=dict)


class KnowledgeGraph:
    """
    çŸ¥è­˜åœ–è­œ

    â€¹1â€º å­˜å„²å¯¦é«”å’Œé—œä¿‚
    â€¹2â€º æ”¯æ´è·¯å¾‘æŸ¥è©¢
    â€¹3â€º æä¾›çŸ¥è­˜æ¨ç†
    """

    def __init__(self):
        self._entities: Dict[str, Entity] = {}
        self._relations: List[Relation] = []
        self._adjacency: Dict[str, List[Tuple[str, Relation]]] = {}

    def add_entity(self, entity: Entity) -> None:
        """æ·»åŠ å¯¦é«”"""
        key = f"{entity.entity_type}:{entity.name}"
        self._entities[key] = entity
        if key not in self._adjacency:
            self._adjacency[key] = []

    def add_relation(self, relation: Relation) -> None:
        """æ·»åŠ é—œä¿‚"""
        # ç¢ºä¿å¯¦é«”å­˜åœ¨
        source_key = f"{relation.source.entity_type}:{relation.source.name}"
        target_key = f"{relation.target.entity_type}:{relation.target.name}"

        if source_key not in self._entities:
            self.add_entity(relation.source)
        if target_key not in self._entities:
            self.add_entity(relation.target)

        self._relations.append(relation)
        self._adjacency[source_key].append((target_key, relation))

    def get_entity(self, name: str, entity_type: str) -> Optional[Entity]:
        """ç²å–å¯¦é«”"""
        key = f"{entity_type}:{name}"
        return self._entities.get(key)

    def get_relations(
        self,
        source: Optional[Entity] = None,
        target: Optional[Entity] = None,
        relation_type: Optional[str] = None
    ) -> List[Relation]:
        """æŸ¥è©¢é—œä¿‚"""
        results = []
        for relation in self._relations:
            if source and relation.source != source:
                continue
            if target and relation.target != target:
                continue
            if relation_type and relation.relation_type != relation_type:
                continue
            results.append(relation)
        return results

    def find_path(
        self,
        source: Entity,
        target: Entity,
        max_depth: int = 3
    ) -> Optional[List[Relation]]:
        """
        æŸ¥æ‰¾å…©å€‹å¯¦é«”ä¹‹é–“çš„è·¯å¾‘

        â€¹1â€º ä½¿ç”¨ BFS æœå°‹
        â€¹2â€º è¿”å›æœ€çŸ­è·¯å¾‘
        """
        from collections import deque

        source_key = f"{source.entity_type}:{source.name}"
        target_key = f"{target.entity_type}:{target.name}"

        if source_key not in self._entities or target_key not in self._entities:
            return None

        # BFS
        queue = deque([(source_key, [])])
        visited = {source_key}

        while queue:
            current, path = queue.popleft()

            if current == target_key:
                return path

            if len(path) >= max_depth:
                continue

            for next_key, relation in self._adjacency.get(current, []):
                if next_key not in visited:
                    visited.add(next_key)
                    queue.append((next_key, path + [relation]))

        return None

    def get_neighbors(
        self,
        entity: Entity,
        relation_type: Optional[str] = None
    ) -> List[Tuple[Entity, Relation]]:
        """ç²å–ç›¸é„°å¯¦é«”"""
        key = f"{entity.entity_type}:{entity.name}"
        neighbors = []

        for next_key, relation in self._adjacency.get(key, []):
            if relation_type and relation.relation_type != relation_type:
                continue
            neighbors.append((self._entities[next_key], relation))

        return neighbors

    def to_prompt(self, max_triples: int = 20) -> str:
        """è½‰æ›ç‚º prompt æ ¼å¼"""
        lines = ["[çŸ¥è­˜åœ–è­œ]"]

        for i, relation in enumerate(self._relations[:max_triples]):
            line = f"({relation.source.name}) --[{relation.relation_type}]--> ({relation.target.name})"
            lines.append(line)

        if len(self._relations) > max_triples:
            lines.append(f"... é‚„æœ‰ {len(self._relations) - max_triples} å€‹é—œä¿‚")

        return "\n".join(lines)

    @property
    def entity_count(self) -> int:
        return len(self._entities)

    @property
    def relation_count(self) -> int:
        return len(self._relations)
```

### 7.5.2 å¯¦é«”é—œä¿‚æå–

```python
class EntityRelationExtractor:
    """
    å¯¦é«”é—œä¿‚æå–å™¨

    â€¹1â€º ä½¿ç”¨ LLM æå–å¯¦é«”å’Œé—œä¿‚
    â€¹2â€º æ”¯æ´è‡ªè¨‚å¯¦é«”é¡å‹
    """

    EXTRACTION_PROMPT = """å¾ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å¯¦é«”å’Œé—œä¿‚ã€‚

å¯¦é«”é¡å‹ï¼š
- COMPANY: å…¬å¸ã€çµ„ç¹”
- PERSON: äººç‰©
- PRODUCT: ç”¢å“ã€æŠ€è¡“
- LOCATION: åœ°é»
- EVENT: äº‹ä»¶
- METRIC: æ•¸æ“šæŒ‡æ¨™

è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
    "entities": [
        {{"name": "å¯¦é«”åç¨±", "type": "å¯¦é«”é¡å‹"}}
    ],
    "relations": [
        {{"source": "ä¾†æºå¯¦é«”", "target": "ç›®æ¨™å¯¦é«”", "relation": "é—œä¿‚é¡å‹"}}
    ]
}}

æ–‡æœ¬ï¼š
{text}

JSONï¼š"""

    def __init__(self, client: Optional[AsyncOpenAI] = None):
        self.client = client or AsyncOpenAI()

    async def extract(self, text: str) -> Tuple[List[Entity], List[Relation]]:
        """å¾æ–‡æœ¬æå–å¯¦é«”å’Œé—œä¿‚"""
        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": self.EXTRACTION_PROMPT.format(text=text)
            }],
            response_format={"type": "json_object"},
            max_tokens=1000
        )

        try:
            data = json.loads(response.choices[0].message.content)
        except json.JSONDecodeError:
            return [], []

        entities = []
        entity_map = {}

        for e in data.get("entities", []):
            entity = Entity(
                name=e.get("name", ""),
                entity_type=e.get("type", "UNKNOWN")
            )
            entities.append(entity)
            entity_map[entity.name] = entity

        relations = []
        for r in data.get("relations", []):
            source_name = r.get("source", "")
            target_name = r.get("target", "")

            if source_name in entity_map and target_name in entity_map:
                relation = Relation(
                    source=entity_map[source_name],
                    target=entity_map[target_name],
                    relation_type=r.get("relation", "RELATED")
                )
                relations.append(relation)

        return entities, relations
```

---

## 7.6 æ•´åˆï¼šå®Œæ•´çš„æœå°‹æª¢ç´¢ç³»çµ±

ç¾åœ¨è®“æˆ‘å€‘å°‡æ‰€æœ‰çµ„ä»¶æ•´åˆæˆä¸€å€‹å®Œæ•´çš„ç³»çµ±ï¼š

```python
class SearchRetrievalSystem:
    """
    æœå°‹æª¢ç´¢ç³»çµ±

    â€¹1â€º æ•´åˆæœå°‹ã€ç€è¦½ã€RAGã€çŸ¥è­˜åœ–è­œ
    â€¹2â€º æä¾›çµ±ä¸€çš„æŸ¥è©¢ä»‹é¢
    â€¹3â€º æ”¯æ´å¤šæ¨¡æ…‹è³‡è¨Šæ•´åˆ
    """

    def __init__(
        self,
        search_manager: SearchManager,
        browser: WebBrowser,
        rag_retriever: RAGRetriever,
        knowledge_graph: Optional[KnowledgeGraph] = None,
        client: Optional[AsyncOpenAI] = None
    ):
        self.search_manager = search_manager
        self.browser = browser
        self.rag = rag_retriever
        self.kg = knowledge_graph or KnowledgeGraph()
        self.client = client or AsyncOpenAI()
        self.extractor = EntityRelationExtractor(client=self.client)

    async def research(
        self,
        query: str,
        num_search_results: int = 5,
        browse_top_n: int = 3,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´ç ”ç©¶æµç¨‹

        â€¹1â€º æœå°‹ç›¸é—œç¶²é 
        â€¹2â€º ç€è¦½ä¸¦æ“·å–å…§å®¹
        â€¹3â€º å»ºç«‹ RAG ç´¢å¼•
        â€¹4â€º æå–çŸ¥è­˜åœ–è­œ
        â€¹5â€º ç”Ÿæˆç ”ç©¶çµæœ
        """
        result = {
            "query": query,
            "search_results": [],
            "pages_browsed": [],
            "chunks_indexed": 0,
            "entities_extracted": 0,
            "relations_extracted": 0,
            "context": ""
        }

        # 1. æœå°‹
        if verbose:
            print(f"ğŸ” æœå°‹: {query}")

        search_results = await self.search_manager.search(query, num_search_results)
        result["search_results"] = [r.to_dict() for r in search_results]

        if verbose:
            print(f"   æ‰¾åˆ° {len(search_results)} å€‹çµæœ")

        # 2. ç€è¦½
        if verbose:
            print(f"ğŸŒ ç€è¦½å‰ {browse_top_n} å€‹ç¶²é ")

        urls = [r.url for r in search_results[:browse_top_n]]
        pages = await self.browser.batch_browse(urls)

        for page in pages:
            if page.status_code == 200:
                result["pages_browsed"].append(page.to_dict())

                # 3. å»ºç«‹ RAG ç´¢å¼•
                chunks_added = await self.rag.add_document(
                    page.content,
                    page.url
                )
                result["chunks_indexed"] += chunks_added

                # 4. æå–çŸ¥è­˜åœ–è­œ
                entities, relations = await self.extractor.extract(page.content[:5000])
                for entity in entities:
                    self.kg.add_entity(entity)
                for relation in relations:
                    self.kg.add_relation(relation)

                result["entities_extracted"] += len(entities)
                result["relations_extracted"] += len(relations)

        if verbose:
            print(f"   ç€è¦½ {len(result['pages_browsed'])} å€‹ç¶²é ")
            print(f"   ç´¢å¼• {result['chunks_indexed']} å€‹ç‰‡æ®µ")
            print(f"   æå– {result['entities_extracted']} å€‹å¯¦é«”")

        # 5. ç”Ÿæˆç ”ç©¶ä¸Šä¸‹æ–‡
        rag_context = await self.rag.retrieve_with_context(query, top_k=5)
        kg_context = self.kg.to_prompt(max_triples=10)

        result["context"] = f"{rag_context}\n\n{kg_context}"

        return result

    async def answer(
        self,
        query: str,
        use_rag: bool = True,
        use_kg: bool = True
    ) -> str:
        """
        å›ç­”å•é¡Œ

        â€¹1â€º æª¢ç´¢ç›¸é—œè³‡è¨Š
        â€¹2â€º ç”Ÿæˆç­”æ¡ˆ
        """
        context_parts = []

        if use_rag:
            rag_context = await self.rag.retrieve_with_context(query, top_k=5)
            if rag_context:
                context_parts.append(f"[æª¢ç´¢åˆ°çš„è³‡è¨Š]\n{rag_context}")

        if use_kg:
            kg_context = self.kg.to_prompt(max_triples=10)
            if kg_context:
                context_parts.append(kg_context)

        context = "\n\n".join(context_parts)

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€å€‹ç ”ç©¶åŠ©ç†ï¼Œè«‹åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰è¶³å¤ çš„è³‡è¨Šï¼Œè«‹èªªæ˜ã€‚"
                },
                {
                    "role": "user",
                    "content": f"ä¸Šä¸‹æ–‡ï¼š\n{context}\n\nå•é¡Œï¼š{query}"
                }
            ],
            temperature=0.7
        )

        return response.choices[0].message.content
```

---

## 7.7 ç« ç¯€ç¸½çµ

### æ ¸å¿ƒæ”¶ç©«

1. **æœå°‹å±¤**
   - å¤šå¼•æ“æ•´åˆï¼ˆGoogle, Tavily, DuckDuckGoï¼‰
   - çµæœèšåˆèˆ‡å»é‡
   - å®¹éŒ¯èˆ‡å›é€€æ©Ÿåˆ¶

2. **æ“·å–å±¤**
   - ç¶²é å…§å®¹ç€è¦½
   - æ™ºèƒ½å…§å®¹æå–
   - PDF è§£æ

3. **æª¢ç´¢å±¤**
   - æ–‡ä»¶åˆ†å¡Šç­–ç•¥
   - å‘é‡ç´¢å¼•
   - RAG æª¢ç´¢

4. **çŸ¥è­˜åœ–è­œ**
   - å¯¦é«”é—œä¿‚æå–
   - è·¯å¾‘æŸ¥è©¢
   - çŸ¥è­˜æ•´åˆ

### æª¢æŸ¥æ¸…å–®

- [ ] å¯¦ç¾å¤šå¼•æ“æœå°‹æ•´åˆ
- [ ] å»ºç«‹ç¶²é ç€è¦½å™¨
- [ ] å¯¦ç¾æ–‡ä»¶åˆ†å¡Šå™¨
- [ ] å»ºç«‹å‘é‡ç´¢å¼•
- [ ] æ•´åˆçŸ¥è­˜åœ–è­œ
- [ ] æ¸¬è©¦å®Œæ•´ç ”ç©¶æµç¨‹

### æœ¬ç« ç”¢å‡ºç‰©

| é¡å‹ | å…§å®¹ |
|------|------|
| **æœå°‹æä¾›è€…** | SerperSearchProvider, TavilySearchProvider, DuckDuckGoSearchProvider |
| **ç€è¦½å™¨** | WebBrowser, ContentExtractor |
| **RAG** | DocumentChunker, VectorIndex, RAGRetriever |
| **çŸ¥è­˜åœ–è­œ** | KnowledgeGraph, EntityRelationExtractor |
| **æ•´åˆç³»çµ±** | SearchRetrievalSystem |

---

## 7.8 ä¸‹ä¸€ç« é å‘Š

**ç¬¬ 8 ç« ï¼šç’°å¢ƒæ­å»ºèˆ‡éƒ¨ç½²**

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘å€‘å°‡é€²å…¥å·¥ç¨‹å¯¦è¸éƒ¨åˆ†ï¼š

- 8B åˆ° 72B æ¨¡å‹çš„éƒ¨ç½²ç­–ç•¥
- vLLM å’Œ TensorRT-LLM æ¨ç†å„ªåŒ–
- Docker å®¹å™¨åŒ–éƒ¨ç½²
- æ•ˆèƒ½èª¿å„ªæŠ€å·§

ä½ å°‡å­¸æœƒå¦‚ä½•åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­éƒ¨ç½²æ·±åº¦ç ”ç©¶ä»£ç†äººã€‚

---

## æœ¬ç« ç¨‹å¼ç¢¼

**GitHub ä½ç½®**ï¼š`code-examples/chapter-07/`

| æª”æ¡ˆ | è¡Œæ•¸ | èªªæ˜ |
|------|------|------|
| `search_engine.py` | ~400 | æœå°‹å¼•æ“æ•´åˆ |
| `web_browser.py` | ~250 | ç¶²é ç€è¦½å™¨ |
| `rag_retriever.py` | ~350 | RAG æª¢ç´¢ç³»çµ± |
| `knowledge_graph.py` | ~200 | çŸ¥è­˜åœ–è­œ |
| `requirements.txt` | - | ä¾è³´æ¸…å–® |
| `.env.example` | - | ç’°å¢ƒè®Šæ•¸ç¯„ä¾‹ |
