# ç¬¬ 6 ç« ï¼šé•·çŸ­æ™‚è¨˜æ†¶ç®¡ç†

> **æœ¬ç« ç›®æ¨™**ï¼šæŒæ¡æ·±åº¦ç ”ç©¶ä»£ç†äººçš„è¨˜æ†¶æ¶æ§‹è¨­è¨ˆï¼Œå­¸æœƒåœ¨ 256K ä¸Šä¸‹æ–‡è¦–çª—ä¸­æœ‰æ•ˆç®¡ç†é•·çŸ­æœŸè¨˜æ†¶ï¼Œå¯¦ç¾å‹•æ…‹å£“ç¸®èˆ‡æ™ºèƒ½æª¢ç´¢ã€‚

---

## 6.1 å•é¡Œï¼šç•¶è¨˜æ†¶æˆç‚ºç“¶é ¸

æƒ³åƒä½ æ­£åœ¨é€²è¡Œä¸€é …æ·±åº¦ç ”ç©¶â€”â€”åˆ†æéå»åå¹´çš„ AI ç™¼å±•è¶¨å‹¢ã€‚ä½ çš„ä»£ç†äººå·²ç¶“ï¼š

- æœå°‹äº† 50 å€‹ç›¸é—œç¶²é 
- é–±è®€äº† 20 ç¯‡å­¸è¡“è«–æ–‡æ‘˜è¦
- åŸ·è¡Œäº† 30 æ¬¡å·¥å…·èª¿ç”¨
- ç´¯ç©äº†è¶…é 100,000 tokens çš„å°è©±æ­·å²

ç¾åœ¨ï¼Œç•¶ä½ å•ä»£ç†äººï¼šã€Œæ ¹æ“šå‰›æ‰çš„ç ”ç©¶ï¼Œ2018 å¹´çš„é—œéµçªç ´æ˜¯ä»€éº¼ï¼Ÿã€

ä»£ç†äººå»å›ç­”ï¼šã€ŒæŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰çœ‹åˆ°é—œæ–¼ 2018 å¹´çš„è³‡è¨Šã€‚ã€

é€™ä¸æ˜¯å¹»è¦ºï¼Œè€Œæ˜¯**è¨˜æ†¶ä¸Ÿå¤±**â€”â€”æ—©æœŸçš„è³‡è¨Šè¢«å¾Œä¾†çš„å…§å®¹æ“ å‡ºäº†æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡è¦–çª—ã€‚

### è¨˜æ†¶ç®¡ç†çš„ä¸‰å¤§æŒ‘æˆ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¨˜æ†¶ç®¡ç†æŒ‘æˆ°                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. å®¹é‡é™åˆ¶                                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚ 256K tokens â‰ˆ 200 é æ–‡å­—              â”‚              â”‚
â”‚     â”‚ æ·±åº¦ç ”ç©¶å¯èƒ½éœ€è¦ 500+ é è³‡è¨Š           â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                             â”‚
â”‚  2. æª¢ç´¢æ•ˆç‡                                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚ ç·šæ€§æƒæ 256K tokens = é«˜å»¶é²           â”‚              â”‚
â”‚     â”‚ æ³¨æ„åŠ›è¨ˆç®— O(nÂ²) = æˆæœ¬æ¿€å¢             â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                             â”‚
â”‚  3. è³‡è¨Šè¡°æ¸›                                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚ æ—©æœŸè³‡è¨Šè¢«ç¨€é‡‹                         â”‚              â”‚
â”‚     â”‚ é—œéµç´°ç¯€è¢«éºå¿˜                         â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MiroThinker çš„è§£æ±ºæ–¹æ¡ˆ

MiroThinker æ¡ç”¨äº†ä¸€å¥—**åˆ†å±¤è¨˜æ†¶æ¶æ§‹**ï¼Œæ¨¡ä»¿äººé¡çš„è¨˜æ†¶ç³»çµ±ï¼š

| è¨˜æ†¶å±¤ç´š | äººé¡é¡æ¯” | ç‰¹é» | å®¹é‡ |
|----------|----------|------|------|
| **å·¥ä½œè¨˜æ†¶** | çŸ­æœŸè¨˜æ†¶ | ç•¶å‰å°è©±ä¸Šä¸‹æ–‡ | ~8K tokens |
| **æƒ…ç¯€è¨˜æ†¶** | æœ€è¿‘ç¶“æ­· | è¿‘æœŸç ”ç©¶æ­¥é©Ÿ | ~32K tokens |
| **èªç¾©è¨˜æ†¶** | çŸ¥è­˜åº« | å£“ç¸®å¾Œçš„æ ¸å¿ƒçŸ¥è­˜ | ç„¡é™ï¼ˆå¤–éƒ¨å­˜å„²ï¼‰ |

æœ¬ç« å°‡å¸¶ä½ å¯¦ç¾é€™å¥—å®Œæ•´çš„è¨˜æ†¶ç®¡ç†ç³»çµ±ã€‚

---

## 6.2 è¨˜æ†¶æ¶æ§‹è¨­è¨ˆ

### 6.2.1 ä¸‰å±¤è¨˜æ†¶æ¨¡å‹

MiroThinker çš„è¨˜æ†¶ç³»çµ±åˆ†ç‚ºä¸‰å€‹å±¤ç´šï¼Œæ¯å±¤æœ‰ä¸åŒçš„ç”Ÿå‘½é€±æœŸå’Œå­˜å–æ¨¡å¼ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ä¸‰å±¤è¨˜æ†¶æ¶æ§‹                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              å·¥ä½œè¨˜æ†¶ï¼ˆWorking Memoryï¼‰              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ç•¶å‰å•é¡Œ â”‚ â”‚æœ€è¿‘æ€è€ƒ â”‚ â”‚å¾…è™•ç†   â”‚ â”‚è‡¨æ™‚è®Šæ•¸ â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚  å®¹é‡ï¼š~8K tokens | ç”Ÿå‘½é€±æœŸï¼šå–®æ¬¡äº¤äº’               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†“ æº¢å‡º                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              æƒ…ç¯€è¨˜æ†¶ï¼ˆEpisodic Memoryï¼‰             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ æ­¥é©Ÿ 1 â†’ æ­¥é©Ÿ 2 â†’ æ­¥é©Ÿ 3 â†’ ... â†’ æ­¥é©Ÿ N    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ (æœå°‹)   (é–±è®€)   (åˆ†æ)        (æ•´åˆ)     â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚  å®¹é‡ï¼š~32K tokens | ç”Ÿå‘½é€±æœŸï¼šå–®æ¬¡ä»»å‹™              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†“ å£“ç¸®                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              èªç¾©è¨˜æ†¶ï¼ˆSemantic Memoryï¼‰             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚æ¦‚å¿µ A â”‚â”€â”€â”‚æ¦‚å¿µ B â”‚â”€â”€â”‚æ¦‚å¿µ C â”‚â”€â”€â”‚æ¦‚å¿µ D â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â”‚            ï¼¼        â•±                              â”‚   â”‚
â”‚  â”‚              çŸ¥è­˜åœ–è­œ                               â”‚   â”‚
â”‚  â”‚  å®¹é‡ï¼šç„¡é™ | ç”Ÿå‘½é€±æœŸï¼šæ°¸ä¹…                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2.2 è¨˜æ†¶é …ç›®è³‡æ–™çµæ§‹

è®“æˆ‘å€‘å®šç¾©è¨˜æ†¶é …ç›®çš„åŸºæœ¬çµæ§‹ï¼š

```python
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional
import hashlib
import json


class MemoryType(Enum):
    """è¨˜æ†¶é¡å‹"""
    WORKING = "working"      # å·¥ä½œè¨˜æ†¶
    EPISODIC = "episodic"    # æƒ…ç¯€è¨˜æ†¶
    SEMANTIC = "semantic"    # èªç¾©è¨˜æ†¶


class MemoryPriority(Enum):
    """è¨˜æ†¶å„ªå…ˆç´š"""
    CRITICAL = 4    # é—œéµè³‡è¨Šï¼Œä¸å¯åˆªé™¤
    HIGH = 3        # é«˜å„ªå…ˆç´š
    MEDIUM = 2      # ä¸­ç­‰å„ªå…ˆç´š
    LOW = 1         # ä½å„ªå…ˆç´š


@dataclass
class MemoryItem:
    """
    è¨˜æ†¶é …ç›®

    â€¹1â€º æ¯å€‹è¨˜æ†¶é …ç›®éƒ½æœ‰å”¯ä¸€è­˜åˆ¥ç¢¼
    â€¹2â€º åŒ…å«é‡è¦æ€§è©•åˆ†å’Œå­˜å–è¨ˆæ•¸
    â€¹3â€º æ”¯æ´å‘é‡åµŒå…¥ä»¥ä¾¿èªç¾©æª¢ç´¢
    """
    content: str                                    # â€¹1â€º è¨˜æ†¶å…§å®¹
    memory_type: MemoryType                         # è¨˜æ†¶é¡å‹
    created_at: datetime = field(                   # å‰µå»ºæ™‚é–“
        default_factory=datetime.now
    )
    last_accessed: datetime = field(                # æœ€å¾Œå­˜å–æ™‚é–“
        default_factory=datetime.now
    )
    access_count: int = 0                           # â€¹2â€º å­˜å–æ¬¡æ•¸
    importance: float = 0.5                         # é‡è¦æ€§è©•åˆ† (0-1)
    priority: MemoryPriority = MemoryPriority.MEDIUM
    embedding: Optional[List[float]] = None         # â€¹3â€º å‘é‡åµŒå…¥
    metadata: Dict[str, Any] = field(default_factory=dict)

    _id: str = field(default="", init=False)

    def __post_init__(self):
        # ç”Ÿæˆå”¯ä¸€ ID
        content_hash = hashlib.md5(
            self.content.encode()
        ).hexdigest()[:12]
        timestamp = self.created_at.strftime("%Y%m%d%H%M%S")
        self._id = f"mem_{timestamp}_{content_hash}"

    @property
    def id(self) -> str:
        return self._id

    @property
    def token_count(self) -> int:
        """ä¼°ç®— token æ•¸é‡"""
        return len(self.content) // 3

    @property
    def recency_score(self) -> float:
        """è¨ˆç®—æ–°é®®åº¦åˆ†æ•¸ï¼ˆ0-1ï¼‰"""
        age = (datetime.now() - self.last_accessed).total_seconds()
        # ä½¿ç”¨æŒ‡æ•¸è¡°æ¸›ï¼š1 å°æ™‚è¡°æ¸›åˆ° 0.5
        decay_rate = 0.693 / 3600  # ln(2) / 1 hour
        return min(1.0, max(0.0, 2.718 ** (-decay_rate * age)))

    @property
    def relevance_score(self) -> float:
        """è¨ˆç®—ç¶œåˆç›¸é—œæ€§åˆ†æ•¸"""
        # çµåˆé‡è¦æ€§ã€å­˜å–é »ç‡å’Œæ–°é®®åº¦
        frequency_score = min(1.0, self.access_count / 10)
        return (
            self.importance * 0.4 +
            frequency_score * 0.3 +
            self.recency_score * 0.3
        )

    def access(self) -> None:
        """è¨˜éŒ„ä¸€æ¬¡å­˜å–"""
        self.last_accessed = datetime.now()
        self.access_count += 1

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "content": self.content,
            "memory_type": self.memory_type.value,
            "created_at": self.created_at.isoformat(),
            "last_accessed": self.last_accessed.isoformat(),
            "access_count": self.access_count,
            "importance": self.importance,
            "priority": self.priority.value,
            "token_count": self.token_count,
            "relevance_score": self.relevance_score,
            "metadata": self.metadata
        }
```

é€™å€‹è³‡æ–™çµæ§‹åŒ…å«äº†è¨˜æ†¶ç®¡ç†æ‰€éœ€çš„æ‰€æœ‰è³‡è¨Šï¼š

- **å…§å®¹èˆ‡é¡å‹**ï¼šè¨˜æ†¶çš„å¯¦éš›å…§å®¹å’Œæ‰€å±¬å±¤ç´š
- **æ™‚é–“è¿½è¹¤**ï¼šå‰µå»ºæ™‚é–“å’Œæœ€å¾Œå­˜å–æ™‚é–“ï¼Œç”¨æ–¼è¨ˆç®—æ–°é®®åº¦
- **å­˜å–çµ±è¨ˆ**ï¼šå­˜å–æ¬¡æ•¸ï¼Œç”¨æ–¼åˆ¤æ–·é‡è¦æ€§
- **å‘é‡åµŒå…¥**ï¼šæ”¯æ´èªç¾©æª¢ç´¢

---

## 6.3 å·¥ä½œè¨˜æ†¶ç®¡ç†

å·¥ä½œè¨˜æ†¶æ˜¯ä»£ç†äººçš„ã€Œæ„è­˜ã€â€”â€”ç•¶å‰æ­£åœ¨è™•ç†çš„è³‡è¨Šã€‚å®ƒéœ€è¦å¿«é€Ÿå­˜å–ã€åš´æ ¼çš„å®¹é‡æ§åˆ¶ã€‚

### 6.3.1 å·¥ä½œè¨˜æ†¶å¯¦ç¾

```python
from collections import OrderedDict
from typing import Generator


class WorkingMemory:
    """
    å·¥ä½œè¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º ä½¿ç”¨ LRUï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ç­–ç•¥ç®¡ç†å®¹é‡
    â€¹2â€º æ”¯æ´å„ªå…ˆç´šä¿è­·ï¼Œé—œéµè³‡è¨Šä¸è¢«é©…é€
    â€¹3â€º æä¾›å¿«é€Ÿçš„ key-value å­˜å–
    """

    def __init__(
        self,
        max_tokens: int = 8000,
        protected_ratio: float = 0.2  # ä¿è­·å€ä½”æ¯”
    ):
        self.max_tokens = max_tokens
        self.protected_tokens = int(max_tokens * protected_ratio)
        self._items: OrderedDict[str, MemoryItem] = OrderedDict()
        self._current_tokens = 0

    @property
    def available_tokens(self) -> int:
        """å¯ç”¨ token æ•¸"""
        return self.max_tokens - self._current_tokens

    @property
    def utilization(self) -> float:
        """ä½¿ç”¨ç‡"""
        return self._current_tokens / self.max_tokens

    def add(
        self,
        content: str,
        importance: float = 0.5,
        priority: MemoryPriority = MemoryPriority.MEDIUM,
        **metadata
    ) -> MemoryItem:
        """
        æ·»åŠ è¨˜æ†¶é …ç›®

        â€¹1â€º å¦‚æœè¶…å‡ºå®¹é‡ï¼Œå…ˆé©…é€ä½å„ªå…ˆç´šé …ç›®
        â€¹2â€º é—œéµå„ªå…ˆç´šé …ç›®é€²å…¥ä¿è­·å€
        """
        item = MemoryItem(
            content=content,
            memory_type=MemoryType.WORKING,
            importance=importance,
            priority=priority,
            metadata=metadata
        )

        # æª¢æŸ¥æ˜¯å¦éœ€è¦é©…é€
        while (
            self._current_tokens + item.token_count > self.max_tokens
            and self._items
        ):
            evicted = self._evict_one()
            if evicted is None:
                # ç„¡æ³•é©…é€æ›´å¤šé …ç›®
                break

        # æ·»åŠ æ–°é …ç›®
        self._items[item.id] = item
        self._items.move_to_end(item.id)  # LRU: ç§»åˆ°æœ«å°¾
        self._current_tokens += item.token_count

        return item

    def get(self, item_id: str) -> Optional[MemoryItem]:
        """ç²å–è¨˜æ†¶é …ç›®ï¼ˆä¸¦æ›´æ–°å­˜å–è¨˜éŒ„ï¼‰"""
        if item_id not in self._items:
            return None

        item = self._items[item_id]
        item.access()
        self._items.move_to_end(item_id)  # LRU: ç§»åˆ°æœ«å°¾
        return item

    def search(
        self,
        query: str,
        limit: int = 5
    ) -> List[MemoryItem]:
        """
        æœå°‹ç›¸é—œè¨˜æ†¶

        â€¹1â€º ç°¡æ˜“é—œéµå­—åŒ¹é…
        â€¹2â€º æŒ‰ç›¸é—œæ€§æ’åº
        """
        query_lower = query.lower()
        results = []

        for item in self._items.values():
            # ç°¡æ˜“ç›¸é—œæ€§è¨ˆç®—
            content_lower = item.content.lower()
            if query_lower in content_lower:
                score = item.relevance_score
                # é—œéµå­—åŒ¹é…åŠ åˆ†
                score += 0.3
                results.append((score, item))

        # æŒ‰åˆ†æ•¸æ’åº
        results.sort(key=lambda x: x[0], reverse=True)
        return [item for _, item in results[:limit]]

    def _evict_one(self) -> Optional[MemoryItem]:
        """
        é©…é€ä¸€å€‹é …ç›®

        â€¹1â€º å„ªå…ˆé©…é€ä½å„ªå…ˆç´šé …ç›®
        â€¹2â€º åŒå„ªå…ˆç´šé©…é€æœ€èˆŠçš„ï¼ˆLRU é ­éƒ¨ï¼‰
        â€¹3â€º ä¿è­·å€å…§çš„é …ç›®ä¸é©…é€
        """
        # æŒ‰å„ªå…ˆç´šåˆ†çµ„
        candidates = []
        protected_tokens = 0

        for item_id, item in self._items.items():
            if item.priority == MemoryPriority.CRITICAL:
                protected_tokens += item.token_count
                continue
            candidates.append((item.priority.value, item.relevance_score, item_id))

        if not candidates:
            return None

        # æŒ‰å„ªå…ˆç´šå’Œç›¸é—œæ€§æ’åºï¼ˆæœ€ä½çš„åœ¨å‰ï¼‰
        candidates.sort(key=lambda x: (x[0], x[1]))
        evict_id = candidates[0][2]

        item = self._items.pop(evict_id)
        self._current_tokens -= item.token_count
        return item

    def clear(self, keep_critical: bool = True) -> int:
        """æ¸…ç©ºå·¥ä½œè¨˜æ†¶"""
        if keep_critical:
            to_remove = [
                item_id for item_id, item in self._items.items()
                if item.priority != MemoryPriority.CRITICAL
            ]
            for item_id in to_remove:
                item = self._items.pop(item_id)
                self._current_tokens -= item.token_count
            return len(to_remove)
        else:
            count = len(self._items)
            self._items.clear()
            self._current_tokens = 0
            return count

    def to_prompt(self) -> str:
        """å°‡å·¥ä½œè¨˜æ†¶è½‰æ›ç‚º prompt æ ¼å¼"""
        if not self._items:
            return ""

        lines = ["[å·¥ä½œè¨˜æ†¶]"]
        for item in self._items.values():
            priority_marker = {
                MemoryPriority.CRITICAL: "ğŸ”´",
                MemoryPriority.HIGH: "ğŸŸ ",
                MemoryPriority.MEDIUM: "ğŸŸ¡",
                MemoryPriority.LOW: "âšª"
            }.get(item.priority, "âšª")
            lines.append(f"{priority_marker} {item.content}")

        return "\n".join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        priority_counts = {}
        for item in self._items.values():
            key = item.priority.name
            priority_counts[key] = priority_counts.get(key, 0) + 1

        return {
            "item_count": len(self._items),
            "total_tokens": self._current_tokens,
            "max_tokens": self.max_tokens,
            "utilization": self.utilization,
            "available_tokens": self.available_tokens,
            "priority_distribution": priority_counts
        }
```

### 6.3.2 å·¥ä½œè¨˜æ†¶ä½¿ç”¨ç¯„ä¾‹

```python
# å‰µå»ºå·¥ä½œè¨˜æ†¶
working_memory = WorkingMemory(max_tokens=8000)

# æ·»åŠ ç•¶å‰ä»»å‹™
working_memory.add(
    content="ç”¨æˆ¶å•é¡Œï¼šåˆ†æ 2024 å¹´ AI æ™¶ç‰‡å¸‚å ´è¶¨å‹¢",
    importance=1.0,
    priority=MemoryPriority.CRITICAL,
    source="user_query"
)

# æ·»åŠ ä¸­é–“æ€è€ƒ
working_memory.add(
    content="éœ€è¦æœå°‹å¸‚å ´æ•¸æ“šã€ä¸»è¦å» å•†è³‡è¨Šã€æŠ€è¡“è¶¨å‹¢",
    importance=0.8,
    priority=MemoryPriority.HIGH,
    source="agent_thought"
)

# æ·»åŠ å·¥å…·çµæœæ‘˜è¦
working_memory.add(
    content="æœå°‹çµæœï¼šNVIDIA å¸‚å ´ä»½é¡ 80%ï¼ŒAMD 10%ï¼ŒIntel 5%",
    importance=0.9,
    priority=MemoryPriority.HIGH,
    source="tool_result"
)

# æª¢è¦–çµ±è¨ˆ
print(working_memory.get_statistics())
# è¼¸å‡ºï¼š
# {
#     'item_count': 3,
#     'total_tokens': 156,
#     'max_tokens': 8000,
#     'utilization': 0.0195,
#     'priority_distribution': {'CRITICAL': 1, 'HIGH': 2}
# }

# ç”Ÿæˆ prompt
print(working_memory.to_prompt())
# è¼¸å‡ºï¼š
# [å·¥ä½œè¨˜æ†¶]
# ğŸ”´ ç”¨æˆ¶å•é¡Œï¼šåˆ†æ 2024 å¹´ AI æ™¶ç‰‡å¸‚å ´è¶¨å‹¢
# ğŸŸ  éœ€è¦æœå°‹å¸‚å ´æ•¸æ“šã€ä¸»è¦å» å•†è³‡è¨Šã€æŠ€è¡“è¶¨å‹¢
# ğŸŸ  æœå°‹çµæœï¼šNVIDIA å¸‚å ´ä»½é¡ 80%ï¼ŒAMD 10%ï¼ŒIntel 5%
```

---

## 6.4 æƒ…ç¯€è¨˜æ†¶ç®¡ç†

æƒ…ç¯€è¨˜æ†¶å­˜å„²ä»£ç†äººçš„ã€Œç¶“æ­·ã€â€”â€”ç ”ç©¶éç¨‹ä¸­çš„æ¯å€‹æ­¥é©Ÿã€‚é€™æ˜¯æ·±åº¦ç ”ç©¶ä»£ç†äººçš„æ ¸å¿ƒè¨˜æ†¶å±¤ã€‚

### 6.4.1 æƒ…ç¯€è¨˜æ†¶å¯¦ç¾

```python
from typing import Callable, Iterator
import asyncio


@dataclass
class Episode:
    """
    æƒ…ç¯€ï¼ˆç ”ç©¶æ­¥é©Ÿï¼‰

    â€¹1â€º æ¯å€‹æƒ…ç¯€æ˜¯ä¸€å€‹å®Œæ•´çš„ ReAct å¾ªç’°
    â€¹2â€º åŒ…å«æ€è€ƒã€è¡Œå‹•ã€è§€å¯Ÿ
    """
    step_number: int
    thought: str
    action: Optional[Dict[str, Any]] = None
    observation: Optional[str] = None
    summary: Optional[str] = None  # å£“ç¸®å¾Œçš„æ‘˜è¦
    created_at: datetime = field(default_factory=datetime.now)
    importance: float = 0.5

    @property
    def token_count(self) -> int:
        """ä¼°ç®— token æ•¸é‡"""
        total = len(self.thought) // 3
        if self.action:
            total += len(json.dumps(self.action)) // 3
        if self.observation:
            total += len(self.observation) // 3
        return total

    @property
    def compressed_token_count(self) -> int:
        """å£“ç¸®å¾Œçš„ token æ•¸é‡"""
        if self.summary:
            return len(self.summary) // 3
        return self.token_count

    def compress(self, summary: str) -> None:
        """å£“ç¸®æƒ…ç¯€"""
        self.summary = summary

    def to_prompt(self, use_summary: bool = False) -> str:
        """è½‰æ›ç‚º prompt æ ¼å¼"""
        if use_summary and self.summary:
            return f"[æ­¥é©Ÿ {self.step_number}] {self.summary}"

        lines = [f"[æ­¥é©Ÿ {self.step_number}]"]
        lines.append(f"æ€è€ƒï¼š{self.thought}")

        if self.action:
            tool_name = self.action.get("tool_name", "unknown")
            lines.append(f"è¡Œå‹•ï¼šèª¿ç”¨ {tool_name}")

        if self.observation:
            # æˆªæ–·éé•·çš„è§€å¯Ÿ
            obs = self.observation
            if len(obs) > 500:
                obs = obs[:500] + "..."
            lines.append(f"è§€å¯Ÿï¼š{obs}")

        return "\n".join(lines)


class EpisodicMemory:
    """
    æƒ…ç¯€è¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º é †åºå­˜å„²ç ”ç©¶æ­¥é©Ÿ
    â€¹2â€º æ”¯æ´æ»‘å‹•è¦–çª—å’Œé¸æ“‡æ€§å£“ç¸®
    â€¹3â€º æä¾›éˆæ´»çš„æª¢ç´¢æ©Ÿåˆ¶
    """

    def __init__(
        self,
        max_tokens: int = 32000,
        compression_threshold: float = 0.8,  # é”åˆ° 80% å®¹é‡æ™‚é–‹å§‹å£“ç¸®
        window_size: int = 10  # æ»‘å‹•è¦–çª—å¤§å°
    ):
        self.max_tokens = max_tokens
        self.compression_threshold = compression_threshold
        self.window_size = window_size
        self._episodes: List[Episode] = []
        self._current_tokens = 0
        self._compressor: Optional[Callable] = None

    def set_compressor(self, compressor: Callable[[str], str]) -> None:
        """è¨­ç½®å£“ç¸®å™¨ï¼ˆé€šå¸¸æ˜¯ LLM èª¿ç”¨ï¼‰"""
        self._compressor = compressor

    @property
    def episode_count(self) -> int:
        return len(self._episodes)

    @property
    def utilization(self) -> float:
        return self._current_tokens / self.max_tokens

    def add_episode(
        self,
        thought: str,
        action: Optional[Dict[str, Any]] = None,
        observation: Optional[str] = None,
        importance: float = 0.5
    ) -> Episode:
        """
        æ·»åŠ æ–°æƒ…ç¯€

        â€¹1â€º è‡ªå‹•ç·¨è™Ÿ
        â€¹2â€º æª¢æŸ¥æ˜¯å¦éœ€è¦å£“ç¸®
        """
        episode = Episode(
            step_number=len(self._episodes) + 1,
            thought=thought,
            action=action,
            observation=observation,
            importance=importance
        )

        self._episodes.append(episode)
        self._current_tokens += episode.token_count

        # æª¢æŸ¥æ˜¯å¦éœ€è¦å£“ç¸®
        if self.utilization > self.compression_threshold:
            self._trigger_compression()

        return episode

    def _trigger_compression(self) -> int:
        """
        è§¸ç™¼å£“ç¸®

        â€¹1â€º å£“ç¸®æœ€èˆŠçš„ã€ä½é‡è¦æ€§çš„æƒ…ç¯€
        â€¹2â€º ä¿ç•™æœ€è¿‘çš„ window_size å€‹æƒ…ç¯€ä¸å£“ç¸®
        """
        if not self._compressor:
            return 0

        compressed_count = 0
        target_tokens = int(self.max_tokens * 0.6)  # å£“ç¸®åˆ° 60%

        # æ‰¾å‡ºå¯å£“ç¸®çš„æƒ…ç¯€ï¼ˆä¸åœ¨æ»‘å‹•è¦–çª—å…§çš„ï¼‰
        compressible = self._episodes[:-self.window_size] if len(self._episodes) > self.window_size else []

        for episode in compressible:
            if episode.summary:  # å·²å£“ç¸®
                continue

            if self._current_tokens <= target_tokens:
                break

            # ç”Ÿæˆæ‘˜è¦
            original_content = episode.to_prompt(use_summary=False)
            summary = self._compressor(original_content)

            # æ›´æ–° token è¨ˆæ•¸
            old_tokens = episode.token_count
            episode.compress(summary)
            new_tokens = episode.compressed_token_count

            self._current_tokens -= (old_tokens - new_tokens)
            compressed_count += 1

        return compressed_count

    def get_recent(self, n: int = 5) -> List[Episode]:
        """ç²å–æœ€è¿‘ N å€‹æƒ…ç¯€"""
        return self._episodes[-n:]

    def get_by_importance(
        self,
        min_importance: float = 0.7,
        limit: int = 10
    ) -> List[Episode]:
        """æŒ‰é‡è¦æ€§ç²å–æƒ…ç¯€"""
        important = [
            ep for ep in self._episodes
            if ep.importance >= min_importance
        ]
        return sorted(
            important,
            key=lambda x: x.importance,
            reverse=True
        )[:limit]

    def search(
        self,
        query: str,
        limit: int = 5
    ) -> List[Episode]:
        """
        æœå°‹ç›¸é—œæƒ…ç¯€

        â€¹1â€º ç°¡æ˜“é—œéµå­—åŒ¹é…
        â€¹2â€º å¯æ“´å±•ç‚ºå‘é‡æª¢ç´¢
        """
        query_lower = query.lower()
        results = []

        for episode in self._episodes:
            content = episode.to_prompt().lower()
            if query_lower in content:
                # è¨ˆç®—åŒ¹é…åº¦
                match_count = content.count(query_lower)
                score = match_count * 0.1 + episode.importance
                results.append((score, episode))

        results.sort(key=lambda x: x[0], reverse=True)
        return [ep for _, ep in results[:limit]]

    def to_prompt(
        self,
        use_summary_for_old: bool = True,
        include_all: bool = False
    ) -> str:
        """
        ç”Ÿæˆ prompt

        â€¹1â€º èˆŠæƒ…ç¯€ä½¿ç”¨æ‘˜è¦
        â€¹2â€º æœ€è¿‘æƒ…ç¯€ä¿ç•™å®Œæ•´å…§å®¹
        """
        if not self._episodes:
            return ""

        lines = ["[ç ”ç©¶æ­·ç¨‹]"]

        # ç¢ºå®šå“ªäº›ä½¿ç”¨æ‘˜è¦
        summary_cutoff = len(self._episodes) - self.window_size

        for i, episode in enumerate(self._episodes):
            use_summary = use_summary_for_old and i < summary_cutoff
            lines.append(episode.to_prompt(use_summary=use_summary))
            lines.append("")  # ç©ºè¡Œåˆ†éš”

        return "\n".join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        compressed = sum(1 for ep in self._episodes if ep.summary)

        return {
            "episode_count": len(self._episodes),
            "compressed_count": compressed,
            "compression_rate": compressed / len(self._episodes) if self._episodes else 0,
            "total_tokens": self._current_tokens,
            "max_tokens": self.max_tokens,
            "utilization": self.utilization
        }
```

### 6.4.2 å‹•æ…‹å£“ç¸®ç­–ç•¥

å£“ç¸®æ˜¯æƒ…ç¯€è¨˜æ†¶çš„é—œéµåŠŸèƒ½ã€‚æˆ‘å€‘éœ€è¦åœ¨ä¿ç•™é—œéµè³‡è¨Šçš„åŒæ™‚æ¸›å°‘ token ä½¿ç”¨ï¼š

```python
from openai import AsyncOpenAI


class EpisodeCompressor:
    """
    æƒ…ç¯€å£“ç¸®å™¨

    â€¹1â€º ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
    â€¹2â€º ä¿ç•™é—œéµè³‡è¨Šï¼šç™¼ç¾ã€æ±ºç­–ã€çµæœ
    â€¹3â€º å¯é…ç½®å£“ç¸®æ¯”ä¾‹
    """

    COMPRESSION_PROMPT = """è«‹å°‡ä»¥ä¸‹ç ”ç©¶æ­¥é©Ÿå£“ç¸®ç‚ºç°¡æ½”æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. ä¿ç•™é—œéµç™¼ç¾å’Œçµè«–
2. ä¿ç•™é‡è¦çš„æ•¸æ“šå’Œäº‹å¯¦
3. çœç•¥éç¨‹ç´°ç¯€
4. æ§åˆ¶åœ¨ 50 å­—ä»¥å…§

åŸå§‹å…§å®¹ï¼š
{content}

æ‘˜è¦ï¼š"""

    def __init__(
        self,
        client: Optional[AsyncOpenAI] = None,
        model: str = "gpt-4o-mini"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model

    async def compress(self, content: str) -> str:
        """å£“ç¸®å–®å€‹æƒ…ç¯€"""
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": self.COMPRESSION_PROMPT.format(content=content)
            }],
            max_tokens=100,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()

    async def batch_compress(
        self,
        contents: List[str],
        concurrency: int = 5
    ) -> List[str]:
        """æ‰¹æ¬¡å£“ç¸®å¤šå€‹æƒ…ç¯€"""
        semaphore = asyncio.Semaphore(concurrency)

        async def compress_one(content: str) -> str:
            async with semaphore:
                return await self.compress(content)

        tasks = [compress_one(c) for c in contents]
        return await asyncio.gather(*tasks)


# ä½¿ç”¨ç¯„ä¾‹
async def demo_compression():
    compressor = EpisodeCompressor()

    original = """
    [æ­¥é©Ÿ 5]
    æ€è€ƒï¼šæ ¹æ“šæœå°‹çµæœï¼Œæˆ‘éœ€è¦æ·±å…¥äº†è§£ NVIDIA çš„ GPU æ¶æ§‹å„ªå‹¢ã€‚
    ä¸»è¦é—œæ³¨ CUDA ç”Ÿæ…‹ç³»çµ±ã€Tensor Core æŠ€è¡“ã€ä»¥åŠèˆ‡ç«¶çˆ­å°æ‰‹çš„å°æ¯”ã€‚

    è¡Œå‹•ï¼šèª¿ç”¨ web_browser

    è§€å¯Ÿï¼šNVIDIA çš„ GPU æ¶æ§‹å„ªå‹¢ä¸»è¦é«”ç¾åœ¨ä¸‰å€‹æ–¹é¢ï¼š
    1. CUDA ç”Ÿæ…‹ç³»çµ±ï¼šè¶…é 400 è¬é–‹ç™¼è€…ï¼Œ10+ å¹´ç©ç´¯
    2. Tensor Coreï¼šå°ˆç‚º AI å„ªåŒ–çš„è¨ˆç®—å–®å…ƒï¼ŒFP8 ç²¾åº¦æ”¯æ´
    3. è»Ÿé«”æ£§ï¼šcuDNNã€TensorRTã€Triton æ¨ç†ä¼ºæœå™¨
    èˆ‡ AMD ç›¸æ¯”ï¼ŒNVIDIA çš„è»Ÿé«”ç”Ÿæ…‹æ›´æˆç†Ÿï¼Œä½† AMD åœ¨æ€§åƒ¹æ¯”ä¸Šæœ‰å„ªå‹¢ã€‚
    """

    summary = await compressor.compress(original)
    print(f"åŸå§‹é•·åº¦: {len(original)} å­—ç¬¦")
    print(f"æ‘˜è¦é•·åº¦: {len(summary)} å­—ç¬¦")
    print(f"å£“ç¸®æ¯”: {len(summary)/len(original)*100:.1f}%")
    print(f"\næ‘˜è¦: {summary}")

# è¼¸å‡ºï¼š
# åŸå§‹é•·åº¦: 456 å­—ç¬¦
# æ‘˜è¦é•·åº¦: 68 å­—ç¬¦
# å£“ç¸®æ¯”: 14.9%
#
# æ‘˜è¦: NVIDIA GPU å„ªå‹¢ï¼šCUDA ç”Ÿæ…‹ï¼ˆ400è¬é–‹ç™¼è€…ï¼‰ã€Tensor Coreã€
#       å®Œæ•´è»Ÿé«”æ£§ã€‚è»Ÿé«”ç”Ÿæ…‹é ˜å…ˆ AMDï¼ŒAMD å‹åœ¨æ€§åƒ¹æ¯”ã€‚
```

---

## 6.5 èªç¾©è¨˜æ†¶ç®¡ç†

èªç¾©è¨˜æ†¶æ˜¯ä»£ç†äººçš„ã€ŒçŸ¥è­˜åº«ã€â€”â€”å£“ç¸®ã€çµæ§‹åŒ–å¾Œçš„é•·æœŸçŸ¥è­˜ã€‚å®ƒä½¿ç”¨å‘é‡è³‡æ–™åº«å¯¦ç¾é«˜æ•ˆæª¢ç´¢ã€‚

### 6.5.1 èªç¾©è¨˜æ†¶å¯¦ç¾

```python
import numpy as np
from typing import Tuple


@dataclass
class KnowledgeChunk:
    """
    çŸ¥è­˜ç‰‡æ®µ

    â€¹1â€º å¾æƒ…ç¯€è¨˜æ†¶å£“ç¸®è€Œä¾†
    â€¹2â€º åŒ…å«å‘é‡åµŒå…¥ä¾›èªç¾©æª¢ç´¢
    â€¹3â€º è¨˜éŒ„ä¾†æºä»¥ä¾¿æº¯æº
    """
    content: str
    embedding: List[float]
    source_episodes: List[int]  # ä¾†æºæƒ…ç¯€ç·¨è™Ÿ
    created_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

    _id: str = field(default="", init=False)

    def __post_init__(self):
        content_hash = hashlib.md5(self.content.encode()).hexdigest()[:12]
        self._id = f"know_{content_hash}"

    @property
    def id(self) -> str:
        return self._id


class SemanticMemory:
    """
    èªç¾©è¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é€²è¡Œèªç¾©æª¢ç´¢
    â€¹2â€º æ”¯æ´çŸ¥è­˜æ•´åˆï¼ˆå»é‡ã€åˆä½µï¼‰
    â€¹3â€º å¯æŒä¹…åŒ–åˆ°å‘é‡è³‡æ–™åº«
    """

    def __init__(
        self,
        embedding_dim: int = 1536,  # OpenAI embedding ç¶­åº¦
        similarity_threshold: float = 0.85  # å»é‡é–¾å€¼
    ):
        self.embedding_dim = embedding_dim
        self.similarity_threshold = similarity_threshold
        self._chunks: Dict[str, KnowledgeChunk] = {}
        self._embeddings: Optional[np.ndarray] = None
        self._chunk_ids: List[str] = []
        self._embedder: Optional[Callable] = None

    def set_embedder(
        self,
        embedder: Callable[[str], List[float]]
    ) -> None:
        """è¨­ç½®åµŒå…¥å‡½æ•¸"""
        self._embedder = embedder

    def add_knowledge(
        self,
        content: str,
        embedding: Optional[List[float]] = None,
        source_episodes: Optional[List[int]] = None,
        **metadata
    ) -> Optional[KnowledgeChunk]:
        """
        æ·»åŠ çŸ¥è­˜ç‰‡æ®µ

        â€¹1â€º è‡ªå‹•ç”ŸæˆåµŒå…¥ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        â€¹2â€º æª¢æŸ¥é‡è¤‡ï¼ˆåŸºæ–¼èªç¾©ç›¸ä¼¼åº¦ï¼‰
        â€¹3â€º å¦‚æœç›¸ä¼¼ï¼Œåˆä½µè€Œéæ·»åŠ 
        """
        # ç”ŸæˆåµŒå…¥
        if embedding is None:
            if self._embedder is None:
                raise ValueError("æœªè¨­ç½®åµŒå…¥å‡½æ•¸")
            embedding = self._embedder(content)

        # æª¢æŸ¥é‡è¤‡
        if self._chunks:
            similar_id, similarity = self._find_similar(embedding)
            if similarity > self.similarity_threshold:
                # åˆä½µåˆ°ç¾æœ‰çŸ¥è­˜
                self._merge_knowledge(similar_id, content, source_episodes or [])
                return self._chunks[similar_id]

        # æ·»åŠ æ–°çŸ¥è­˜
        chunk = KnowledgeChunk(
            content=content,
            embedding=embedding,
            source_episodes=source_episodes or [],
            metadata=metadata
        )

        self._chunks[chunk.id] = chunk
        self._update_index(chunk)

        return chunk

    def _update_index(self, chunk: KnowledgeChunk) -> None:
        """æ›´æ–°å‘é‡ç´¢å¼•"""
        new_embedding = np.array(chunk.embedding).reshape(1, -1)

        if self._embeddings is None:
            self._embeddings = new_embedding
        else:
            self._embeddings = np.vstack([self._embeddings, new_embedding])

        self._chunk_ids.append(chunk.id)

    def _find_similar(
        self,
        embedding: List[float]
    ) -> Tuple[Optional[str], float]:
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„çŸ¥è­˜ç‰‡æ®µ"""
        if self._embeddings is None or len(self._chunk_ids) == 0:
            return None, 0.0

        query = np.array(embedding)

        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        norms = np.linalg.norm(self._embeddings, axis=1)
        query_norm = np.linalg.norm(query)

        if query_norm == 0:
            return None, 0.0

        similarities = np.dot(self._embeddings, query) / (norms * query_norm)

        max_idx = np.argmax(similarities)
        max_similarity = similarities[max_idx]

        return self._chunk_ids[max_idx], float(max_similarity)

    def _merge_knowledge(
        self,
        chunk_id: str,
        new_content: str,
        source_episodes: List[int]
    ) -> None:
        """åˆä½µçŸ¥è­˜åˆ°ç¾æœ‰ç‰‡æ®µ"""
        chunk = self._chunks[chunk_id]

        # æ·»åŠ ä¾†æº
        chunk.source_episodes.extend(source_episodes)
        chunk.source_episodes = list(set(chunk.source_episodes))

        # å¯é¸ï¼šæ›´æ–°å…§å®¹ï¼ˆé€™è£¡ç°¡å–®åœ°é™„åŠ ï¼‰
        # å¯¦éš›æ‡‰ç”¨ä¸­å¯èƒ½éœ€è¦ç”¨ LLM æ™ºèƒ½åˆä½µ
        if new_content not in chunk.content:
            chunk.metadata["merged_contents"] = chunk.metadata.get(
                "merged_contents", []
            )
            chunk.metadata["merged_contents"].append(new_content)

    def search(
        self,
        query: str,
        limit: int = 5,
        min_similarity: float = 0.5
    ) -> List[Tuple[KnowledgeChunk, float]]:
        """
        èªç¾©æœå°‹

        â€¹1â€º å°‡æŸ¥è©¢è½‰æ›ç‚ºåµŒå…¥å‘é‡
        â€¹2â€º è¨ˆç®—èˆ‡æ‰€æœ‰çŸ¥è­˜çš„ç›¸ä¼¼åº¦
        â€¹3â€º è¿”å›æœ€ç›¸é—œçš„çµæœ
        """
        if self._embedder is None:
            raise ValueError("æœªè¨­ç½®åµŒå…¥å‡½æ•¸")

        if not self._chunks:
            return []

        query_embedding = self._embedder(query)
        query_vec = np.array(query_embedding)

        # è¨ˆç®—ç›¸ä¼¼åº¦
        norms = np.linalg.norm(self._embeddings, axis=1)
        query_norm = np.linalg.norm(query_vec)

        if query_norm == 0:
            return []

        similarities = np.dot(self._embeddings, query_vec) / (norms * query_norm)

        # éæ¿¾å’Œæ’åº
        results = []
        for i, sim in enumerate(similarities):
            if sim >= min_similarity:
                chunk_id = self._chunk_ids[i]
                chunk = self._chunks[chunk_id]
                results.append((chunk, float(sim)))

        results.sort(key=lambda x: x[1], reverse=True)
        return results[:limit]

    def get_all_knowledge(self) -> List[KnowledgeChunk]:
        """ç²å–æ‰€æœ‰çŸ¥è­˜"""
        return list(self._chunks.values())

    def to_prompt(
        self,
        query: Optional[str] = None,
        limit: int = 5
    ) -> str:
        """
        ç”Ÿæˆ prompt

        â€¹1â€º å¦‚æœæœ‰æŸ¥è©¢ï¼Œè¿”å›ç›¸é—œçŸ¥è­˜
        â€¹2â€º å¦å‰‡è¿”å›æ‰€æœ‰çŸ¥è­˜æ‘˜è¦
        """
        if not self._chunks:
            return ""

        lines = ["[çŸ¥è­˜åº«]"]

        if query:
            results = self.search(query, limit=limit)
            for chunk, similarity in results:
                lines.append(f"[ç›¸é—œåº¦: {similarity:.2f}] {chunk.content}")
        else:
            for chunk in list(self._chunks.values())[:limit]:
                lines.append(f"â€¢ {chunk.content}")

        return "\n".join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        return {
            "chunk_count": len(self._chunks),
            "total_sources": sum(
                len(c.source_episodes) for c in self._chunks.values()
            ),
            "embedding_dim": self.embedding_dim
        }
```

### 6.5.2 åµŒå…¥ç”Ÿæˆå™¨

```python
class EmbeddingGenerator:
    """
    åµŒå…¥ç”Ÿæˆå™¨

    â€¹1â€º ä½¿ç”¨ OpenAI embedding API
    â€¹2â€º æ”¯æ´æ‰¹æ¬¡è™•ç†
    â€¹3â€º å…§å»ºå¿«å–æ©Ÿåˆ¶
    """

    def __init__(
        self,
        client: Optional[AsyncOpenAI] = None,
        model: str = "text-embedding-3-small"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model
        self._cache: Dict[str, List[float]] = {}

    async def embed(self, text: str) -> List[float]:
        """ç”Ÿæˆå–®å€‹æ–‡æœ¬çš„åµŒå…¥"""
        # æª¢æŸ¥å¿«å–
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self._cache:
            return self._cache[cache_key]

        response = await self.client.embeddings.create(
            model=self.model,
            input=text
        )

        embedding = response.data[0].embedding

        # å­˜å…¥å¿«å–
        self._cache[cache_key] = embedding

        return embedding

    async def batch_embed(
        self,
        texts: List[str],
        batch_size: int = 100
    ) -> List[List[float]]:
        """æ‰¹æ¬¡ç”ŸæˆåµŒå…¥"""
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            # æª¢æŸ¥å¿«å–
            to_embed = []
            cached_indices = []
            for j, text in enumerate(batch):
                cache_key = hashlib.md5(text.encode()).hexdigest()
                if cache_key in self._cache:
                    cached_indices.append((j, self._cache[cache_key]))
                else:
                    to_embed.append((j, text))

            # æ‰¹æ¬¡åµŒå…¥
            if to_embed:
                response = await self.client.embeddings.create(
                    model=self.model,
                    input=[t for _, t in to_embed]
                )

                for k, (j, text) in enumerate(to_embed):
                    embedding = response.data[k].embedding
                    cache_key = hashlib.md5(text.encode()).hexdigest()
                    self._cache[cache_key] = embedding
                    cached_indices.append((j, embedding))

            # æŒ‰åŸå§‹é †åºæ’åˆ—
            cached_indices.sort(key=lambda x: x[0])
            all_embeddings.extend([emb for _, emb in cached_indices])

        return all_embeddings

    def embed_sync(self, text: str) -> List[float]:
        """åŒæ­¥ç‰ˆæœ¬ï¼ˆç”¨æ–¼å›èª¿ï¼‰"""
        return asyncio.get_event_loop().run_until_complete(
            self.embed(text)
        )
```

---

## 6.6 æ•´åˆï¼šçµ±ä¸€è¨˜æ†¶ç®¡ç†å™¨

ç¾åœ¨è®“æˆ‘å€‘å°‡ä¸‰å±¤è¨˜æ†¶æ•´åˆæˆä¸€å€‹çµ±ä¸€çš„ç®¡ç†å™¨ï¼š

```python
class UnifiedMemoryManager:
    """
    çµ±ä¸€è¨˜æ†¶ç®¡ç†å™¨

    â€¹1â€º æ•´åˆå·¥ä½œã€æƒ…ç¯€ã€èªç¾©ä¸‰å±¤è¨˜æ†¶
    â€¹2â€º è‡ªå‹•è™•ç†è¨˜æ†¶å±¤ç´šé–“çš„è½‰æ›
    â€¹3â€º æä¾›çµ±ä¸€çš„æŸ¥è©¢ä»‹é¢
    """

    def __init__(
        self,
        working_memory_tokens: int = 8000,
        episodic_memory_tokens: int = 32000,
        client: Optional[AsyncOpenAI] = None,
        model: str = "gpt-4o-mini"
    ):
        self.client = client or AsyncOpenAI()
        self.model = model

        # åˆå§‹åŒ–ä¸‰å±¤è¨˜æ†¶
        self.working = WorkingMemory(max_tokens=working_memory_tokens)
        self.episodic = EpisodicMemory(max_tokens=episodic_memory_tokens)
        self.semantic = SemanticMemory()

        # åˆå§‹åŒ–è¼”åŠ©çµ„ä»¶
        self._compressor = EpisodeCompressor(client=self.client, model=model)
        self._embedder = EmbeddingGenerator(client=self.client)

        # è¨­ç½®å›èª¿
        self.episodic.set_compressor(
            lambda content: asyncio.get_event_loop().run_until_complete(
                self._compressor.compress(content)
            )
        )
        self.semantic.set_embedder(self._embedder.embed_sync)

    async def process_step(
        self,
        thought: str,
        action: Optional[Dict[str, Any]] = None,
        observation: Optional[str] = None,
        importance: float = 0.5
    ) -> Episode:
        """
        è™•ç†ç ”ç©¶æ­¥é©Ÿ

        â€¹1â€º æ·»åŠ åˆ°æƒ…ç¯€è¨˜æ†¶
        â€¹2â€º æ›´æ–°å·¥ä½œè¨˜æ†¶
        â€¹3â€º å¿…è¦æ™‚æå–çŸ¥è­˜åˆ°èªç¾©è¨˜æ†¶
        """
        # 1. æ·»åŠ æƒ…ç¯€
        episode = self.episodic.add_episode(
            thought=thought,
            action=action,
            observation=observation,
            importance=importance
        )

        # 2. æ›´æ–°å·¥ä½œè¨˜æ†¶ï¼ˆåªä¿ç•™æœ€æ–°æ€è€ƒå’Œè§€å¯Ÿï¼‰
        self.working.add(
            content=f"æ­¥é©Ÿ {episode.step_number}: {thought[:100]}...",
            importance=importance,
            priority=MemoryPriority.MEDIUM if importance < 0.7 else MemoryPriority.HIGH,
            source="episode",
            step_number=episode.step_number
        )

        # 3. å¦‚æœæ˜¯é«˜é‡è¦æ€§æ­¥é©Ÿï¼Œæå–åˆ°èªç¾©è¨˜æ†¶
        if importance >= 0.8 and observation:
            await self._extract_knowledge(episode)

        return episode

    async def _extract_knowledge(self, episode: Episode) -> None:
        """å¾æƒ…ç¯€ä¸­æå–çŸ¥è­˜"""
        # ä½¿ç”¨ LLM æå–é—œéµçŸ¥è­˜
        extraction_prompt = f"""å¾ä»¥ä¸‹ç ”ç©¶æ­¥é©Ÿä¸­æå–å¯é‡ç”¨çš„çŸ¥è­˜é»ã€‚
åªæå–äº‹å¯¦æ€§è³‡è¨Šï¼Œä¸åŒ…å«éç¨‹æè¿°ã€‚
æ¯å€‹çŸ¥è­˜é»ä¸€è¡Œï¼Œæœ€å¤š 3 æ¢ã€‚

ç ”ç©¶æ­¥é©Ÿï¼š
{episode.to_prompt()}

çŸ¥è­˜é»ï¼š"""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": extraction_prompt}],
            max_tokens=200,
            temperature=0.3
        )

        knowledge_text = response.choices[0].message.content.strip()

        # åˆ†å‰²çŸ¥è­˜é»ä¸¦æ·»åŠ 
        for line in knowledge_text.split("\n"):
            line = line.strip()
            if line and not line.startswith("#"):
                embedding = await self._embedder.embed(line)
                self.semantic.add_knowledge(
                    content=line,
                    embedding=embedding,
                    source_episodes=[episode.step_number]
                )

    async def query(
        self,
        query: str,
        include_working: bool = True,
        include_episodic: bool = True,
        include_semantic: bool = True,
        max_tokens: int = 4000
    ) -> str:
        """
        çµ±ä¸€æŸ¥è©¢ä»‹é¢

        â€¹1â€º æœå°‹æ‰€æœ‰è¨˜æ†¶å±¤
        â€¹2â€º æ•´åˆä¸¦æ’åºçµæœ
        â€¹3â€º æ§åˆ¶è¼¸å‡ºé•·åº¦
        """
        results = []
        current_tokens = 0

        # 1. å·¥ä½œè¨˜æ†¶ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
        if include_working:
            working_prompt = self.working.to_prompt()
            working_tokens = len(working_prompt) // 3
            if current_tokens + working_tokens <= max_tokens:
                results.append(working_prompt)
                current_tokens += working_tokens

        # 2. èªç¾©è¨˜æ†¶ï¼ˆç›¸é—œçŸ¥è­˜ï¼‰
        if include_semantic:
            semantic_results = self.semantic.search(query, limit=5)
            for chunk, similarity in semantic_results:
                chunk_tokens = len(chunk.content) // 3
                if current_tokens + chunk_tokens > max_tokens:
                    break
                results.append(f"[çŸ¥è­˜] {chunk.content}")
                current_tokens += chunk_tokens

        # 3. æƒ…ç¯€è¨˜æ†¶ï¼ˆæœ€è¿‘æ­¥é©Ÿï¼‰
        if include_episodic:
            recent = self.episodic.get_recent(5)
            for episode in recent:
                ep_content = episode.to_prompt(use_summary=True)
                ep_tokens = len(ep_content) // 3
                if current_tokens + ep_tokens > max_tokens:
                    break
                results.append(ep_content)
                current_tokens += ep_tokens

        return "\n\n".join(results)

    def build_context(
        self,
        current_query: str,
        system_prompt: str = "",
        max_context_tokens: int = 8000
    ) -> List[Dict[str, str]]:
        """
        æ§‹å»ºå®Œæ•´çš„å°è©±ä¸Šä¸‹æ–‡

        â€¹1â€º ç³»çµ±æç¤ºè©
        â€¹2â€º è¨˜æ†¶å…§å®¹
        â€¹3â€º ç•¶å‰å•é¡Œ
        """
        messages = []

        # ç³»çµ±æç¤ºè©
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })

        # è¨˜æ†¶ä¸Šä¸‹æ–‡
        memory_context = asyncio.get_event_loop().run_until_complete(
            self.query(
                current_query,
                max_tokens=max_context_tokens - 1000  # é ç•™ç©ºé–“
            )
        )

        if memory_context:
            messages.append({
                "role": "system",
                "content": f"ä»¥ä¸‹æ˜¯ç›¸é—œçš„ç ”ç©¶ä¸Šä¸‹æ–‡ï¼š\n\n{memory_context}"
            })

        # ç•¶å‰å•é¡Œ
        messages.append({
            "role": "user",
            "content": current_query
        })

        return messages

    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–å®Œæ•´çµ±è¨ˆ"""
        return {
            "working_memory": self.working.get_statistics(),
            "episodic_memory": self.episodic.get_statistics(),
            "semantic_memory": self.semantic.get_statistics(),
            "total_tokens": (
                self.working._current_tokens +
                self.episodic._current_tokens
            )
        }
```

---

## 6.7 å¯¦æˆ°ï¼šè¨˜æ†¶å¢å¼·çš„ç ”ç©¶ä»£ç†äºº

è®“æˆ‘å€‘å°‡è¨˜æ†¶ç®¡ç†æ•´åˆåˆ°ç ”ç©¶ä»£ç†äººä¸­ï¼š

```python
class MemoryEnhancedResearchAgent:
    """
    è¨˜æ†¶å¢å¼·ç ”ç©¶ä»£ç†äºº

    â€¹1â€º æ•´åˆçµ±ä¸€è¨˜æ†¶ç®¡ç†å™¨
    â€¹2â€º è‡ªå‹•è™•ç†è¨˜æ†¶å±¤ç´š
    â€¹3â€º æ”¯æ´é•·ç¨‹ç ”ç©¶ä»»å‹™
    """

    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç ”ç©¶åŠ©ç†ï¼Œå…·å‚™ä»¥ä¸‹èƒ½åŠ›ï¼š

1. æ·±åº¦æœå°‹ï¼šèƒ½å¤ å¾ç¶²è·¯ç²å–æœ€æ–°è³‡è¨Š
2. æ‰¹åˆ¤æ€è€ƒï¼šèƒ½å¤ è©•ä¼°è³‡è¨Šçš„å¯é æ€§
3. çŸ¥è­˜æ•´åˆï¼šèƒ½å¤ å°‡å¤šå€‹ä¾†æºçš„è³‡è¨Šæ•´åˆæˆæœ‰æ¢ç†çš„å ±å‘Š

ä½ å¯ä»¥å­˜å–ä»¥ä¸‹è¨˜æ†¶ç³»çµ±ï¼š
- å·¥ä½œè¨˜æ†¶ï¼šç•¶å‰ä»»å‹™çš„é—œéµè³‡è¨Š
- ç ”ç©¶æ­·ç¨‹ï¼šéå»çš„ç ”ç©¶æ­¥é©Ÿæ‘˜è¦
- çŸ¥è­˜åº«ï¼šå·²é©—è­‰çš„äº‹å¯¦å’Œç™¼ç¾

è«‹åŸºæ–¼é€™äº›è¨˜æ†¶ï¼Œæä¾›æº–ç¢ºã€æœ‰ä¾æ“šçš„å›ç­”ã€‚"""

    def __init__(
        self,
        model: str = "gpt-4o-mini",
        max_iterations: int = 20
    ):
        self.client = AsyncOpenAI()
        self.model = model
        self.max_iterations = max_iterations

        # åˆå§‹åŒ–è¨˜æ†¶ç®¡ç†å™¨
        self.memory = UnifiedMemoryManager(
            client=self.client,
            model=model
        )

        # åˆå§‹åŒ–å·¥å…·ï¼ˆä½¿ç”¨ç¬¬ 5 ç« çš„ ToolManagerï¼‰
        # self.tools = ToolManager(client=self.client)

    async def research(
        self,
        query: str,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œç ”ç©¶ä»»å‹™

        â€¹1â€º åˆå§‹åŒ–è¨˜æ†¶
        â€¹2â€º åŸ·è¡Œ ReAct å¾ªç’°
        â€¹3â€º æ•´åˆçµæœ
        """
        # è¨˜éŒ„åŸå§‹å•é¡Œï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
        self.memory.working.add(
            content=f"ç ”ç©¶å•é¡Œï¼š{query}",
            importance=1.0,
            priority=MemoryPriority.CRITICAL,
            source="user"
        )

        iteration = 0
        final_answer = None

        while iteration < self.max_iterations:
            iteration += 1

            if verbose:
                print(f"\nğŸ”„ è¿­ä»£ {iteration}/{self.max_iterations}")

            # æ§‹å»ºä¸Šä¸‹æ–‡
            messages = self.memory.build_context(
                current_query=query,
                system_prompt=self.SYSTEM_PROMPT
            )

            # èª¿ç”¨ LLM
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7
            )

            content = response.choices[0].message.content

            # è§£æå›æ‡‰ï¼ˆç°¡åŒ–ç‰ˆï¼Œå¯¦éš›æ‡‰ä½¿ç”¨å·¥å…·èª¿ç”¨ï¼‰
            if "[æœ€çµ‚ç­”æ¡ˆ]" in content:
                final_answer = content.split("[æœ€çµ‚ç­”æ¡ˆ]")[1].strip()
                break

            # è¨˜éŒ„æ€è€ƒéç¨‹
            thought = content[:500] if len(content) > 500 else content

            # æ¨¡æ“¬å·¥å…·èª¿ç”¨çµæœ
            observation = f"é€™æ˜¯æ­¥é©Ÿ {iteration} çš„è§€å¯Ÿçµæœ..."

            # è¨ˆç®—é‡è¦æ€§
            importance = 0.5
            if any(kw in content.lower() for kw in ["ç™¼ç¾", "é—œéµ", "é‡è¦", "çµè«–"]):
                importance = 0.8

            # è¨˜éŒ„åˆ°è¨˜æ†¶
            await self.memory.process_step(
                thought=thought,
                action={"tool_name": "think", "step": iteration},
                observation=observation,
                importance=importance
            )

            if verbose:
                stats = self.memory.get_statistics()
                print(f"   ğŸ“Š è¨˜æ†¶ä½¿ç”¨ï¼šå·¥ä½œ {stats['working_memory']['utilization']*100:.1f}%ï¼Œ"
                      f"æƒ…ç¯€ {stats['episodic_memory']['utilization']*100:.1f}%")

        # æ•´åˆçµæœ
        return {
            "query": query,
            "answer": final_answer or "æœªèƒ½å®Œæˆç ”ç©¶",
            "iterations": iteration,
            "memory_stats": self.memory.get_statistics()
        }


# ä½¿ç”¨ç¯„ä¾‹
async def demo():
    agent = MemoryEnhancedResearchAgent()

    result = await agent.research(
        "åˆ†æ 2024 å¹´å…¨çƒé›»å‹•è»Šå¸‚å ´çš„ä¸»è¦è¶¨å‹¢å’Œç«¶çˆ­æ ¼å±€"
    )

    print("\n" + "=" * 60)
    print("ğŸ“ ç ”ç©¶çµæœ")
    print("=" * 60)
    print(result["answer"])
    print(f"\nğŸ“Š ç¸½è¿­ä»£æ¬¡æ•¸: {result['iterations']}")
```

---

## 6.8 æ•ˆèƒ½å„ªåŒ–æŠ€å·§

### 6.8.1 æ¼¸é€²å¼å£“ç¸®

```python
class ProgressiveCompressor:
    """
    æ¼¸é€²å¼å£“ç¸®å™¨

    â€¹1â€º æ ¹æ“šè¨˜æ†¶å¹´é½¡èª¿æ•´å£“ç¸®ç¨‹åº¦
    â€¹2â€º è¶ŠèˆŠçš„è¨˜æ†¶å£“ç¸®è¶Šç‹ 
    """

    COMPRESSION_LEVELS = {
        "light": "ä¿ç•™ 80% ç´°ç¯€ï¼Œå£“ç¸®åˆ° 100 å­—",
        "medium": "ä¿ç•™ 50% ç´°ç¯€ï¼Œå£“ç¸®åˆ° 50 å­—",
        "heavy": "åªä¿ç•™é—œéµçµè«–ï¼Œå£“ç¸®åˆ° 20 å­—"
    }

    def __init__(self, client: AsyncOpenAI):
        self.client = client

    def get_compression_level(self, age_hours: float) -> str:
        """æ ¹æ“šå¹´é½¡æ±ºå®šå£“ç¸®ç´šåˆ¥"""
        if age_hours < 1:
            return "light"
        elif age_hours < 4:
            return "medium"
        else:
            return "heavy"

    async def compress(
        self,
        content: str,
        level: str = "medium"
    ) -> str:
        instruction = self.COMPRESSION_LEVELS[level]

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": f"è«‹{instruction}ï¼š\n\n{content}"
            }],
            max_tokens=150
        )

        return response.choices[0].message.content.strip()
```

### 6.8.2 é¸æ“‡æ€§æª¢ç´¢

```python
class SelectiveRetriever:
    """
    é¸æ“‡æ€§æª¢ç´¢å™¨

    â€¹1â€º æ ¹æ“šæŸ¥è©¢é¡å‹é¸æ“‡è¨˜æ†¶å±¤
    â€¹2â€º å„ªåŒ–æª¢ç´¢æ•ˆç‡
    """

    QUERY_PATTERNS = {
        "recent": ["æœ€è¿‘", "å‰›æ‰", "ä¸Šä¸€æ­¥", "ä¹‹å‰"],
        "factual": ["ä»€éº¼æ˜¯", "å®šç¾©", "è§£é‡‹", "æ˜¯ä»€éº¼"],
        "historical": ["éç¨‹", "å¦‚ä½•", "ç‚ºä»€éº¼", "æ­¥é©Ÿ"]
    }

    def classify_query(self, query: str) -> str:
        """åˆ†é¡æŸ¥è©¢é¡å‹"""
        for query_type, patterns in self.QUERY_PATTERNS.items():
            if any(p in query for p in patterns):
                return query_type
        return "general"

    async def retrieve(
        self,
        query: str,
        memory: UnifiedMemoryManager
    ) -> str:
        query_type = self.classify_query(query)

        if query_type == "recent":
            # å„ªå…ˆæª¢ç´¢æƒ…ç¯€è¨˜æ†¶
            return await memory.query(
                query,
                include_working=True,
                include_episodic=True,
                include_semantic=False
            )

        elif query_type == "factual":
            # å„ªå…ˆæª¢ç´¢èªç¾©è¨˜æ†¶
            return await memory.query(
                query,
                include_working=False,
                include_episodic=False,
                include_semantic=True
            )

        else:
            # å…¨é¢æª¢ç´¢
            return await memory.query(query)
```

### 6.8.3 è¨˜æ†¶åƒåœ¾å›æ”¶

```python
class MemoryGarbageCollector:
    """
    è¨˜æ†¶åƒåœ¾å›æ”¶å™¨

    â€¹1â€º å®šæœŸæ¸…ç†ä½åƒ¹å€¼è¨˜æ†¶
    â€¹2â€º åˆä½µé‡è¤‡çŸ¥è­˜
    â€¹3â€º é‡‹æ”¾ç©ºé–“ä¾›æ–°è¨˜æ†¶ä½¿ç”¨
    """

    def __init__(
        self,
        min_access_count: int = 0,
        min_importance: float = 0.2,
        max_age_hours: float = 24
    ):
        self.min_access_count = min_access_count
        self.min_importance = min_importance
        self.max_age_hours = max_age_hours

    def collect(self, memory: UnifiedMemoryManager) -> Dict[str, int]:
        """åŸ·è¡Œåƒåœ¾å›æ”¶"""
        stats = {"working": 0, "episodic": 0, "semantic": 0}

        # æ¸…ç†å·¥ä½œè¨˜æ†¶ä¸­çš„ä½å„ªå…ˆç´šé …ç›®
        items_to_remove = []
        for item_id, item in memory.working._items.items():
            age_hours = (datetime.now() - item.created_at).total_seconds() / 3600
            if (
                item.access_count <= self.min_access_count and
                item.importance < self.min_importance and
                age_hours > 1 and
                item.priority.value < MemoryPriority.HIGH.value
            ):
                items_to_remove.append(item_id)

        for item_id in items_to_remove:
            item = memory.working._items.pop(item_id)
            memory.working._current_tokens -= item.token_count
            stats["working"] += 1

        # æ›´å¤šæ¸…ç†é‚è¼¯...

        return stats
```

---

## 6.9 ç« ç¯€ç¸½çµ

### æ ¸å¿ƒæ”¶ç©«

1. **ä¸‰å±¤è¨˜æ†¶æ¶æ§‹**
   - å·¥ä½œè¨˜æ†¶ï¼šç•¶å‰ä¸Šä¸‹æ–‡ï¼ˆ~8K tokensï¼‰
   - æƒ…ç¯€è¨˜æ†¶ï¼šç ”ç©¶æ­¥é©Ÿï¼ˆ~32K tokensï¼‰
   - èªç¾©è¨˜æ†¶ï¼šæŒä¹…çŸ¥è­˜ï¼ˆç„¡é™ï¼‰

2. **é—œéµæŠ€è¡“**
   - LRU é©…é€ç­–ç•¥
   - å‹•æ…‹å£“ç¸®
   - å‘é‡èªç¾©æª¢ç´¢
   - æ¼¸é€²å¼å£“ç¸®

3. **è¨­è¨ˆåŸå‰‡**
   - æŒ‰é‡è¦æ€§åˆ†é…è³‡æº
   - æ–°é®®åº¦èˆ‡ç›¸é—œæ€§å¹³è¡¡
   - è‡ªå‹•å±¤ç´šè½‰æ›

### æª¢æŸ¥æ¸…å–®

- [ ] å¯¦ç¾ä¸‰å±¤è¨˜æ†¶è³‡æ–™çµæ§‹
- [ ] å»ºç«‹ LRU é©…é€æ©Ÿåˆ¶
- [ ] æ•´åˆ LLM å£“ç¸®åŠŸèƒ½
- [ ] å¯¦ç¾å‘é‡èªç¾©æª¢ç´¢
- [ ] å»ºç«‹çµ±ä¸€æŸ¥è©¢ä»‹é¢
- [ ] æ¸¬è©¦é•·ç¨‹ç ”ç©¶ä»»å‹™

### æœ¬ç« ç”¢å‡ºç‰©

| é¡å‹ | å…§å®¹ |
|------|------|
| **è³‡æ–™çµæ§‹** | MemoryItem, Episode, KnowledgeChunk |
| **ç®¡ç†å™¨** | WorkingMemory, EpisodicMemory, SemanticMemory |
| **æ•´åˆå™¨** | UnifiedMemoryManager |
| **è¼”åŠ©å·¥å…·** | EpisodeCompressor, EmbeddingGenerator |
| **å„ªåŒ–å™¨** | ProgressiveCompressor, SelectiveRetriever |

---

## 6.10 ä¸‹ä¸€ç« é å‘Š

**ç¬¬ 7 ç« ï¼šæœå°‹èˆ‡æª¢ç´¢å¼•æ“**

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘å€‘å°‡æ·±å…¥æ¢è¨ï¼š

- ç¶²é ç€è¦½èˆ‡å…§å®¹æ“·å–
- RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰å¯¦ç¾
- çŸ¥è­˜åœ–è­œå»ºæ§‹
- å¤šæ¨¡æ…‹è³‡è¨Šæª¢ç´¢

ä½ å°‡å­¸æœƒå¦‚ä½•è®“ä»£ç†äººå¾ç¶²è·¯ç²å–è³‡è¨Šï¼Œä¸¦å°‡å…¶æ•´åˆåˆ°è¨˜æ†¶ç³»çµ±ä¸­ã€‚

---

## æœ¬ç« ç¨‹å¼ç¢¼

**GitHub ä½ç½®**ï¼š`code-examples/chapter-06/`

| æª”æ¡ˆ | è¡Œæ•¸ | èªªæ˜ |
|------|------|------|
| `memory_manager.py` | ~600 | çµ±ä¸€è¨˜æ†¶ç®¡ç†ç³»çµ± |
| `compressor.py` | ~150 | æƒ…ç¯€å£“ç¸®å™¨ |
| `embedder.py` | ~100 | åµŒå…¥ç”Ÿæˆå™¨ |
| `requirements.txt` | - | ä¾è³´æ¸…å–® |
| `.env.example` | - | ç’°å¢ƒè®Šæ•¸ç¯„ä¾‹ |
