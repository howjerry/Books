# ç¬¬ 13 ç« ï¼šå¹»è¦ºè™•ç†èˆ‡äº‹å¯¦æŸ¥æ ¸

> **æœ¬ç« ç›®æ¨™**ï¼šæ·±å…¥ç†è§£ LLM å¹»è¦ºçš„æˆå› èˆ‡é¡å‹ï¼ŒæŒæ¡æ™‚åºæ•æ„Ÿè¨“ç·´ã€å› æœå¾‹ç´„æŸç­‰é€²éšæŠ€è¡“ï¼Œå»ºæ§‹è‡ªå‹•åŒ–çš„äº‹å¯¦æŸ¥æ ¸ç³»çµ±ã€‚

---

## 13.1 èªè­˜ LLM å¹»è¦º

ã€Œä½ ç¢ºå®šé€™æ˜¯çœŸçš„å—ï¼Ÿã€

é€™å¯èƒ½æ˜¯ä½¿ç”¨ LLM é€²è¡Œæ·±åº¦ç ”ç©¶æ™‚æœ€å¸¸å•çš„å•é¡Œã€‚LLM å¹»è¦ºï¼ˆHallucinationï¼‰æ˜¯æŒ‡æ¨¡å‹ç”Ÿæˆçœ‹èµ·ä¾†åˆç†ä½†å¯¦éš›ä¸Šä¸æ­£ç¢ºæˆ–è™›æ§‹çš„è³‡è¨Šã€‚å°æ–¼æ·±åº¦ç ”ç©¶ä»£ç†äººä¾†èªªï¼Œé€™æ˜¯æœ€å¤§çš„æŒ‘æˆ°ä¹‹ä¸€ã€‚

### 13.1.1 å¹»è¦ºçš„é¡å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM å¹»è¦ºåˆ†é¡                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. äº‹å¯¦æ€§å¹»è¦º (Factual Hallucination)                    â”‚  â”‚
â”‚  â”‚     â””â”€â”€ ç”Ÿæˆèˆ‡ç¾å¯¦ä¸ç¬¦çš„äº‹å¯¦é™³è¿°                          â”‚  â”‚
â”‚  â”‚     ä¾‹ï¼šã€Œæ„›å› æ–¯å¦åœ¨ 1921 å¹´ç²å¾—è«¾è²çˆ¾åŒ–å­¸çã€            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. è™›æ§‹æ€§å¹»è¦º (Fabrication Hallucination)                â”‚  â”‚
â”‚  â”‚     â””â”€â”€ å®Œå…¨è™›æ§‹ä¸å­˜åœ¨çš„å¯¦é«”æˆ–äº‹ä»¶                        â”‚  â”‚
â”‚  â”‚     ä¾‹ï¼šå¼•ç”¨ä¸å­˜åœ¨çš„è«–æ–‡æˆ–å°ˆå®¶                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. æ™‚åºæ€§å¹»è¦º (Temporal Hallucination)                   â”‚  â”‚
â”‚  â”‚     â””â”€â”€ æ··æ·†äº‹ä»¶çš„æ™‚é–“é †åºæˆ–æ™‚æ•ˆæ€§                        â”‚  â”‚
â”‚  â”‚     ä¾‹ï¼šå°‡éæ™‚è³‡è¨Šç•¶ä½œæœ€æ–°è³‡è¨Š                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. æ¨ç†æ€§å¹»è¦º (Reasoning Hallucination)                  â”‚  â”‚
â”‚  â”‚     â””â”€â”€ æ¨ç†éç¨‹ä¸­çš„é‚è¼¯éŒ¯èª¤                              â”‚  â”‚
â”‚  â”‚     ä¾‹ï¼šéŒ¯èª¤çš„å› æœæ¨æ–·æˆ–æ•¸å­¸è¨ˆç®—                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  5. ä¾†æºæ€§å¹»è¦º (Source Hallucination)                     â”‚  â”‚
â”‚  â”‚     â””â”€â”€ éŒ¯èª¤æ­¸å› æˆ–æé€ å¼•ç”¨ä¾†æº                            â”‚  â”‚
â”‚  â”‚     ä¾‹ï¼šã€Œæ ¹æ“šã€Šè‡ªç„¶ã€‹æœŸåˆŠ 2023 å¹´çš„ç ”ç©¶...ã€ï¼ˆä¸å­˜åœ¨ï¼‰   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.1.2 å¹»è¦ºçš„æˆå› åˆ†æ

```python
#!/usr/bin/env python3
"""
å¹»è¦ºæˆå› åˆ†ææ¡†æ¶

åˆ†æå’Œåˆ†é¡ LLM å¹»è¦ºçš„æˆå› 
"""

from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from enum import Enum


class HallucinationType(Enum):
    """å¹»è¦ºé¡å‹"""
    FACTUAL = "factual"           # äº‹å¯¦æ€§å¹»è¦º
    FABRICATION = "fabrication"   # è™›æ§‹æ€§å¹»è¦º
    TEMPORAL = "temporal"         # æ™‚åºæ€§å¹»è¦º
    REASONING = "reasoning"       # æ¨ç†æ€§å¹»è¦º
    SOURCE = "source"             # ä¾†æºæ€§å¹»è¦º


class HallucinationCause(Enum):
    """å¹»è¦ºæˆå› """
    TRAINING_DATA_OUTDATED = "training_data_outdated"    # è¨“ç·´è³‡æ–™éæ™‚
    TRAINING_DATA_ERROR = "training_data_error"          # è¨“ç·´è³‡æ–™éŒ¯èª¤
    KNOWLEDGE_CUTOFF = "knowledge_cutoff"                # çŸ¥è­˜æˆªæ­¢æ—¥æœŸ
    PATTERN_OVERFITTING = "pattern_overfitting"          # æ¨¡å¼éæ“¬åˆ
    CONTEXT_CONFUSION = "context_confusion"              # ä¸Šä¸‹æ–‡æ··æ·†
    PROBABILITY_SAMPLING = "probability_sampling"        # æ©Ÿç‡æ¡æ¨£èª¤å·®
    INSTRUCTION_MISUNDERSTANDING = "instruction_misunderstanding"  # æŒ‡ä»¤èª¤è§£


@dataclass
class HallucinationInstance:
    """
    å¹»è¦ºå¯¦ä¾‹

    è¨˜éŒ„å–®å€‹å¹»è¦ºçš„è©³ç´°è³‡è¨Š
    """
    hallucination_id: str
    content: str                           # å¹»è¦ºå…§å®¹
    hallucination_type: HallucinationType
    possible_causes: List[HallucinationCause]
    correct_information: Optional[str]     # æ­£ç¢ºè³‡è¨Šï¼ˆå¦‚æœå·²çŸ¥ï¼‰
    confidence: float                      # åˆ¤å®šä¿¡å¿ƒåº¦
    context: str                           # ç™¼ç”Ÿçš„ä¸Šä¸‹æ–‡
    metadata: Dict[str, Any] = None

    def to_dict(self) -> dict:
        return {
            "id": self.hallucination_id,
            "content": self.content,
            "type": self.hallucination_type.value,
            "causes": [c.value for c in self.possible_causes],
            "correct_info": self.correct_information,
            "confidence": self.confidence,
            "context": self.context[:200] + "..." if len(self.context) > 200 else self.context
        }


class HallucinationAnalyzer:
    """
    å¹»è¦ºåˆ†æå™¨

    â€¹1â€º æª¢æ¸¬æ½›åœ¨çš„å¹»è¦º
    â€¹2â€º åˆ†é¡å¹»è¦ºé¡å‹
    â€¹3â€º æ¨æ–·å¯èƒ½æˆå› 
    """

    # å¸¸è¦‹çš„å¹»è¦ºæŒ‡æ¨™
    HALLUCINATION_INDICATORS = {
        "temporal": [
            "æœ€æ–°", "ç›®å‰", "ç¾åœ¨", "ä»Šå¹´", "recently", "currently",
            "as of", "latest", "now"
        ],
        "fabrication": [
            "æ ¹æ“šç ”ç©¶", "å°ˆå®¶è¡¨ç¤º", "æ“šå ±å°", "according to",
            "study shows", "research indicates"
        ],
        "factual": [
            "æ˜¯", "ç‚º", "æœ‰", "é”åˆ°", "è¶…é",
            "is", "was", "has", "reached", "exceeded"
        ]
    }

    def __init__(self, llm_client=None, knowledge_cutoff: str = "2024-01"):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            llm_client: LLM å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼è¼”åŠ©åˆ†æï¼‰
            knowledge_cutoff: æ¨¡å‹çŸ¥è­˜æˆªæ­¢æ—¥æœŸ
        """
        self.llm_client = llm_client
        self.knowledge_cutoff = knowledge_cutoff

    def detect_potential_hallucinations(
        self,
        text: str,
        context: str = ""
    ) -> List[Dict[str, Any]]:
        """
        æª¢æ¸¬æ½›åœ¨çš„å¹»è¦º

        â€¹2â€º åŸºæ–¼è¦å‰‡çš„åˆæ­¥ç¯©é¸
        """
        potential_hallucinations = []

        # æª¢æŸ¥æ™‚åºæ€§å•é¡Œ
        for indicator in self.HALLUCINATION_INDICATORS["temporal"]:
            if indicator in text.lower():
                potential_hallucinations.append({
                    "type": HallucinationType.TEMPORAL,
                    "indicator": indicator,
                    "reason": "åŒ…å«æ™‚æ•ˆæ€§æ•æ„Ÿè©å½™ï¼Œå¯èƒ½æ¶‰åŠéæ™‚è³‡è¨Š"
                })
                break

        # æª¢æŸ¥è™›æ§‹å¼•ç”¨
        for indicator in self.HALLUCINATION_INDICATORS["fabrication"]:
            if indicator in text.lower():
                # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦æœ‰å…·é«”ä¾†æº
                if not self._has_verifiable_source(text):
                    potential_hallucinations.append({
                        "type": HallucinationType.SOURCE,
                        "indicator": indicator,
                        "reason": "å¼•ç”¨ä¾†æºä½†ç„¡æ³•é©—è­‰"
                    })
                break

        # æª¢æŸ¥æ•¸å­—å’Œçµ±è¨ˆ
        numbers = self._extract_numbers(text)
        if numbers:
            potential_hallucinations.append({
                "type": HallucinationType.FACTUAL,
                "indicator": str(numbers),
                "reason": "åŒ…å«å…·é«”æ•¸å­—ï¼Œéœ€è¦é©—è­‰"
            })

        return potential_hallucinations

    def _has_verifiable_source(self, text: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰å¯é©—è­‰çš„ä¾†æº"""
        import re

        # æª¢æŸ¥ URL
        url_pattern = r'https?://[^\s]+'
        if re.search(url_pattern, text):
            return True

        # æª¢æŸ¥æ¨™æº–å¼•ç”¨æ ¼å¼
        citation_patterns = [
            r'\[\d+\]',                    # [1], [2] ç­‰
            r'\([A-Z][a-z]+,?\s*\d{4}\)',  # (Author, 2023)
            r'doi:\s*\d+\.\d+',            # DOI
        ]
        for pattern in citation_patterns:
            if re.search(pattern, text):
                return True

        return False

    def _extract_numbers(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„æ•¸å­—"""
        import re
        # åŒ¹é…å„ç¨®æ•¸å­—æ ¼å¼
        patterns = [
            r'\d+\.?\d*%',           # ç™¾åˆ†æ¯”
            r'\$\d+(?:,\d{3})*(?:\.\d+)?(?:\s*(?:billion|million))?',  # é‡‘é¡
            r'\d{4}å¹´',              # å¹´ä»½
            r'\d+(?:,\d{3})*',       # ä¸€èˆ¬æ•¸å­—
        ]
        numbers = []
        for pattern in patterns:
            numbers.extend(re.findall(pattern, text))
        return numbers[:5]  # é™åˆ¶æ•¸é‡

    async def analyze_with_llm(
        self,
        text: str,
        context: str = ""
    ) -> List[HallucinationInstance]:
        """
        ä½¿ç”¨ LLM é€²è¡Œæ·±åº¦åˆ†æ

        â€¹3â€º æ›´æº–ç¢ºçš„å¹»è¦ºæª¢æ¸¬
        """
        if not self.llm_client:
            return []

        prompt = f"""åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­å¯èƒ½å­˜åœ¨çš„å¹»è¦ºæˆ–ä¸æº–ç¢ºè³‡è¨Šã€‚

æ–‡æœ¬å…§å®¹ï¼š
{text}

èƒŒæ™¯è³‡è¨Šï¼š
{context}

æ¨¡å‹çŸ¥è­˜æˆªæ­¢æ—¥æœŸï¼š{self.knowledge_cutoff}

è«‹è­˜åˆ¥ï¼š
1. å¯èƒ½éæ™‚çš„è³‡è¨Š
2. å¯èƒ½è™›æ§‹çš„äº‹å¯¦
3. ç„¡æ³•é©—è­‰çš„å¼•ç”¨
4. é‚è¼¯æ¨ç†éŒ¯èª¤

ä»¥ JSON æ ¼å¼å›è¦†ï¼Œæ¯å€‹å•é¡ŒåŒ…å«ï¼š
- content: å•é¡Œå…§å®¹
- type: é¡å‹ï¼ˆfactual/fabrication/temporal/reasoning/sourceï¼‰
- confidence: ä¿¡å¿ƒåº¦ï¼ˆ0-1ï¼‰
- reason: åˆ¤æ–·åŸå› 
"""

        response = await self.llm_client.generate(prompt)
        return self._parse_llm_analysis(response, text)

    def _parse_llm_analysis(
        self,
        response: str,
        original_text: str
    ) -> List[HallucinationInstance]:
        """è§£æ LLM åˆ†æçµæœ"""
        import json
        import uuid

        instances = []

        try:
            # æå– JSON
            json_start = response.find("[")
            json_end = response.rfind("]") + 1
            if json_start >= 0 and json_end > json_start:
                data = json.loads(response[json_start:json_end])

                for item in data:
                    instances.append(HallucinationInstance(
                        hallucination_id=str(uuid.uuid4())[:8],
                        content=item.get("content", ""),
                        hallucination_type=HallucinationType(
                            item.get("type", "factual")
                        ),
                        possible_causes=[HallucinationCause.TRAINING_DATA_OUTDATED],
                        correct_information=None,
                        confidence=item.get("confidence", 0.5),
                        context=original_text
                    ))
        except Exception:
            pass

        return instances
```

---

## 13.2 äº‹å¯¦æŸ¥æ ¸ç³»çµ±è¨­è¨ˆ

äº‹å¯¦æŸ¥æ ¸ï¼ˆFact Checkingï¼‰æ˜¯å°æŠ—å¹»è¦ºçš„æ ¸å¿ƒæ©Ÿåˆ¶ã€‚ä¸€å€‹å®Œå–„çš„äº‹å¯¦æŸ¥æ ¸ç³»çµ±éœ€è¦å¤šå±¤æ¬¡çš„é©—è­‰ç­–ç•¥ã€‚

### 13.2.1 å¤šå±¤æ¬¡æŸ¥æ ¸æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å¤šå±¤æ¬¡äº‹å¯¦æŸ¥æ ¸æ¶æ§‹                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  è¼¸å…¥æ–‡æœ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Layer 1: èªæ³•å±¤æŸ¥æ ¸                                       â”‚  â”‚
â”‚  â”‚  - è­˜åˆ¥å¯é©—è­‰è²æ˜                                          â”‚  â”‚
â”‚  â”‚  - æå–å¯¦é«”å’Œé—œä¿‚                                          â”‚  â”‚
â”‚  â”‚  - æ¨™è¨˜éœ€è¦é©—è­‰çš„å…§å®¹                                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Layer 2: èªç¾©å±¤æŸ¥æ ¸                                       â”‚  â”‚
â”‚  â”‚  - äº¤å‰é©—è­‰å¤šå€‹ä¾†æº                                        â”‚  â”‚
â”‚  â”‚  - æª¢æŸ¥é‚è¼¯ä¸€è‡´æ€§                                          â”‚  â”‚
â”‚  â”‚  - è©•ä¼°ä¾†æºå¯ä¿¡åº¦                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Layer 3: çŸ¥è­˜å±¤æŸ¥æ ¸                                       â”‚  â”‚
â”‚  â”‚  - çŸ¥è­˜åœ–è­œæ¯”å°                                            â”‚  â”‚
â”‚  â”‚  - æ™‚é–“ç·šé©—è­‰                                              â”‚  â”‚
â”‚  â”‚  - å°ˆæ¥­é ˜åŸŸé©—è­‰                                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Layer 4: äººå·¥å¯©æ ¸ï¼ˆå¯é¸ï¼‰                                 â”‚  â”‚
â”‚  â”‚  - é«˜é¢¨éšªè²æ˜äººå·¥ç¢ºèª                                      â”‚  â”‚
â”‚  â”‚  - çˆ­è­°æ€§å…§å®¹æ¨™è¨˜                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  è¼¸å‡ºï¼šæŸ¥æ ¸å ±å‘Š + ä¿¡å¿ƒåˆ†æ•¸ + ä¿®æ­£å»ºè­°                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.2.2 äº‹å¯¦æŸ¥æ ¸å¼•æ“å¯¦ä½œ

```python
#!/usr/bin/env python3
"""
äº‹å¯¦æŸ¥æ ¸å¼•æ“

å¤šå±¤æ¬¡äº‹å¯¦é©—è­‰ç³»çµ±
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
from enum import Enum
import asyncio
from datetime import datetime
import re


class VerificationStatus(Enum):
    """é©—è­‰ç‹€æ…‹"""
    VERIFIED = "verified"           # å·²é©—è­‰æ­£ç¢º
    REFUTED = "refuted"             # å·²é©—è­‰éŒ¯èª¤
    UNCERTAIN = "uncertain"         # ç„¡æ³•ç¢ºå®š
    OUTDATED = "outdated"           # è³‡è¨Šéæ™‚
    UNVERIFIABLE = "unverifiable"   # ç„¡æ³•é©—è­‰


@dataclass
class Claim:
    """
    å¯é©—è­‰è²æ˜

    å¾æ–‡æœ¬ä¸­æå–çš„éœ€è¦é©—è­‰çš„è²æ˜
    """
    claim_id: str
    text: str
    claim_type: str  # factual, numerical, temporal, attribution
    entities: List[str] = field(default_factory=list)
    source_context: str = ""


@dataclass
class Evidence:
    """
    è­‰æ“š

    ç”¨æ–¼æ”¯æŒæˆ–åé§è²æ˜çš„è­‰æ“š
    """
    source: str                    # ä¾†æºåç¨±
    url: Optional[str]             # ä¾†æº URL
    content: str                   # ç›¸é—œå…§å®¹
    credibility_score: float       # å¯ä¿¡åº¦åˆ†æ•¸ 0-1
    publication_date: Optional[str] # ç™¼å¸ƒæ—¥æœŸ
    supports_claim: Optional[bool]  # æ”¯æŒ/åé§/ä¸­ç«‹


@dataclass
class VerificationResult:
    """
    é©—è­‰çµæœ
    """
    claim: Claim
    status: VerificationStatus
    confidence: float              # ä¿¡å¿ƒåº¦ 0-1
    evidences: List[Evidence] = field(default_factory=list)
    explanation: str = ""
    correction: Optional[str] = None  # å¦‚æœéŒ¯èª¤ï¼Œæä¾›æ­£ç¢ºè³‡è¨Š


class ClaimExtractor:
    """
    è²æ˜æå–å™¨

    â€¹1â€º å¾æ–‡æœ¬ä¸­æå–å¯é©—è­‰çš„è²æ˜
    """

    # è²æ˜é¡å‹çš„æ¨¡å¼
    CLAIM_PATTERNS = {
        "numerical": [
            r"(\d+(?:\.\d+)?%)",                    # ç™¾åˆ†æ¯”
            r"(\$\d+(?:,\d{3})*(?:\.\d+)?[MB]?)",   # é‡‘é¡
            r"(\d+(?:,\d{3})*\s*(?:äºº|å€‹|å®¶|æ¬¡))",  # æ•¸é‡
        ],
        "temporal": [
            r"((?:19|20)\d{2}å¹´)",                  # å¹´ä»½
            r"((?:ä¸Š|æœ¬|å»|ä»Š)å¹´)",                 # ç›¸å°æ™‚é–“
            r"(\d+æœˆ\d+æ—¥)",                        # æ—¥æœŸ
        ],
        "attribution": [
            r"((?:æ“š|æ ¹æ“š).*?(?:è¡¨ç¤º|æŒ‡å‡º|å ±å°))",   # å¼•ç”¨
            r"(.*?èªªï¼šã€Œ.*?ã€)",                    # ç›´æ¥å¼•ç”¨
        ]
    }

    def __init__(self, llm_client=None):
        self.llm_client = llm_client

    def extract_claims(self, text: str) -> List[Claim]:
        """
        æå–è²æ˜

        â€¹2â€º çµåˆè¦å‰‡å’Œ NLP æå–
        """
        claims = []
        claim_id = 0

        # å°‡æ–‡æœ¬åˆ†å‰²æˆå¥å­
        sentences = self._split_sentences(text)

        for sentence in sentences:
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å¯é©—è­‰å…§å®¹
            claim_type = self._identify_claim_type(sentence)
            if claim_type:
                claim_id += 1
                entities = self._extract_entities(sentence)
                claims.append(Claim(
                    claim_id=f"CLM-{claim_id:03d}",
                    text=sentence.strip(),
                    claim_type=claim_type,
                    entities=entities,
                    source_context=text[:500]
                ))

        return claims

    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        # ç°¡å–®çš„å¥å­åˆ†å‰²
        separators = r'[ã€‚ï¼ï¼Ÿ\.!?]'
        sentences = re.split(separators, text)
        return [s.strip() for s in sentences if s.strip()]

    def _identify_claim_type(self, sentence: str) -> Optional[str]:
        """è­˜åˆ¥è²æ˜é¡å‹"""
        for claim_type, patterns in self.CLAIM_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, sentence):
                    return claim_type

        # åŒ…å«ç‰¹å®šå‹•è©çš„é™³è¿°å¥
        fact_verbs = ["æ˜¯", "ç‚º", "æœ‰", "é”åˆ°", "è¶…é", "å¢é•·", "ä¸‹é™"]
        for verb in fact_verbs:
            if verb in sentence and len(sentence) > 10:
                return "factual"

        return None

    def _extract_entities(self, sentence: str) -> List[str]:
        """æå–å¯¦é«”"""
        entities = []

        # ç°¡å–®çš„å¯¦é«”è­˜åˆ¥ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­å¯ç”¨ NER æ¨¡å‹ï¼‰
        # å…¬å¸åç¨±
        company_pattern = r'([A-Z][a-z]*(?:\s[A-Z][a-z]*)*(?:\s(?:Inc|Corp|Ltd)\.?)?)'
        entities.extend(re.findall(company_pattern, sentence))

        # äººåï¼ˆä¸­æ–‡ï¼‰
        name_pattern = r'([ç‹æå¼µåŠ‰é™³æ¥Šé»ƒè¶™å‘¨å³][^\s]{1,3})'
        entities.extend(re.findall(name_pattern, sentence))

        return list(set(entities))[:5]


class SourceVerifier:
    """
    ä¾†æºé©—è­‰å™¨

    â€¹3â€º äº¤å‰é©—è­‰å¤šå€‹ä¾†æº
    """

    # å¯ä¿¡ä¾†æºç­‰ç´š
    SOURCE_CREDIBILITY = {
        "academic": 0.95,      # å­¸è¡“ä¾†æº
        "government": 0.90,    # æ”¿åºœå®˜æ–¹
        "major_news": 0.85,    # ä¸»æµåª’é«”
        "tech_news": 0.80,     # ç§‘æŠ€åª’é«”
        "general": 0.60,       # ä¸€èˆ¬ä¾†æº
        "social": 0.30,        # ç¤¾äº¤åª’é«”
        "unknown": 0.10,       # æœªçŸ¥ä¾†æº
    }

    TRUSTED_DOMAINS = {
        "academic": [
            "arxiv.org", "nature.com", "science.org", "ieee.org",
            "acm.org", "springer.com", "sciencedirect.com"
        ],
        "government": [
            "gov", "edu", "gov.tw", "gov.cn", "whitehouse.gov"
        ],
        "major_news": [
            "reuters.com", "apnews.com", "bbc.com", "nytimes.com",
            "wsj.com", "economist.com"
        ],
        "tech_news": [
            "techcrunch.com", "wired.com", "arstechnica.com",
            "theverge.com", "venturebeat.com"
        ]
    }

    def __init__(self, search_engine=None):
        self.search_engine = search_engine

    async def verify_claim(
        self,
        claim: Claim,
        max_sources: int = 5
    ) -> Tuple[VerificationStatus, List[Evidence], float]:
        """
        é©—è­‰è²æ˜

        Returns:
            (ç‹€æ…‹, è­‰æ“šåˆ—è¡¨, ä¿¡å¿ƒåº¦)
        """
        # æœå°‹ç›¸é—œè³‡è¨Š
        evidences = await self._search_evidences(claim, max_sources)

        if not evidences:
            return VerificationStatus.UNVERIFIABLE, [], 0.0

        # åˆ†æè­‰æ“š
        support_count = sum(1 for e in evidences if e.supports_claim)
        refute_count = sum(1 for e in evidences if e.supports_claim is False)
        neutral_count = len(evidences) - support_count - refute_count

        # è¨ˆç®—åŠ æ¬Šä¿¡å¿ƒåº¦
        weighted_support = sum(
            e.credibility_score for e in evidences
            if e.supports_claim
        )
        weighted_refute = sum(
            e.credibility_score for e in evidences
            if e.supports_claim is False
        )

        total_weight = weighted_support + weighted_refute
        if total_weight == 0:
            return VerificationStatus.UNCERTAIN, evidences, 0.3

        # åˆ¤å®šç‹€æ…‹
        if weighted_support > weighted_refute * 2:
            status = VerificationStatus.VERIFIED
            confidence = min(weighted_support / (total_weight + 0.5), 0.95)
        elif weighted_refute > weighted_support * 2:
            status = VerificationStatus.REFUTED
            confidence = min(weighted_refute / (total_weight + 0.5), 0.95)
        else:
            status = VerificationStatus.UNCERTAIN
            confidence = 0.5 - abs(weighted_support - weighted_refute) / total_weight * 0.3

        return status, evidences, confidence

    async def _search_evidences(
        self,
        claim: Claim,
        max_sources: int
    ) -> List[Evidence]:
        """æœå°‹è­‰æ“š"""
        if not self.search_engine:
            return []

        # æ§‹å»ºæœå°‹æŸ¥è©¢
        query = self._build_search_query(claim)

        # æœå°‹
        results = await self.search_engine.search(query, max_sources)

        evidences = []
        for result in results:
            credibility = self._assess_source_credibility(result.get("url", ""))
            supports = await self._check_support(claim.text, result.get("content", ""))

            evidences.append(Evidence(
                source=result.get("title", "Unknown"),
                url=result.get("url"),
                content=result.get("snippet", "")[:500],
                credibility_score=credibility,
                publication_date=result.get("date"),
                supports_claim=supports
            ))

        return evidences

    def _build_search_query(self, claim: Claim) -> str:
        """æ§‹å»ºæœå°‹æŸ¥è©¢"""
        # æå–é—œéµè©
        keywords = claim.entities[:3] if claim.entities else []

        # æ·»åŠ è²æ˜ä¸­çš„é—œéµå…§å®¹
        claim_words = [
            w for w in claim.text.split()
            if len(w) > 2 and not w.isdigit()
        ][:5]

        query_parts = keywords + claim_words
        return " ".join(query_parts)

    def _assess_source_credibility(self, url: str) -> float:
        """è©•ä¼°ä¾†æºå¯ä¿¡åº¦"""
        if not url:
            return self.SOURCE_CREDIBILITY["unknown"]

        url_lower = url.lower()

        for category, domains in self.TRUSTED_DOMAINS.items():
            for domain in domains:
                if domain in url_lower:
                    return self.SOURCE_CREDIBILITY[category]

        return self.SOURCE_CREDIBILITY["general"]

    async def _check_support(
        self,
        claim_text: str,
        evidence_content: str
    ) -> Optional[bool]:
        """æª¢æŸ¥è­‰æ“šæ˜¯å¦æ”¯æŒè²æ˜"""
        if not evidence_content:
            return None

        # ç°¡å–®çš„é—œéµè©åŒ¹é…
        claim_words = set(claim_text.lower().split())
        evidence_words = set(evidence_content.lower().split())

        overlap = len(claim_words & evidence_words)
        if overlap < 3:
            return None  # ç›¸é—œæ€§ä¸è¶³

        # æª¢æŸ¥å¦å®šè©
        negation_words = ["ä¸", "æ²’", "ç„¡", "éŒ¯", "å‡", "å¦", "not", "no", "false", "wrong"]
        has_negation = any(w in evidence_content.lower() for w in negation_words)

        # ç°¡åŒ–åˆ¤æ–·ï¼šæœ‰è¼ƒå¤šé‡ç–Šä¸”ç„¡å¦å®š = æ”¯æŒ
        if overlap > 5 and not has_negation:
            return True
        elif has_negation and overlap > 3:
            return False
        else:
            return None


class FactCheckEngine:
    """
    äº‹å¯¦æŸ¥æ ¸å¼•æ“

    æ•´åˆæ‰€æœ‰æŸ¥æ ¸åŠŸèƒ½
    """

    def __init__(
        self,
        llm_client=None,
        search_engine=None
    ):
        self.extractor = ClaimExtractor(llm_client)
        self.verifier = SourceVerifier(search_engine)
        self.llm_client = llm_client

    async def check(self, text: str) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„äº‹å¯¦æŸ¥æ ¸

        Returns:
            æŸ¥æ ¸å ±å‘Š
        """
        start_time = datetime.now()

        # Step 1: æå–è²æ˜
        claims = self.extractor.extract_claims(text)

        # Step 2: é©—è­‰æ¯å€‹è²æ˜
        results = []
        for claim in claims:
            status, evidences, confidence = await self.verifier.verify_claim(claim)

            result = VerificationResult(
                claim=claim,
                status=status,
                confidence=confidence,
                evidences=evidences,
                explanation=self._generate_explanation(status, evidences)
            )

            # å¦‚æœè¢«åé§ï¼Œå˜—è©¦æ‰¾åˆ°æ­£ç¢ºè³‡è¨Š
            if status == VerificationStatus.REFUTED:
                result.correction = await self._find_correction(claim, evidences)

            results.append(result)

        # Step 3: ç”Ÿæˆå ±å‘Š
        end_time = datetime.now()

        return {
            "summary": self._generate_summary(results),
            "claims_count": len(claims),
            "verified_count": sum(
                1 for r in results
                if r.status == VerificationStatus.VERIFIED
            ),
            "refuted_count": sum(
                1 for r in results
                if r.status == VerificationStatus.REFUTED
            ),
            "uncertain_count": sum(
                1 for r in results
                if r.status in [
                    VerificationStatus.UNCERTAIN,
                    VerificationStatus.UNVERIFIABLE
                ]
            ),
            "overall_credibility": self._calculate_credibility(results),
            "duration_seconds": (end_time - start_time).total_seconds(),
            "detailed_results": [
                {
                    "claim": r.claim.text,
                    "status": r.status.value,
                    "confidence": r.confidence,
                    "explanation": r.explanation,
                    "correction": r.correction
                }
                for r in results
            ]
        }

    def _generate_explanation(
        self,
        status: VerificationStatus,
        evidences: List[Evidence]
    ) -> str:
        """ç”Ÿæˆè§£é‡‹"""
        if status == VerificationStatus.VERIFIED:
            sources = [e.source for e in evidences if e.supports_claim][:3]
            return f"æ­¤è²æ˜å¾—åˆ°ä»¥ä¸‹ä¾†æºçš„é©—è­‰ï¼š{', '.join(sources)}"
        elif status == VerificationStatus.REFUTED:
            sources = [e.source for e in evidences if not e.supports_claim][:3]
            return f"æ­¤è²æ˜èˆ‡ä»¥ä¸‹ä¾†æºçš„è³‡è¨Šä¸ç¬¦ï¼š{', '.join(sources)}"
        elif status == VerificationStatus.OUTDATED:
            return "æ­¤è³‡è¨Šå¯èƒ½å·²éæ™‚ï¼Œå»ºè­°æŸ¥é–±æœ€æ–°è³‡æ–™"
        elif status == VerificationStatus.UNCERTAIN:
            return "ç„¡æ³•ç¢ºå®šæ­¤è²æ˜çš„æº–ç¢ºæ€§ï¼Œå»ºè­°é€²ä¸€æ­¥é©—è­‰"
        else:
            return "ç„¡æ³•æ‰¾åˆ°è¶³å¤ çš„è­‰æ“šä¾†é©—è­‰æ­¤è²æ˜"

    async def _find_correction(
        self,
        claim: Claim,
        evidences: List[Evidence]
    ) -> Optional[str]:
        """å°‹æ‰¾æ­£ç¢ºè³‡è¨Š"""
        # å¾åé§è­‰æ“šä¸­æå–å¯èƒ½çš„æ­£ç¢ºè³‡è¨Š
        refuting_evidences = [
            e for e in evidences
            if e.supports_claim is False
        ]

        if refuting_evidences and self.llm_client:
            evidence_texts = "\n".join([
                f"- {e.content}" for e in refuting_evidences[:3]
            ])

            prompt = f"""æ ¹æ“šä»¥ä¸‹è­‰æ“šï¼Œæä¾›æ­£ç¢ºçš„è³‡è¨Šï¼š

åŸå§‹è²æ˜ï¼š{claim.text}

åé§è­‰æ“šï¼š
{evidence_texts}

è«‹ç°¡æ½”åœ°èªªæ˜æ­£ç¢ºçš„è³‡è¨Šæ˜¯ä»€éº¼ã€‚"""

            correction = await self.llm_client.generate(prompt)
            return correction.strip()

        return None

    def _generate_summary(self, results: List[VerificationResult]) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        if not results:
            return "æœªç™¼ç¾éœ€è¦é©—è­‰çš„è²æ˜"

        verified = sum(1 for r in results if r.status == VerificationStatus.VERIFIED)
        refuted = sum(1 for r in results if r.status == VerificationStatus.REFUTED)
        total = len(results)

        if refuted == 0:
            return f"å…±æª¢æŸ¥ {total} å€‹è²æ˜ï¼Œæ‰€æœ‰å¯é©—è­‰çš„è²æ˜å‡æº–ç¢º"
        elif refuted < total / 3:
            return f"å…±æª¢æŸ¥ {total} å€‹è²æ˜ï¼Œç™¼ç¾ {refuted} å€‹ä¸æº–ç¢ºçš„å…§å®¹"
        else:
            return f"è­¦å‘Šï¼šå…±æª¢æŸ¥ {total} å€‹è²æ˜ï¼Œ{refuted} å€‹ä¸æº–ç¢ºï¼Œå»ºè­°è¬¹æ…ä½¿ç”¨"

    def _calculate_credibility(self, results: List[VerificationResult]) -> float:
        """è¨ˆç®—æ•´é«”å¯ä¿¡åº¦"""
        if not results:
            return 1.0

        verified_weight = sum(
            r.confidence for r in results
            if r.status == VerificationStatus.VERIFIED
        )
        refuted_weight = sum(
            r.confidence for r in results
            if r.status == VerificationStatus.REFUTED
        )
        uncertain_weight = sum(
            0.5 for r in results
            if r.status in [
                VerificationStatus.UNCERTAIN,
                VerificationStatus.UNVERIFIABLE
            ]
        )

        total = verified_weight + refuted_weight + uncertain_weight
        if total == 0:
            return 0.5

        return verified_weight / total
```

---

## 13.3 æ™‚åºæ•æ„Ÿè¨“ç·´

æ™‚åºæ€§å¹»è¦ºæ˜¯æ·±åº¦ç ”ç©¶ä»£ç†äººé¢è‡¨çš„ç‰¹æ®ŠæŒ‘æˆ°ã€‚æ¨¡å‹çš„çŸ¥è­˜æœ‰æˆªæ­¢æ—¥æœŸï¼Œä½†ç ”ç©¶å•é¡Œå¾€å¾€éœ€è¦æœ€æ–°è³‡è¨Šã€‚

### 13.3.1 æ™‚åºæ„ŸçŸ¥æ©Ÿåˆ¶

```python
#!/usr/bin/env python3
"""
æ™‚åºæ•æ„Ÿè¨“ç·´èˆ‡æ¨ç†

è™•ç†æ™‚é–“ç›¸é—œçš„è³‡è¨Šå’Œå¹»è¦º
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from enum import Enum
import re


class TemporalCategory(Enum):
    """æ™‚é–“é¡åˆ¥"""
    STATIC = "static"              # éœæ…‹äº‹å¯¦ï¼ˆä¸éš¨æ™‚é–“è®ŠåŒ–ï¼‰
    DYNAMIC = "dynamic"            # å‹•æ…‹è³‡è¨Šï¼ˆç¶“å¸¸è®ŠåŒ–ï¼‰
    PERIODIC = "periodic"          # å‘¨æœŸæ€§è³‡è¨Š
    EVENT = "event"                # ä¸€æ¬¡æ€§äº‹ä»¶
    PREDICTION = "prediction"      # é æ¸¬æ€§è³‡è¨Š


@dataclass
class TemporalContext:
    """
    æ™‚åºä¸Šä¸‹æ–‡

    è¨˜éŒ„è³‡è¨Šçš„æ™‚é–“ç›¸é—œå±¬æ€§
    """
    reference_time: datetime       # åƒè€ƒæ™‚é–“
    validity_period: timedelta     # æœ‰æ•ˆæœŸ
    category: TemporalCategory
    confidence_decay: float        # ä¿¡å¿ƒè¡°æ¸›ç‡ï¼ˆæ¯å¤©ï¼‰


class TemporalAwareProcessor:
    """
    æ™‚åºæ„ŸçŸ¥è™•ç†å™¨

    â€¹1â€º è­˜åˆ¥æ™‚é–“æ•æ„Ÿè³‡è¨Š
    â€¹2â€º è©•ä¼°è³‡è¨Šæ™‚æ•ˆæ€§
    â€¹3â€º æ¨™è¨˜éœ€è¦æ›´æ–°çš„å…§å®¹
    """

    # æ™‚é–“æ•æ„Ÿè©å½™
    TEMPORAL_INDICATORS = {
        "current": ["ç›®å‰", "ç¾åœ¨", "ç•¶å‰", "ç¾ä»Š", "current", "currently", "now"],
        "recent": ["æœ€è¿‘", "è¿‘æœŸ", "æœ€æ–°", "å‰›å‰›", "recent", "recently", "latest"],
        "future": ["å°‡", "é è¨ˆ", "å³å°‡", "æœªä¾†", "will", "expected", "upcoming"],
        "past": ["æ›¾", "éå»", "ä»¥å‰", "æ­·å²", "was", "were", "previously"],
    }

    # è³‡è¨Šé¡å‹çš„é è¨­æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰
    VALIDITY_PERIODS = {
        "stock_price": 0,           # è‚¡åƒ¹ï¼šå³æ™‚
        "exchange_rate": 0,         # åŒ¯ç‡ï¼šå³æ™‚
        "news": 1,                  # æ–°èï¼š1 å¤©
        "market_data": 7,           # å¸‚å ´æ•¸æ“šï¼š1 é€±
        "company_info": 30,         # å…¬å¸è³‡è¨Šï¼š1 å€‹æœˆ
        "research_data": 90,        # ç ”ç©¶æ•¸æ“šï¼š3 å€‹æœˆ
        "historical_fact": 36500,   # æ­·å²äº‹å¯¦ï¼š100 å¹´
    }

    def __init__(self, model_cutoff: str = "2024-01-01"):
        """
        åˆå§‹åŒ–è™•ç†å™¨

        Args:
            model_cutoff: æ¨¡å‹çŸ¥è­˜æˆªæ­¢æ—¥æœŸ
        """
        self.model_cutoff = datetime.fromisoformat(model_cutoff)
        self.current_time = datetime.now()

    def analyze_temporal_sensitivity(
        self,
        text: str
    ) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æœ¬çš„æ™‚åºæ•æ„Ÿæ€§

        â€¹2â€º è­˜åˆ¥éœ€è¦æ™‚åºé©—è­‰çš„å…§å®¹
        """
        analysis = {
            "has_temporal_references": False,
            "temporal_markers": [],
            "requires_update": False,
            "sensitive_phrases": [],
            "recommended_actions": []
        }

        # æª¢æŸ¥æ™‚é–“æŒ‡ç¤ºè©
        for category, indicators in self.TEMPORAL_INDICATORS.items():
            for indicator in indicators:
                if indicator in text.lower():
                    analysis["has_temporal_references"] = True
                    analysis["temporal_markers"].append({
                        "marker": indicator,
                        "category": category
                    })

        # æª¢æŸ¥æ—¥æœŸå¼•ç”¨
        date_patterns = [
            r"((?:19|20)\d{2})å¹´",
            r"(\d{1,2})æœˆ(\d{1,2})æ—¥",
            r"(Q[1-4])\s*((?:19|20)\d{2})",
        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                analysis["has_temporal_references"] = True
                for match in matches:
                    if isinstance(match, tuple):
                        match = "".join(match)
                    analysis["sensitive_phrases"].append(match)

        # åˆ¤æ–·æ˜¯å¦éœ€è¦æ›´æ–°
        if analysis["has_temporal_references"]:
            # æª¢æŸ¥æ˜¯å¦å¼•ç”¨äº†æ¨¡å‹çŸ¥è­˜æˆªæ­¢å¾Œçš„æ—¥æœŸ
            for phrase in analysis["sensitive_phrases"]:
                try:
                    year_match = re.search(r"((?:19|20)\d{2})", phrase)
                    if year_match:
                        year = int(year_match.group(1))
                        if year >= self.model_cutoff.year:
                            analysis["requires_update"] = True
                            analysis["recommended_actions"].append(
                                f"é©—è­‰ {year} å¹´çš„è³‡è¨Šæ˜¯å¦æœ€æ–°"
                            )
                except ValueError:
                    pass

        return analysis

    def estimate_information_age(
        self,
        text: str,
        source_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """
        ä¼°è¨ˆè³‡è¨Šå¹´é½¡

        â€¹3â€º è©•ä¼°è³‡è¨Šçš„æ–°é®®åº¦
        """
        if source_date:
            age_days = (self.current_time - source_date).days
        else:
            # å¾æ–‡æœ¬ä¸­æå–æ—¥æœŸ
            source_date = self._extract_date(text)
            if source_date:
                age_days = (self.current_time - source_date).days
            else:
                # å‡è¨­ä¾†è‡ªæ¨¡å‹çŸ¥è­˜
                age_days = (self.current_time - self.model_cutoff).days

        # æ ¹æ“šè³‡è¨Šé¡å‹è©•ä¼°
        info_type = self._classify_information_type(text)
        validity_days = self.VALIDITY_PERIODS.get(info_type, 30)

        is_stale = age_days > validity_days
        freshness_score = max(0, 1 - age_days / validity_days) if validity_days else 0

        return {
            "age_days": age_days,
            "information_type": info_type,
            "validity_period_days": validity_days,
            "is_stale": is_stale,
            "freshness_score": freshness_score,
            "recommendation": self._get_freshness_recommendation(
                is_stale, age_days, info_type
            )
        }

    def _extract_date(self, text: str) -> Optional[datetime]:
        """å¾æ–‡æœ¬æå–æ—¥æœŸ"""
        # å˜—è©¦å¤šç¨®æ—¥æœŸæ ¼å¼
        patterns = [
            (r"(20\d{2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", "%Y-%m-%d"),
            (r"(20\d{2})-(\d{2})-(\d{2})", "%Y-%m-%d"),
            (r"(\d{1,2})/(\d{1,2})/(20\d{2})", "%m/%d/%Y"),
        ]

        for pattern, fmt in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    groups = match.groups()
                    if fmt == "%Y-%m-%d":
                        return datetime(int(groups[0]), int(groups[1]), int(groups[2]))
                    elif fmt == "%m/%d/%Y":
                        return datetime(int(groups[2]), int(groups[0]), int(groups[1]))
                except ValueError:
                    continue

        return None

    def _classify_information_type(self, text: str) -> str:
        """åˆ†é¡è³‡è¨Šé¡å‹"""
        text_lower = text.lower()

        if any(w in text_lower for w in ["è‚¡åƒ¹", "stock", "price", "trading"]):
            return "stock_price"
        elif any(w in text_lower for w in ["åŒ¯ç‡", "exchange", "currency"]):
            return "exchange_rate"
        elif any(w in text_lower for w in ["æ–°è", "å ±å°", "news", "reported"]):
            return "news"
        elif any(w in text_lower for w in ["å¸‚å ´", "market", "industry"]):
            return "market_data"
        elif any(w in text_lower for w in ["å…¬å¸", "ä¼æ¥­", "company", "corporation"]):
            return "company_info"
        elif any(w in text_lower for w in ["ç ”ç©¶", "è«–æ–‡", "study", "research"]):
            return "research_data"
        elif any(w in text_lower for w in ["æ­·å²", "æ­·å²ä¸Š", "history", "historical"]):
            return "historical_fact"
        else:
            return "general"

    def _get_freshness_recommendation(
        self,
        is_stale: bool,
        age_days: int,
        info_type: str
    ) -> str:
        """ç²å–æ–°é®®åº¦å»ºè­°"""
        if not is_stale:
            return "è³‡è¨Šåœ¨æœ‰æ•ˆæœŸå…§ï¼Œå¯ç›´æ¥ä½¿ç”¨"

        if info_type in ["stock_price", "exchange_rate"]:
            return "æ­¤é¡è³‡è¨Šè®ŠåŒ–æ¥µå¿«ï¼Œè«‹ç²å–å³æ™‚æ•¸æ“š"
        elif info_type == "news":
            return "æ–°èè³‡è¨Šå·²éæœŸï¼Œè«‹æŸ¥é–±æœ€æ–°å ±å°"
        elif age_days > 365:
            return f"è³‡è¨Šå·²è¶…é {age_days // 365} å¹´ï¼Œå»ºè­°é‡æ–°é©—è­‰"
        else:
            return f"è³‡è¨Šå·²æœ‰ {age_days} å¤©ï¼Œå»ºè­°ç¢ºèªæ˜¯å¦æœ‰æ›´æ–°"

    def generate_temporal_disclaimer(
        self,
        text: str,
        source_date: Optional[datetime] = None
    ) -> str:
        """
        ç”Ÿæˆæ™‚åºå…è²¬è²æ˜

        â€¹4â€º ç‚ºè¼¸å‡ºæ·»åŠ æ™‚é–“ç›¸é—œè­¦å‘Š
        """
        analysis = self.analyze_temporal_sensitivity(text)
        age_info = self.estimate_information_age(text, source_date)

        disclaimers = []

        if age_info["is_stale"]:
            disclaimers.append(
                f"âš ï¸ æ­¤è³‡è¨Šå·²æœ‰ {age_info['age_days']} å¤©ï¼Œ"
                f"å¯èƒ½å·²éæ™‚ã€‚{age_info['recommendation']}"
            )

        if analysis["requires_update"]:
            for action in analysis["recommended_actions"][:3]:
                disclaimers.append(f"ğŸ“Œ {action}")

        if not disclaimers:
            return ""

        return "\n\n---\n**æ™‚æ•ˆæ€§æé†’**ï¼š\n" + "\n".join(disclaimers)
```

---

## 13.4 å› æœå¾‹ç´„æŸ

å› æœå¾‹ç´„æŸæ˜¯é˜²æ­¢æ¨ç†æ€§å¹»è¦ºçš„é‡è¦æ©Ÿåˆ¶ã€‚å®ƒç¢ºä¿æ¨¡å‹çš„æ¨ç†éµå¾ªé‚è¼¯å› æœé—œä¿‚ã€‚

### 13.4.1 å› æœæ¨ç†é©—è­‰å™¨

```python
#!/usr/bin/env python3
"""
å› æœå¾‹ç´„æŸç³»çµ±

é©—è­‰æ¨ç†éç¨‹çš„é‚è¼¯æ­£ç¢ºæ€§
"""

from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
from enum import Enum


class CausalRelationType(Enum):
    """å› æœé—œä¿‚é¡å‹"""
    CAUSES = "causes"              # A å°è‡´ B
    ENABLES = "enables"            # A ä½¿ B æˆç‚ºå¯èƒ½
    PREVENTS = "prevents"          # A é˜»æ­¢ B
    CORRELATES = "correlates"      # A èˆ‡ B ç›¸é—œï¼ˆéå› æœï¼‰
    CONTRADICTS = "contradicts"    # A èˆ‡ B çŸ›ç›¾


@dataclass
class CausalClaim:
    """
    å› æœè²æ˜

    è¡¨ç¤ºä¸€å€‹å› æœé—œä¿‚ä¸»å¼µ
    """
    cause: str                     # åŸå› 
    effect: str                    # çµæœ
    relation_type: CausalRelationType
    confidence: float              # ä¿¡å¿ƒåº¦
    evidence: Optional[str] = None # æ”¯æŒè­‰æ“š


@dataclass
class CausalValidation:
    """
    å› æœé©—è­‰çµæœ
    """
    claim: CausalClaim
    is_valid: bool
    issues: List[str]
    suggestions: List[str]


class CausalReasoningValidator:
    """
    å› æœæ¨ç†é©—è­‰å™¨

    â€¹1â€º è­˜åˆ¥æ–‡æœ¬ä¸­çš„å› æœä¸»å¼µ
    â€¹2â€º é©—è­‰å› æœé—œä¿‚çš„åˆç†æ€§
    â€¹3â€º æª¢æ¸¬å¸¸è¦‹çš„é‚è¼¯è¬¬èª¤
    """

    # å› æœæŒ‡ç¤ºè©
    CAUSAL_INDICATORS = {
        "causes": [
            "å°è‡´", "é€ æˆ", "å¼•èµ·", "ä½¿", "è®“",
            "å› ç‚º", "ç”±æ–¼", "æ‰€ä»¥", "å› æ­¤",
            "causes", "leads to", "results in", "because"
        ],
        "enables": [
            "ä¿ƒé€²", "æ¨å‹•", "æœ‰åŠ©æ–¼", "å¹«åŠ©",
            "enables", "allows", "facilitates"
        ],
        "prevents": [
            "é˜»æ­¢", "é˜²æ­¢", "é¿å…", "æŠ‘åˆ¶",
            "prevents", "blocks", "inhibits"
        ],
    }

    # å¸¸è¦‹é‚è¼¯è¬¬èª¤æ¨¡å¼
    FALLACY_PATTERNS = {
        "post_hoc": {
            "description": "å¾Œæ­¤è¬¬èª¤ï¼šåƒ…å› æ™‚åºå…ˆå¾Œæ¨æ–·å› æœ",
            "indicators": ["ä¹‹å¾Œ", "æ¥è‘—", "ç„¶å¾Œ", "after", "then", "subsequently"]
        },
        "correlation_causation": {
            "description": "ç›¸é—œæ€§è¬¬èª¤ï¼šå°‡ç›¸é—œæ€§ç­‰åŒæ–¼å› æœæ€§",
            "indicators": ["ç›¸é—œ", "ä¼´éš¨", "åŒæ™‚", "correlates", "associated"]
        },
        "single_cause": {
            "description": "å–®ä¸€åŸå› è¬¬èª¤ï¼šè¤‡é›œç¾è±¡æ­¸å› æ–¼å–®ä¸€åŸå› ",
            "indicators": ["å”¯ä¸€", "åªæ˜¯å› ç‚º", "ç´”ç²¹æ˜¯", "solely", "only because"]
        },
        "reverse_causation": {
            "description": "å› æœå€’ç½®ï¼šæ··æ·†åŸå› å’Œçµæœçš„æ–¹å‘",
            "indicators": []  # éœ€è¦èªç¾©åˆ†æ
        }
    }

    def __init__(self, llm_client=None):
        self.llm_client = llm_client

    def extract_causal_claims(self, text: str) -> List[CausalClaim]:
        """
        æå–å› æœä¸»å¼µ

        â€¹2â€º å¾æ–‡æœ¬ä¸­è­˜åˆ¥å› æœé—œä¿‚
        """
        claims = []
        sentences = self._split_into_sentences(text)

        for sentence in sentences:
            relation_type = self._identify_causal_relation(sentence)
            if relation_type:
                cause, effect = self._extract_cause_effect(
                    sentence, relation_type
                )
                if cause and effect:
                    claims.append(CausalClaim(
                        cause=cause,
                        effect=effect,
                        relation_type=relation_type,
                        confidence=0.7,
                        evidence=sentence
                    ))

        return claims

    def _split_into_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        import re
        return [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ\.!?]', text) if s.strip()]

    def _identify_causal_relation(
        self,
        sentence: str
    ) -> Optional[CausalRelationType]:
        """è­˜åˆ¥å› æœé—œä¿‚é¡å‹"""
        sentence_lower = sentence.lower()

        for relation_type, indicators in self.CAUSAL_INDICATORS.items():
            for indicator in indicators:
                if indicator in sentence_lower:
                    return CausalRelationType(relation_type)

        return None

    def _extract_cause_effect(
        self,
        sentence: str,
        relation_type: CausalRelationType
    ) -> Tuple[Optional[str], Optional[str]]:
        """æå–åŸå› å’Œçµæœ"""
        # ç°¡åŒ–ç‰ˆï¼šæ ¹æ“šæŒ‡ç¤ºè©åˆ†å‰²
        for indicator in self.CAUSAL_INDICATORS.get(relation_type.value, []):
            if indicator in sentence:
                parts = sentence.split(indicator, 1)
                if len(parts) == 2:
                    return parts[0].strip(), parts[1].strip()

        return None, None

    def validate_causal_claim(
        self,
        claim: CausalClaim
    ) -> CausalValidation:
        """
        é©—è­‰å› æœä¸»å¼µ

        â€¹3â€º æª¢æŸ¥é‚è¼¯åˆç†æ€§
        """
        issues = []
        suggestions = []

        # æª¢æŸ¥å¸¸è¦‹è¬¬èª¤
        for fallacy_name, fallacy_info in self.FALLACY_PATTERNS.items():
            if self._check_fallacy(claim, fallacy_info):
                issues.append(f"å¯èƒ½å­˜åœ¨{fallacy_info['description']}")
                suggestions.append(self._get_fallacy_suggestion(fallacy_name))

        # æª¢æŸ¥å› æœæ–¹å‘åˆç†æ€§
        if self._check_reverse_causation_risk(claim):
            issues.append("å› æœæ–¹å‘å¯èƒ½éœ€è¦é©—è­‰")
            suggestions.append("å»ºè­°æª¢æŸ¥æ˜¯å¦å­˜åœ¨åå‘å› æœé—œä¿‚")

        # æª¢æŸ¥éåº¦ç°¡åŒ–
        if self._check_oversimplification(claim):
            issues.append("å¯èƒ½éåº¦ç°¡åŒ–äº†å› æœé—œä¿‚")
            suggestions.append("å»ºè­°è€ƒæ…®å…¶ä»–å¯èƒ½çš„å½±éŸ¿å› ç´ ")

        is_valid = len(issues) == 0

        return CausalValidation(
            claim=claim,
            is_valid=is_valid,
            issues=issues,
            suggestions=suggestions
        )

    def _check_fallacy(
        self,
        claim: CausalClaim,
        fallacy_info: Dict[str, Any]
    ) -> bool:
        """æª¢æŸ¥æ˜¯å¦å­˜åœ¨ç‰¹å®šè¬¬èª¤"""
        if not claim.evidence:
            return False

        for indicator in fallacy_info.get("indicators", []):
            if indicator in claim.evidence.lower():
                return True

        return False

    def _check_reverse_causation_risk(self, claim: CausalClaim) -> bool:
        """æª¢æŸ¥åå‘å› æœé¢¨éšª"""
        # ç°¡åŒ–å¯¦ç¾ï¼šæª¢æŸ¥å¸¸è¦‹çš„å¯é€†å› æœå°
        reversible_pairs = [
            ("æ”¶å…¥", "æ•™è‚²"),
            ("å¥åº·", "é‹å‹•"),
            ("æˆåŠŸ", "ä¿¡å¿ƒ"),
            ("income", "education"),
            ("health", "exercise"),
        ]

        for pair in reversible_pairs:
            if (pair[0] in claim.cause.lower() and pair[1] in claim.effect.lower()) or \
               (pair[1] in claim.cause.lower() and pair[0] in claim.effect.lower()):
                return True

        return False

    def _check_oversimplification(self, claim: CausalClaim) -> bool:
        """æª¢æŸ¥éåº¦ç°¡åŒ–"""
        # è¤‡é›œç¾è±¡çš„ç°¡å–®æ¨™èªŒ
        complex_phenomena = [
            "ç¶“æ¿Ÿå¢é•·", "æ°£å€™è®ŠåŒ–", "ç¤¾æœƒå•é¡Œ", "å¥åº·",
            "economic growth", "climate change", "social issues", "health"
        ]

        for phenomenon in complex_phenomena:
            if phenomenon in claim.effect.lower():
                # è¤‡é›œç¾è±¡ä¸å¤ªå¯èƒ½æœ‰å–®ä¸€åŸå› 
                return True

        return False

    def _get_fallacy_suggestion(self, fallacy_name: str) -> str:
        """ç²å–è¬¬èª¤å»ºè­°"""
        suggestions = {
            "post_hoc": "æ™‚é–“å…ˆå¾Œä¸ç­‰æ–¼å› æœé—œä¿‚ï¼Œå»ºè­°å°‹æ‰¾æ©Ÿåˆ¶è§£é‡‹",
            "correlation_causation": "ç›¸é—œæ€§éœ€è¦é€²ä¸€æ­¥é©—è­‰æ‰èƒ½ç¢ºå®šå› æœé—œä¿‚",
            "single_cause": "è€ƒæ…®æ˜¯å¦å­˜åœ¨å…¶ä»–å…±åŒå½±éŸ¿å› ç´ ",
            "reverse_causation": "ç¢ºèªå› æœæ–¹å‘ï¼Œè€ƒæ…®æ˜¯å¦å­˜åœ¨åå‘é—œä¿‚"
        }
        return suggestions.get(fallacy_name, "è«‹é€²ä¸€æ­¥é©—è­‰å› æœé—œä¿‚")

    async def validate_with_llm(
        self,
        claim: CausalClaim
    ) -> CausalValidation:
        """
        ä½¿ç”¨ LLM é€²è¡Œæ·±åº¦é©—è­‰

        â€¹4â€º æ›´æº–ç¢ºçš„å› æœåˆ†æ
        """
        if not self.llm_client:
            return self.validate_causal_claim(claim)

        prompt = f"""åˆ†æä»¥ä¸‹å› æœä¸»å¼µçš„é‚è¼¯åˆç†æ€§ï¼š

åŸå› ï¼š{claim.cause}
é—œä¿‚ï¼š{claim.relation_type.value}
çµæœï¼š{claim.effect}

è«‹è©•ä¼°ï¼š
1. é€™å€‹å› æœé—œä¿‚æ˜¯å¦åˆç†ï¼Ÿ
2. æ˜¯å¦å­˜åœ¨é‚è¼¯è¬¬èª¤ï¼Ÿ
3. æ˜¯å¦éœ€è¦é¡å¤–æ¢ä»¶æˆ–å‰æï¼Ÿ
4. æ˜¯å¦å¯èƒ½å­˜åœ¨åå‘å› æœï¼Ÿ

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼ŒåŒ…å«ï¼š
- is_valid: æ˜¯å¦æœ‰æ•ˆï¼ˆtrue/falseï¼‰
- issues: å•é¡Œåˆ—è¡¨
- suggestions: å»ºè­°åˆ—è¡¨
- confidence: è©•ä¼°ä¿¡å¿ƒåº¦ï¼ˆ0-1ï¼‰
"""

        response = await self.llm_client.generate(prompt)
        return self._parse_llm_validation(response, claim)

    def _parse_llm_validation(
        self,
        response: str,
        claim: CausalClaim
    ) -> CausalValidation:
        """è§£æ LLM é©—è­‰çµæœ"""
        import json

        try:
            # æå– JSON
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            if json_start >= 0 and json_end > json_start:
                data = json.loads(response[json_start:json_end])

                return CausalValidation(
                    claim=claim,
                    is_valid=data.get("is_valid", True),
                    issues=data.get("issues", []),
                    suggestions=data.get("suggestions", [])
                )
        except Exception:
            pass

        # è§£æå¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡é©—è­‰
        return self.validate_causal_claim(claim)
```

---

## 13.5 æ•´åˆï¼šè‡ªå‹•äº‹å¯¦æŸ¥æ ¸ç®¡é“

å°‡ä»¥ä¸Šæ‰€æœ‰çµ„ä»¶æ•´åˆæˆä¸€å€‹å®Œæ•´çš„äº‹å¯¦æŸ¥æ ¸ç®¡é“ã€‚

### 13.5.1 å®Œæ•´ç®¡é“å¯¦ä½œ

```python
#!/usr/bin/env python3
"""
è‡ªå‹•äº‹å¯¦æŸ¥æ ¸ç®¡é“

æ•´åˆæ‰€æœ‰æŸ¥æ ¸åŠŸèƒ½
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional
from datetime import datetime
import asyncio


@dataclass
class FactCheckReport:
    """
    äº‹å¯¦æŸ¥æ ¸å ±å‘Š
    """
    input_text: str
    check_time: datetime
    duration_seconds: float

    # å¹»è¦ºåˆ†æ
    hallucination_analysis: Dict[str, Any] = field(default_factory=dict)

    # äº‹å¯¦é©—è­‰
    fact_verification: Dict[str, Any] = field(default_factory=dict)

    # æ™‚åºåˆ†æ
    temporal_analysis: Dict[str, Any] = field(default_factory=dict)

    # å› æœé©—è­‰
    causal_validation: Dict[str, Any] = field(default_factory=dict)

    # ç¸½é«”è©•ä¼°
    overall_credibility: float = 0.0
    risk_level: str = "low"
    summary: str = ""
    recommendations: List[str] = field(default_factory=list)


class FactCheckPipeline:
    """
    è‡ªå‹•äº‹å¯¦æŸ¥æ ¸ç®¡é“

    æ•´åˆæ‰€æœ‰æŸ¥æ ¸åŠŸèƒ½
    """

    def __init__(
        self,
        llm_client=None,
        search_engine=None,
        model_cutoff: str = "2024-01-01"
    ):
        """
        åˆå§‹åŒ–ç®¡é“

        Args:
            llm_client: LLM å®¢æˆ¶ç«¯
            search_engine: æœå°‹å¼•æ“
            model_cutoff: æ¨¡å‹çŸ¥è­˜æˆªæ­¢æ—¥æœŸ
        """
        self.llm_client = llm_client
        self.search_engine = search_engine
        self.model_cutoff = model_cutoff

        # åˆå§‹åŒ–å„çµ„ä»¶
        from hle_evaluator import HallucinationAnalyzer  # å‡è¨­å­˜åœ¨
        self.hallucination_analyzer = HallucinationAnalyzer(
            llm_client, model_cutoff
        )

        self.fact_checker = FactCheckEngine(llm_client, search_engine)
        self.temporal_processor = TemporalAwareProcessor(model_cutoff)
        self.causal_validator = CausalReasoningValidator(llm_client)

    async def check(self, text: str) -> FactCheckReport:
        """
        åŸ·è¡Œå®Œæ•´çš„äº‹å¯¦æŸ¥æ ¸

        Args:
            text: å¾…æŸ¥æ ¸çš„æ–‡æœ¬

        Returns:
            å®Œæ•´çš„æŸ¥æ ¸å ±å‘Š
        """
        start_time = datetime.now()

        # ä¸¦è¡ŒåŸ·è¡Œå„é …æª¢æŸ¥
        tasks = [
            self._analyze_hallucinations(text),
            self._verify_facts(text),
            self._analyze_temporal(text),
            self._validate_causal(text),
        ]

        results = await asyncio.gather(*tasks)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # æ•´åˆçµæœ
        report = FactCheckReport(
            input_text=text[:1000] + "..." if len(text) > 1000 else text,
            check_time=start_time,
            duration_seconds=duration,
            hallucination_analysis=results[0],
            fact_verification=results[1],
            temporal_analysis=results[2],
            causal_validation=results[3]
        )

        # è¨ˆç®—ç¸½é«”è©•ä¼°
        self._compute_overall_assessment(report)

        return report

    async def _analyze_hallucinations(self, text: str) -> Dict[str, Any]:
        """å¹»è¦ºåˆ†æ"""
        potential = self.hallucination_analyzer.detect_potential_hallucinations(
            text
        )
        detailed = await self.hallucination_analyzer.analyze_with_llm(text)

        return {
            "potential_issues": len(potential),
            "detected_hallucinations": len(detailed),
            "details": [h.to_dict() for h in detailed]
        }

    async def _verify_facts(self, text: str) -> Dict[str, Any]:
        """äº‹å¯¦é©—è­‰"""
        result = await self.fact_checker.check(text)
        return result

    async def _analyze_temporal(self, text: str) -> Dict[str, Any]:
        """æ™‚åºåˆ†æ"""
        sensitivity = self.temporal_processor.analyze_temporal_sensitivity(text)
        age_info = self.temporal_processor.estimate_information_age(text)
        disclaimer = self.temporal_processor.generate_temporal_disclaimer(text)

        return {
            "sensitivity": sensitivity,
            "age_info": age_info,
            "disclaimer": disclaimer
        }

    async def _validate_causal(self, text: str) -> Dict[str, Any]:
        """å› æœé©—è­‰"""
        claims = self.causal_validator.extract_causal_claims(text)
        validations = []

        for claim in claims:
            if self.llm_client:
                validation = await self.causal_validator.validate_with_llm(claim)
            else:
                validation = self.causal_validator.validate_causal_claim(claim)
            validations.append({
                "claim": f"{claim.cause} â†’ {claim.effect}",
                "is_valid": validation.is_valid,
                "issues": validation.issues
            })

        return {
            "claims_found": len(claims),
            "valid_claims": sum(1 for v in validations if v["is_valid"]),
            "validations": validations
        }

    def _compute_overall_assessment(self, report: FactCheckReport):
        """è¨ˆç®—ç¸½é«”è©•ä¼°"""
        scores = []

        # å¹»è¦ºåˆ†æè©•åˆ†
        hallucination_count = report.hallucination_analysis.get(
            "detected_hallucinations", 0
        )
        hallucination_score = max(0, 1 - hallucination_count * 0.2)
        scores.append(hallucination_score)

        # äº‹å¯¦é©—è­‰è©•åˆ†
        fact_credibility = report.fact_verification.get("overall_credibility", 0.5)
        scores.append(fact_credibility)

        # æ™‚åºè©•åˆ†
        freshness = report.temporal_analysis.get("age_info", {}).get(
            "freshness_score", 0.5
        )
        scores.append(freshness)

        # å› æœè©•åˆ†
        causal_data = report.causal_validation
        if causal_data.get("claims_found", 0) > 0:
            causal_score = (
                causal_data.get("valid_claims", 0) /
                causal_data.get("claims_found", 1)
            )
        else:
            causal_score = 1.0
        scores.append(causal_score)

        # è¨ˆç®—åŠ æ¬Šå¹³å‡
        weights = [0.3, 0.4, 0.15, 0.15]
        report.overall_credibility = sum(
            s * w for s, w in zip(scores, weights)
        )

        # ç¢ºå®šé¢¨éšªç­‰ç´š
        if report.overall_credibility >= 0.8:
            report.risk_level = "low"
        elif report.overall_credibility >= 0.6:
            report.risk_level = "medium"
        else:
            report.risk_level = "high"

        # ç”Ÿæˆæ‘˜è¦
        report.summary = self._generate_summary(report)

        # ç”Ÿæˆå»ºè­°
        report.recommendations = self._generate_recommendations(report)

    def _generate_summary(self, report: FactCheckReport) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        credibility_pct = report.overall_credibility * 100
        risk_labels = {"low": "ä½", "medium": "ä¸­", "high": "é«˜"}

        return (
            f"æ•´é«”å¯ä¿¡åº¦è©•ä¼°ï¼š{credibility_pct:.1f}%ï¼Œ"
            f"é¢¨éšªç­‰ç´šï¼š{risk_labels[report.risk_level]}ã€‚"
            f"æª¢æŸ¥è€—æ™‚ï¼š{report.duration_seconds:.2f} ç§’ã€‚"
        )

    def _generate_recommendations(self, report: FactCheckReport) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []

        # åŸºæ–¼å¹»è¦ºåˆ†æ
        if report.hallucination_analysis.get("detected_hallucinations", 0) > 0:
            recommendations.append("å»ºè­°å°æ¨™è¨˜çš„æ½›åœ¨å¹»è¦ºé€²è¡Œäººå·¥é©—è­‰")

        # åŸºæ–¼äº‹å¯¦é©—è­‰
        refuted = report.fact_verification.get("refuted_count", 0)
        if refuted > 0:
            recommendations.append(f"ç™¼ç¾ {refuted} å€‹ä¸æº–ç¢ºçš„è²æ˜ï¼Œè«‹ä¿®æ­£")

        # åŸºæ–¼æ™‚åºåˆ†æ
        if report.temporal_analysis.get("age_info", {}).get("is_stale", False):
            recommendations.append("éƒ¨åˆ†è³‡è¨Šå¯èƒ½éæ™‚ï¼Œå»ºè­°æ›´æ–°")

        # åŸºæ–¼å› æœåˆ†æ
        invalid_causal = (
            report.causal_validation.get("claims_found", 0) -
            report.causal_validation.get("valid_claims", 0)
        )
        if invalid_causal > 0:
            recommendations.append(f"{invalid_causal} å€‹å› æœæ¨ç†éœ€è¦é‡æ–°å¯©è¦–")

        if not recommendations:
            recommendations.append("æŸ¥æ ¸é€šéï¼Œå…§å®¹å¯ä¿¡åº¦è¼ƒé«˜")

        return recommendations


# ===== ä½¿ç”¨ç¤ºä¾‹ =====

async def demo_fact_check():
    """ç¤ºç¯„äº‹å¯¦æŸ¥æ ¸"""
    text = """
    æ ¹æ“šæœ€æ–°ç ”ç©¶ï¼Œè˜‹æœå…¬å¸åœ¨ 2024 å¹´çš„ç‡Ÿæ”¶é”åˆ° 4000 å„„ç¾å…ƒï¼Œ
    é€™ä¸»è¦æ˜¯å› ç‚º iPhone 15 çš„æˆåŠŸã€‚ç”±æ–¼ AI æŠ€è¡“çš„ç™¼å±•ï¼Œ
    æ™ºæ…§å‹æ‰‹æ©Ÿå¸‚å ´å¢é•·äº† 50%ã€‚å°ˆå®¶é æ¸¬ï¼Œåˆ° 2025 å¹´ï¼Œ
    å…¨çƒ AI æ™¶ç‰‡å¸‚å ´å°‡é”åˆ° 1000 å„„ç¾å…ƒè¦æ¨¡ã€‚
    """

    # æ³¨æ„ï¼šå¯¦éš›ä½¿ç”¨éœ€è¦é…ç½® LLM å’Œæœå°‹å¼•æ“
    pipeline = FactCheckPipeline(
        llm_client=None,
        search_engine=None,
        model_cutoff="2024-01-01"
    )

    report = await pipeline.check(text)

    print("=" * 60)
    print("äº‹å¯¦æŸ¥æ ¸å ±å‘Š")
    print("=" * 60)
    print(f"\næ‘˜è¦ï¼š{report.summary}")
    print(f"\nå¯ä¿¡åº¦ï¼š{report.overall_credibility:.2%}")
    print(f"é¢¨éšªç­‰ç´šï¼š{report.risk_level}")
    print("\nå»ºè­°ï¼š")
    for rec in report.recommendations:
        print(f"  - {rec}")


if __name__ == "__main__":
    asyncio.run(demo_fact_check())
```

---

## 13.6 ç« ç¯€ç¸½çµ

æœ¬ç« æ·±å…¥æ¢è¨äº† LLM å¹»è¦ºè™•ç†èˆ‡äº‹å¯¦æŸ¥æ ¸ï¼š

### æ ¸å¿ƒæ¦‚å¿µ

1. **å¹»è¦ºé¡å‹**
   - äº‹å¯¦æ€§å¹»è¦ºï¼šäº‹å¯¦éŒ¯èª¤
   - è™›æ§‹æ€§å¹»è¦ºï¼šå®Œå…¨è™›æ§‹
   - æ™‚åºæ€§å¹»è¦ºï¼šæ™‚é–“æ··æ·†
   - æ¨ç†æ€§å¹»è¦ºï¼šé‚è¼¯éŒ¯èª¤
   - ä¾†æºæ€§å¹»è¦ºï¼šå¼•ç”¨æé€ 

2. **äº‹å¯¦æŸ¥æ ¸ç³»çµ±**
   - å¤šå±¤æ¬¡æŸ¥æ ¸æ¶æ§‹
   - è²æ˜æå–èˆ‡é©—è­‰
   - ä¾†æºå¯ä¿¡åº¦è©•ä¼°

3. **æ™‚åºæ•æ„Ÿè™•ç†**
   - è­˜åˆ¥æ™‚æ•ˆæ€§æ•æ„Ÿè³‡è¨Š
   - è©•ä¼°è³‡è¨Šæ–°é®®åº¦
   - ç”Ÿæˆæ™‚åºå…è²¬è²æ˜

4. **å› æœå¾‹ç´„æŸ**
   - è­˜åˆ¥å› æœä¸»å¼µ
   - æª¢æ¸¬é‚è¼¯è¬¬èª¤
   - é©—è­‰å› æœåˆç†æ€§

### æª¢æŸ¥æ¸…å–®

- [ ] ç†è§£äº”ç¨®ä¸»è¦å¹»è¦ºé¡å‹
- [ ] èƒ½å¤ å»ºæ§‹å¤šå±¤æ¬¡äº‹å¯¦æŸ¥æ ¸ç³»çµ±
- [ ] æŒæ¡æ™‚åºæ•æ„Ÿæ€§è™•ç†æ–¹æ³•
- [ ] äº†è§£å¸¸è¦‹é‚è¼¯è¬¬èª¤åŠæª¢æ¸¬æ–¹æ³•

---

## 13.7 ä¸‹ä¸€ç« é å‘Š

åœ¨ä¸‹ä¸€ç« ã€Œæ•ˆèƒ½å„ªåŒ–èˆ‡æˆæœ¬æ§åˆ¶ã€ä¸­ï¼Œæˆ‘å€‘å°‡æ¢è¨ï¼š

- Token æ•ˆç‡å„ªåŒ–ç­–ç•¥
- æ™ºèƒ½å¿«å–æ©Ÿåˆ¶
- æ‰¹æ¬¡è™•ç†èˆ‡ä¸¦è¡ŒåŒ–
- æˆæœ¬ç›£æ§èˆ‡é ç®—æ§åˆ¶
- æ¨¡å‹é¸æ“‡èˆ‡é™ç´šç­–ç•¥

é€™äº›æŠ€è¡“å°‡å¹«åŠ©ä½ åœ¨ä¿æŒç ”ç©¶å“è³ªçš„åŒæ™‚ï¼Œå¤§å¹…é™ä½é‹ç‡Ÿæˆæœ¬ã€‚
